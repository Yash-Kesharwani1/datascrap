{"id":"job_deloitte_data_engineer_001","role":{"title":"Data Engineer","level":"senior","department":"Technology & Transformation","team":"Data Engineering","reports_to":"Project / Engagement Manager"},"company":{"name":"Deloitte","size":"10000+","industry":"Professional Services / Consulting","culture_keywords":["innovation","collaboration","ownership","impact"]},"description":{"summary":"Design, build, and optimize scalable data pipelines and data management solutions using modern big data and cloud technologies.","key_value_proposition":"Work on complex, real-world data engineering problems for global clients using cutting-edge cloud and big data platforms.","full_text":"The Data Engineer will work on data engineering projects for various business units, focusing on delivery of complex data management solutions by leveraging industry best practices."},"responsibilities":{"primary":["Design and build scalable data pipelines and ETL/ELT solutions","Develop data processing solutions using Python and PySpark","Work with Apache Spark, Hadoop, Kafka for large-scale data processing","Deploy and manage data workloads on cloud platforms","Optimize data pipelines and resolve performance bottlenecks","Ensure data quality, reliability, and scalability"],"secondary":["Collaborate with cross-functional and agile teams","Contribute to architectural decisions","Support production systems and resolve incidents","Participate in CoE and CoP community initiatives"],"time_allocation":{"coding":65,"mentoring":10,"meetings":25}},"requirements":{"must_have":{"experience_years":5,"technical_skills":["Python","PySpark","Apache Spark","SQL","ETL/ELT","Databricks","Azure Data Services"],"domain_expertise":["Data Engineering","Big Data Processing","Cloud Data Platforms"],"soft_skills":["problem-solving","communication","ownership","collaboration"]},"nice_to_have":{"education":[],"technical_skills":["AWS","Google Cloud Platform","Kafka","Hadoop","Kubernetes","CI/CD","JavaScript","REST APIs"],"domain_expertise":["High-volume data systems","Real-time data processing","Cloud-native architectures"]},"deal_breakers":["Lack of hands-on Python and PySpark experience","No experience with big data or cloud platforms"]},"technical_assessment_focus":{"coding":["Python and PySpark development","Data pipeline implementation","Unit and integration testing"],"system_design":["End-to-end ETL pipeline design","Cloud data architecture","Scalable data processing systems"],"domain_knowledge":["ETL/ELT concepts","Data warehousing","Performance optimization techniques"],"problem_solving":["Debugging production data issues","Resolving performance bottlenecks","Handling high volume and velocity data"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":true,"target_percent":null}},"logistics":{"location":"Bangalore, India","remote_policy":"onsite / hybrid","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Managerial / project discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Technical Panel"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Senior Data Engineer role at Deloitte in Bangalore. 5+ years experience required. Tech stack includes Python, PySpark, Databricks, Apache Spark, Azure data services. Focus on building scalable data pipelines and cloud-based big data solutions.","semantic_tags":["data-engineering","senior-engineer","python","pyspark","databricks","azure","big-data","etl","cloud-data"]}}