{"id":"job_deloitte_data_engineer_001","role":{"title":"Data Engineer","level":"senior","department":"Technology & Transformation","team":"Data Engineering","reports_to":"Project / Engagement Manager"},"company":{"name":"Deloitte","size":"10000+","industry":"Professional Services / Consulting","culture_keywords":["innovation","collaboration","ownership","impact"]},"description":{"summary":"Design, build, and optimize scalable data pipelines and data management solutions using modern big data and cloud technologies.","key_value_proposition":"Work on complex, real-world data engineering problems for global clients using cutting-edge cloud and big data platforms.","full_text":"The Data Engineer will work on data engineering projects for various business units, focusing on delivery of complex data management solutions by leveraging industry best practices."},"responsibilities":{"primary":["Design and build scalable data pipelines and ETL/ELT solutions","Develop data processing solutions using Python and PySpark","Work with Apache Spark, Hadoop, Kafka for large-scale data processing","Deploy and manage data workloads on cloud platforms","Optimize data pipelines and resolve performance bottlenecks","Ensure data quality, reliability, and scalability"],"secondary":["Collaborate with cross-functional and agile teams","Contribute to architectural decisions","Support production systems and resolve incidents","Participate in CoE and CoP community initiatives"],"time_allocation":{"coding":65,"mentoring":10,"meetings":25}},"requirements":{"must_have":{"experience_years":5,"technical_skills":["Python","PySpark","Apache Spark","SQL","ETL/ELT","Databricks","Azure Data Services"],"domain_expertise":["Data Engineering","Big Data Processing","Cloud Data Platforms"],"soft_skills":["problem-solving","communication","ownership","collaboration"]},"nice_to_have":{"education":[],"technical_skills":["AWS","Google Cloud Platform","Kafka","Hadoop","Kubernetes","CI/CD","JavaScript","REST APIs"],"domain_expertise":["High-volume data systems","Real-time data processing","Cloud-native architectures"]},"deal_breakers":["Lack of hands-on Python and PySpark experience","No experience with big data or cloud platforms"]},"technical_assessment_focus":{"coding":["Python and PySpark development","Data pipeline implementation","Unit and integration testing"],"system_design":["End-to-end ETL pipeline design","Cloud data architecture","Scalable data processing systems"],"domain_knowledge":["ETL/ELT concepts","Data warehousing","Performance optimization techniques"],"problem_solving":["Debugging production data issues","Resolving performance bottlenecks","Handling high volume and velocity data"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":true,"target_percent":null}},"logistics":{"location":"Bangalore, India","remote_policy":"onsite / hybrid","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Managerial / project discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Technical Panel"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Senior Data Engineer role at Deloitte in Bangalore. 5+ years experience required. Tech stack includes Python, PySpark, Databricks, Apache Spark, Azure data services. Focus on building scalable data pipelines and cloud-based big data solutions.","semantic_tags":["data-engineering","senior-engineer","python","pyspark","databricks","azure","big-data","etl","cloud-data"]}}
{"id":"job_mrf_data_engineer_001","role":{"title":"Data Engineer","level":"mid","department":"Digital Technology","team":"Data Engineering","reports_to":"Data / Application Manager"},"company":{"name":"MRF Limited","size":"10000+","industry":"Motor Vehicle Parts Manufacturing","culture_keywords":["quality","innovation","customer-focus","reliability"]},"description":{"summary":"Maintain and build Azure-based data engineering solutions, including SQL, Databricks, and real-time pipelines for managed enterprise applications.","key_value_proposition":"Work on large-scale enterprise data platforms supporting critical manufacturing and planning systems.","full_text":"Responsible to maintain the data required for managed applications like Advanced Planning System and Dealer Management System, building cloud-based data pipelines using Azure services and Python."},"responsibilities":{"primary":["Maintain and manage data for enterprise applications","Build and maintain real-time data pipelines from on-prem SAP systems to Azure cloud","Develop data extraction and transformation workflows using Azure Data Factory and Databricks","Work with Azure SQL and NoSQL databases for structured and unstructured data","Develop Python models and improve them through containerization","Manage user access for data upload and download"],"secondary":["Collaborate with functional teams for structured data management","Conduct exploratory data analysis for insights","Monitor and troubleshoot Azure data pipeline performance","Support data migration between on-prem and cloud environments"],"time_allocation":{"coding":60,"mentoring":5,"meetings":35}},"requirements":{"must_have":{"experience_years":2,"technical_skills":["Python","PySpark","SQL","Azure Data Factory","Azure Data Lake","Databricks","Azure SQL","Kafka"],"domain_expertise":["Data Engineering","Cloud Data Migration","Real-time Data Pipelines"],"soft_skills":["problem-solving","communication","ownership","troubleshooting"]},"nice_to_have":{"education":["BE CS","B.Tech IT","MCA"],"technical_skills":["Azure Machine Learning","Azure Cosmos DB","CI/CD","TensorFlow","Jupyter","Containerization"],"domain_expertise":["SAP data integration","Time-series databases","API development"]},"deal_breakers":["Lack of hands-on Azure data engineering experience","No SQL or Python proficiency"]},"technical_assessment_focus":{"coding":["Python and PySpark development","SQL query optimization","API development"],"system_design":["Azure-based data pipeline architecture","On-prem to cloud data migration","Real-time streaming pipelines"],"domain_knowledge":["ETL/ELT processes","Azure data services","Database performance tuning"],"problem_solving":["Pipeline performance troubleshooting","Production issue resolution","Data migration challenges"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":null,"remote_policy":"onsite","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Managerial discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Technical Panel"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Data Engineer role at MRF Limited. 2–3 years experience required. Azure data engineering stack including Data Factory, Databricks, Azure SQL, Kafka, Python, PySpark. Focus on real-time pipelines and cloud data migration.","semantic_tags":["data-engineering","azure","databricks","python","pyspark","sql","kafka","cloud-migration","real-time-data"]}}
{"id":"job_infosys_ml_consultant_001","role":{"title":"Machine Learning Engineer / Consultant","level":"mid","department":"Consulting","team":"Digital Transformation","reports_to":"Project Manager / Delivery Manager"},"company":{"name":"Infosys","size":"10000+","industry":"IT Services & Consulting","culture_keywords":["client-focus","innovation","collaboration","ownership"]},"description":{"summary":"Design and deliver machine learning–driven solutions by diagnosing client problems, designing innovative models, and supporting deployment in large-scale consulting engagements.","key_value_proposition":"Work closely with global clients to design, propose, and deploy ML-driven solutions that enable digital transformation and business growth.","full_text":"As part of the Infosys consulting team, the role involves diagnosing customer issues, designing innovative machine learning solutions, supporting deployment, conducting POCs, and leading small projects to deliver high-quality value to clients."},"responsibilities":{"primary":["Design and implement machine learning solutions using Python","Diagnose client problem areas and design innovative technical solutions","Contribute to proposal creation and solution design","Configure solutions and support deployment activities","Conduct POCs, proof-of-technology workshops, and solution demonstrations","Develop value-creating strategies and analytical models for clients"],"secondary":["Assist in effort estimation and budgeting aligned with financial guidelines","Collaborate with cross-functional teams and clients","Lead small projects and contribute to organizational initiatives","Assess existing client processes and suggest technology improvements"],"time_allocation":{"coding":40,"mentoring":10,"meetings":50}},"requirements":{"must_have":{"experience_years":null,"technical_skills":["Python","Machine Learning","Software Configuration Management"],"domain_expertise":["Machine Learning Solutions","Consulting Delivery","Digital Transformation"],"soft_skills":["problem-solving","logical thinking","communication","client-interfacing","collaboration"]},"nice_to_have":{"education":[],"technical_skills":["Data Modeling","POC Development","Industry Tools and Frameworks"],"domain_expertise":["Industry domain knowledge","Financial project models","Pricing strategies"]},"deal_breakers":["Lack of Python or machine learning knowledge","Poor client communication skills"]},"technical_assessment_focus":{"coding":["Python-based machine learning implementation","Model design and optimization"],"system_design":["End-to-end ML solution design","POC and prototype architecture"],"domain_knowledge":["Machine learning fundamentals","Software configuration management","Industry technology trends"],"problem_solving":["Client issue diagnosis","Process improvement identification","Solution trade-off analysis"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":true,"target_percent":null}},"logistics":{"location":null,"remote_policy":"onsite / hybrid","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Managerial / client-facing discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Delivery Manager"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Machine Learning Engineer / Consultant role at Infosys. Python and machine learning focused consulting role involving client problem diagnosis, solution design, POCs, and deployment for digital transformation initiatives.","semantic_tags":["machine-learning","python","consulting","digital-transformation","ml-solutions","client-facing","mid-level"]}}
{"id":"job_exl_associate_data_scientist_001","role":{"title":"Associate Data Scientist","level":"entry","department":"Data Science & Analytics","team":"Analytics","reports_to":"Senior Data Scientist"},"company":{"name":"EXL","size":"10000+","industry":"Business Consulting and Services","culture_keywords":["collaboration","outcomes-driven","innovation","learning"]},"description":{"summary":"Assist senior data scientists in analyzing data, performing preprocessing, and generating data-driven insights using basic statistical and analytical techniques.","key_value_proposition":"Gain hands-on experience in data science within a global consulting organization focused on analytics-led business outcomes.","full_text":"Associate Data Scientists assist in analyzing data and developing data-driven insights. Responsibilities include data collection, preprocessing, and basic statistical analysis while working under the guidance of senior data scientists."},"responsibilities":{"primary":["Collect and preprocess data for analysis","Perform basic statistical analysis and exploratory data analysis","Support development of data-driven insights","Assist in reporting and documentation of analytical findings"],"secondary":["Support internal communications with analytical outputs","Collaborate with team members on analytics tasks","Follow programming and data consistency standards","Work under guidance of senior data scientists"],"time_allocation":{"coding":40,"mentoring":30,"meetings":30}},"requirements":{"must_have":{"experience_years":0,"technical_skills":["Data Analysis","Basic Statistics","Programming Fundamentals","Data Visualization"],"domain_expertise":["Analytics Support","Reporting","Data Preparation"],"soft_skills":["attention to detail","interpersonal skills","working under pressure","results orientation"]},"nice_to_have":{"education":[],"technical_skills":["D3 Data Visualization","Documentation Tools"],"domain_expertise":["Business analytics","Internal communications"]},"deal_breakers":["No basic data analysis or statistical knowledge","Inability to follow programming standards"]},"technical_assessment_focus":{"coding":["Basic data analysis scripts","Data preprocessing tasks"],"system_design":["Simple analytics workflows","Reporting pipelines"],"domain_knowledge":["Statistical methods","Data consistency principles","Visualization fundamentals"],"problem_solving":["Data quality issues","Basic analytical problem solving"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":null,"remote_policy":"onsite / hybrid","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical / analytical interview","Managerial discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Senior Data Scientist"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"low","last_updated":null},"embedding_optimization":{"primary_text":"Associate Data Scientist role at EXL. Entry-level analytics position focused on data collection, preprocessing, basic statistical analysis, reporting, and visualization support in a consulting environment.","semantic_tags":["associate-data-scientist","entry-level","data-analysis","statistics","analytics-support","consulting","visualization"]}}
{"id":"job_capgemini_digital_manufacturing_001","role":{"title":"Digital Manufacturing Engineer","level":"mid","department":"Engineering & Digital Manufacturing","team":"Digital Continuity","reports_to":"Project / Engineering Manager"},"company":{"name":"Capgemini","size":"10000+","industry":"IT Services and IT Consulting","culture_keywords":["innovation","collaboration","client-focus","transformation"]},"description":{"summary":"Develop and deploy industrial digital technologies such as PLM and MES to ensure digital continuity across engineering, manufacturing, and supply chain operations.","key_value_proposition":"Work on enterprise-scale digital manufacturing and continuity solutions for global clients using modern industrial digital technologies.","full_text":"This role involves the development and application of engineering practices in defining, configuring, and deploying industrial digital technologies for managing continuity of information across the engineering enterprise."},"responsibilities":{"primary":["Develop and deploy industrial digital technologies such as PLM and MES","Ensure digital continuity across design, industrialization, manufacturing, and supply chain","Manage manufacturing and engineering data","Interpret client needs and translate them into digital solutions","Work independently or with minimal supervision on assigned tasks"],"secondary":["Provide guidance and support to team members","Identify problems and propose solutions in straightforward situations","Collaborate with cross-functional teams and interact with customers","Contribute to teamwork and delivery excellence"],"time_allocation":{"coding":45,"mentoring":15,"meetings":40}},"requirements":{"must_have":{"experience_years":null,"technical_skills":["PLM","MES","Industrial Digital Technologies","Manufacturing Data Management"],"domain_expertise":["Digital Manufacturing","Digital Continuity","Engineering Enterprise Systems"],"soft_skills":["problem-solving","communication","collaboration","customer-interaction"]},"nice_to_have":{"education":[],"technical_skills":["Enterprise Systems Integration","Manufacturing IT Systems"],"domain_expertise":["Supply Chain Systems","Industrialization Processes"]},"deal_breakers":["No exposure to digital manufacturing or industrial systems"]},"technical_assessment_focus":{"coding":["Configuration and deployment of industrial digital systems"],"system_design":["Digital continuity architecture","Manufacturing data flow design"],"domain_knowledge":["PLM and MES concepts","Manufacturing and supply chain processes"],"problem_solving":["Identifying data continuity issues","Resolving manufacturing system challenges"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":null,"remote_policy":"onsite / hybrid","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Managerial discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Technical Lead"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Digital Manufacturing Engineer role at Capgemini focused on PLM, MES, and digital continuity across engineering, manufacturing, and supply chain using industrial digital technologies.","semantic_tags":["digital-manufacturing","plm","mes","digital-continuity","industrial-systems","engineering","consulting"]}}
{"id":"job_skillzenloop_python_developer_001","role":{"title":"Python Developer","level":"entry","department":"Engineering","team":"Backend Development","reports_to":"Engineering Lead"},"company":{"name":"SkillzenLoop","size":"11-50","industry":"IT Services and IT Consulting","culture_keywords":["learning","collaboration","growth","innovation"]},"description":{"summary":"Develop and maintain backend applications using Python, working on APIs, web services, and real-world client projects in a remote environment.","key_value_proposition":"Gain hands-on experience with real-world Python projects, mentorship, and clear growth paths into senior or full-stack roles.","full_text":"We are seeking a motivated Python Developer with a strong foundation in Python programming to work on backend development, API creation, database integration, and performance optimization."},"responsibilities":{"primary":["Develop, test, and maintain backend applications using Python","Build APIs and web services using Django, Flask, or FastAPI","Write clean, maintainable, and reusable code","Integrate applications with relational and NoSQL databases","Debug and resolve performance and functional issues"],"secondary":["Collaborate with frontend developers and designers","Participate in software planning, documentation, and deployment","Follow coding standards and best practices","Contribute to continuous improvement of development processes"],"time_allocation":{"coding":70,"mentoring":15,"meetings":15}},"requirements":{"must_have":{"experience_years":0,"technical_skills":["Python","Object-Oriented Programming","REST APIs","SQL","Git"],"domain_expertise":["Backend Development","API Development","Database Integration"],"soft_skills":["analytical thinking","problem-solving","communication","collaboration"]},"nice_to_have":{"education":["Bachelor’s degree in Computer Science or IT"],"technical_skills":["Django","Flask","FastAPI","Docker","AWS","Azure","JavaScript","HTML","CSS"],"domain_expertise":["Agile methodologies","Cloud-based deployments"]},"deal_breakers":["No basic Python programming knowledge","Lack of understanding of databases or APIs"]},"technical_assessment_focus":{"coding":["Python fundamentals","API development","Database queries"],"system_design":["Basic backend architecture","API-based application design"],"domain_knowledge":["OOP concepts","Web frameworks","Version control"],"problem_solving":["Debugging backend issues","Optimizing application performance"]},"compensation":{"salary_range":{"min":350000,"max":350000,"currency":"INR","target":350000},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":"Remote","remote_policy":"remote","required_onsite_days":0,"relocation_assistance":false,"visa_sponsorship":false},"interview_process":{"stages":["HR screening","Technical interview","Practical / coding round"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Engineering Lead"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Entry-level Python Developer role at SkillzenLoop. Remote position offering ₹3.5 LPA. Focus on backend development, APIs, Python frameworks, databases, and real-world project experience.","semantic_tags":["python-developer","entry-level","backend-development","api","remote","django","flask","fastapi"]}}
{"id":"job_genpact_senior_principal_databricks_001","role":{"title":"Senior Principal Consultant – Databricks Developer","level":"senior","department":"Advanced Analytics & AI","team":"Data Engineering / Databricks","reports_to":"Architect / Program Lead"},"company":{"name":"Genpact","size":"10000+","industry":"Advanced Technology Services and Consulting","culture_keywords":["innovation","curiosity","integrity","AI-first","inclusion"]},"description":{"summary":"Design, build, and optimize enterprise-scale Databricks Lakehouse solutions to support AI-first data engineering and analytics initiatives.","key_value_proposition":"Lead AI-driven data engineering transformations using Databricks and cloud-native technologies in Genpact’s AI Gigafactory ecosystem.","full_text":"Inviting applications for the role of Senior Principal Consultant – Databricks Developer. The role focuses on solving real-world data engineering problems using Databricks, cloud platforms, and modern Lakehouse architectures to meet functional and non-functional requirements."},"responsibilities":{"primary":["Design and implement enterprise-scale Databricks Lakehouse solutions","Develop complex batch and streaming data pipelines using Spark and PySpark","Build and optimize ETL workflows using Azure Data Factory and Databricks","Implement Delta Lake, Unity Catalog, and Databricks Workflows","Optimize performance and reduce cost through Spark and SQL tuning","Collaborate with architects and lead engineers to meet functional and non-functional requirements"],"secondary":["Integrate Databricks with tools such as DBT and Snowflake","Contribute to CI/CD pipelines for Databricks jobs","Stay current with emerging technologies and industry trends","Mentor team members and contribute to technical excellence","Participate in cloud migration and unified data platform initiatives"],"time_allocation":{"coding":60,"mentoring":15,"meetings":25}},"requirements":{"must_have":{"experience_years":null,"technical_skills":["Python","PySpark","Apache Spark","Spark SQL","SQL","Azure Data Factory","Azure Databricks","Delta Lake","Unity Catalog","ETL"],"domain_expertise":["Data Engineering","Databricks Lakehouse","Batch and Streaming Pipelines","Cloud Data Platforms"],"soft_skills":["analytical thinking","problem-solving","communication","team collaboration","learning mindset"]},"nice_to_have":{"education":["Bachelor’s degree in Computer Science or Engineering"],"technical_skills":["DBT","Databricks SQL Endpoints","CI/CD","Docker","Kubernetes","Snowflake"],"domain_expertise":["Data governance","Unified data platform migration","AI and analytics platforms"]},"deal_breakers":["No hands-on Databricks project experience","Lack of Spark or PySpark expertise"]},"technical_assessment_focus":{"coding":["PySpark and Spark SQL development","Custom ETL script development","Unit and integration testing"],"system_design":["Databricks Lakehouse architecture","Batch and streaming data pipeline design","Cloud-native data platforms"],"domain_knowledge":["Spark and Hive frameworks","Delta Lake and Unity Catalog","Cloud services on Azure/AWS/GCP"],"problem_solving":["Performance optimization","Cost optimization","Complex pipeline troubleshooting"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":true,"target_percent":null}},"logistics":{"location":null,"remote_policy":"onsite / hybrid","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Architectural / managerial discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Architect","Technical Panel"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"high","last_updated":null},"embedding_optimization":{"primary_text":"Senior Principal Consultant – Databricks Developer role at Genpact. Enterprise data engineering role focused on Databricks Lakehouse, Spark, PySpark, Azure Data Factory, Delta Lake, Unity Catalog, and cloud-native batch and streaming pipelines.","semantic_tags":["databricks","lakehouse","data-engineering","spark","pyspark","azure","delta-lake","unity-catalog","senior-consultant","ai-platforms"]}}
{"id":"job_azure_databricks_developer_001","role":{"title":"Azure Databricks Developer","level":"senior","department":"Data & Analytics","team":"Databricks / Data Engineering","reports_to":"Data Engineering Manager"},"company":{"name":null,"size":null,"industry":"Technology / Data Services","culture_keywords":["innovation","performance","scalability","learning"]},"description":{"summary":"Design and deliver cost-effective Azure Databricks solutions using Python, Delta Lake, and SQL to support scalable, secure, and high-performance data platforms.","key_value_proposition":"Lead the development and optimization of enterprise Databricks solutions with a strong focus on performance, scalability, and business impact.","full_text":"As an Azure Databricks Developer, the role involves providing innovative and cost-effective solutions using Databricks and Python, optimizing resources, and building scalable, secure, and high-availability data platforms aligned with business objectives."},"responsibilities":{"primary":["Design and implement Azure Databricks solutions using Delta Lake","Develop and optimize ETL pipelines to meet business requirements","Optimize resource usage to improve performance and reduce cost","Develop solutions aligned with business objectives and procedures","Work with relational databases and data warehouses using SQL"],"secondary":["Build operational excellence practices for scalable and secure platforms","Continuously learn and adapt to new technologies","Support high availability and performance of data systems","Collaborate with stakeholders to understand business needs"],"time_allocation":{"coding":60,"mentoring":15,"meetings":25}},"requirements":{"must_have":{"experience_years":7,"technical_skills":["Azure Databricks","Delta Lake","Python","SQL","ETL"],"domain_expertise":["Data Engineering","Cloud Data Platforms","Data Warehousing"],"soft_skills":["analytical thinking","problem-solving","adaptability","communication"]},"nice_to_have":{"education":[],"technical_skills":["Azure Architecture","Data Modeling","Performance Optimization"],"domain_expertise":["Operational excellence","Scalable platform design"]},"deal_breakers":["No hands-on Azure Databricks experience","Lack of Delta Lake or SQL expertise"]},"technical_assessment_focus":{"coding":["Python scripting","Databricks ETL development","SQL query optimization"],"system_design":["Azure Databricks architecture","Delta Lake-based data platforms","Scalable and secure data solutions"],"domain_knowledge":["ETL methodologies","Azure architecture and design","Data warehouse concepts"],"problem_solving":["Performance tuning","Resource optimization","Production issue resolution"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":null,"remote_policy":"onsite / hybrid","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Managerial discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Technical Panel"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Senior Azure Databricks Developer role requiring 7–10 years of experience with Databricks Delta Lake, Azure cloud, Python, SQL, and ETL for building scalable and cost-effective enterprise data solutions.","semantic_tags":["azure-databricks","delta-lake","data-engineering","python","sql","etl","cloud-data","senior-level"]}}
{"id":"job_azure_databricks_data_engineer_002","role":{"title":"Azure Databricks Data Engineer","level":"mid","department":"Data & Analytics","team":"Data Engineering","reports_to":"Data Engineering Manager"},"company":{"name":null,"size":null,"industry":"Technology / Data Services","culture_keywords":["collaboration","innovation","performance","delivery"]},"description":{"summary":"Design, develop, and optimize Azure-based data solutions using Databricks, Azure Data Factory, SQL, Python, and SAP IS Auto.","key_value_proposition":"Work on enterprise-grade Azure data platforms integrating SAP systems with modern cloud data engineering practices.","full_text":"As an Azure Databricks Data Engineer, the role involves data modelling, ETL/ELT pipeline development, and integration of SAP IS Auto with Azure cloud data services such as Databricks and Azure Synapse."},"responsibilities":{"primary":["Design data models and systematic data layers","Develop and optimize ETL/ELT pipelines using Azure Data Factory and Databricks","Integrate SAP IS Auto data with cloud-based data platforms","Perform database management and big data processing","Develop data solutions using Python and SQL"],"secondary":["Collaborate with cross-functional teams","Support CI/CD practices for data pipelines","Ensure performance, reliability, and scalability of data solutions"],"time_allocation":{"coding":60,"mentoring":10,"meetings":30}},"requirements":{"must_have":{"experience_years":4,"technical_skills":["Azure Data Factory","Azure Databricks","Azure Synapse","Python","SQL","ETL/ELT"],"domain_expertise":["Data Engineering","Cloud Data Platforms","SAP IS Auto Integration"],"soft_skills":["problem-solving","collaboration","communication"]},"nice_to_have":{"education":[],"technical_skills":["CI/CD","Big Data Processing Tools"],"domain_expertise":["Enterprise data modeling","Layered data architecture"]},"deal_breakers":["No hands-on Azure Databricks experience","Lack of SAP IS Auto exposure"]},"technical_assessment_focus":{"coding":["Python data processing","SQL query optimization","ETL pipeline development"],"system_design":["Azure data architecture","Layered data modeling","SAP-to-cloud integration design"],"domain_knowledge":["ETL/ELT methodologies","Azure data services","SAP IS Auto data handling"],"problem_solving":["Pipeline optimization","Data integration issues","Production troubleshooting"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":"Sector V, Kolkata, India","remote_policy":"onsite","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Managerial discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Technical Panel"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Azure Databricks Data Engineer role requiring 4–7 years of experience. Skills include Azure Data Factory, Databricks, Azure Synapse, Python, SQL, ETL/ELT, and SAP IS Auto. Location: Sector V, Kolkata.","semantic_tags":["azure-databricks","data-engineering","azure-data-factory","azure-synapse","python","sql","sap-is-auto","etl"]}}
{"id":"job_databricks_data_engineer_003","role":{"title":"Databricks Data Engineer","level":"senior","department":"Data & Analytics","team":"Data Engineering","reports_to":"Data Engineering Lead / Architect"},"company":{"name":null,"size":null,"industry":"Technology / Data Services","culture_keywords":["scalability","engineering-excellence","collaboration","mentorship"]},"description":{"summary":"Design, build, and optimize scalable data engineering solutions using Databricks, PySpark, AWS, and Azure data services to support enterprise analytics and data platforms.","key_value_proposition":"Lead complex, large-scale data engineering initiatives leveraging Databricks and cloud platforms while mentoring teams and driving best practices.","full_text":"The role requires deep technical expertise in Databricks, PySpark, AWS, and Azure data services to design and deliver scalable batch and streaming data pipelines, optimize performance, and support enterprise-wide analytics initiatives."},"responsibilities":{"primary":["Design and develop scalable data pipelines using Databricks, PySpark, and cloud services","Build and optimize batch ETL pipelines; streaming pipelines are a plus","Develop solutions using Azure Synapse, Azure Data Factory, and SQL Server","Implement dimensional data models, data warehousing, and governance practices","Optimize performance and cost across data platforms","Perform root cause analysis on data and pipeline issues"],"secondary":["Mentor and coach data engineering team members","Collaborate with business analysts and solution architects on enterprise initiatives","Conduct design reviews and code reviews","Develop and maintain data engineering standards and documentation","Foster reuse, scalability, stability, and operational efficiency"],"time_allocation":{"coding":55,"mentoring":20,"meetings":25}},"requirements":{"must_have":{"experience_years":5,"technical_skills":["Databricks","PySpark","Python","Scala","SQL","Azure Synapse","Azure Data Factory","AWS","ETL"],"domain_expertise":["Data Engineering","Cloud Data Platforms","Data Warehousing","Dimensional Modeling"],"soft_skills":["problem-solving","mentorship","communication","collaboration"]},"nice_to_have":{"education":[],"technical_skills":["Boomi","CI/CD","Streaming Pipelines"],"domain_expertise":["Data governance","Enterprise data architecture"]},"deal_breakers":["Less than 5 years Databricks experience","Lack of PySpark or SQL expertise"]},"technical_assessment_focus":{"coding":["PySpark development","SQL and T-SQL query optimization","ETL pipeline implementation"],"system_design":["Enterprise data architecture","Batch and streaming pipeline design","Scalable cloud data platforms"],"domain_knowledge":["Dimensional modeling","Data warehousing","Azure and AWS data services"],"problem_solving":["Root cause analysis","Performance tuning","Complex data issue resolution"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":"Kolkata, India","remote_policy":"onsite","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Architectural / leadership discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Data Architect","Technical Panel"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Senior Databricks Data Engineer role in Kolkata requiring 5+ years experience with Databricks, PySpark, AWS, Azure Synapse, Azure Data Factory, SQL, and enterprise data warehousing. Focus on scalable batch pipelines, mentoring, and performance optimization.","semantic_tags":["databricks","data-engineering","pyspark","azure","aws","synapse","azure-data-factory","sql","senior-level"]}}
{"id":"job_pyspark_developer_004","role":{"title":"PySpark Developer","level":"senior","department":"Data & Analytics","team":"Data Engineering","reports_to":"Data Engineering Manager"},"company":{"name":null,"size":null,"industry":"Technology / Data Services","culture_keywords":["scalability","data-quality","collaboration","engineering-excellence"]},"description":{"summary":"Design, develop, and optimize scalable ETL/ELT data pipelines using PySpark, Python, and AWS cloud services for large-scale data processing and analytics.","key_value_proposition":"Work on distributed data engineering systems handling large and complex datasets with strong focus on performance, data quality, and analytics readiness.","full_text":"The PySpark Developer role involves building robust ETL/ELT pipelines, optimizing Spark jobs, working with AWS data services, and collaborating with cross-functional teams to deliver high-quality, analysis-ready datasets."},"responsibilities":{"primary":["Design and develop scalable ETL/ELT pipelines using PySpark","Build reusable and parameterized Spark jobs for batch and micro-batch processing","Transform raw data into analysis-ready datasets and data marts","Optimize PySpark performance for large and complex datasets","Ensure data quality, consistency, lineage, and documentation","Work with AWS services such as S3, Glue, EMR, and Redshift"],"secondary":["Collaborate with data architects, data modelers, and data scientists","Support CI/CD practices and version control workflows","Implement ingestion frameworks for structured, semi-structured, and unstructured data","Contribute to analytics-driven data architecture"],"time_allocation":{"coding":65,"mentoring":10,"meetings":25}},"requirements":{"must_have":{"experience_years":5,"technical_skills":["Python","PySpark","Apache Spark","Amazon Redshift","PostgreSQL","AWS S3","ETL/ELT","SQL"],"domain_expertise":["Data Engineering","Distributed Data Processing","Data Lake and Data Warehouse Architecture"],"soft_skills":["problem-solving","collaboration","communication","attention to detail"]},"nice_to_have":{"education":[],"technical_skills":["AWS Glue","AWS EMR","AWS Lambda","Delta Lake","Kafka","Spark Structured Streaming","CI/CD","Jenkins","Git"],"domain_expertise":["Data governance","Metadata and lineage management","Streaming data pipelines"]},"deal_breakers":["Lack of PySpark/Spark experience","No hands-on SQL or AWS data platform experience"]},"technical_assessment_focus":{"coding":["PySpark job development","Advanced SQL queries","Python data processing"],"system_design":["ETL/ELT pipeline architecture","AWS-based data platforms","Batch and micro-batch processing systems"],"domain_knowledge":["Data modeling concepts","Data lake and warehouse design","Distributed computing fundamentals"],"problem_solving":["Spark performance tuning","Data quality issues","Pipeline failure troubleshooting"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":"Chennai / Hyderabad / Kolkata, India","remote_policy":"onsite","required_onsite_days":5,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Managerial discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Technical Panel"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"high","last_updated":null},"embedding_optimization":{"primary_text":"Senior PySpark Developer role requiring 5+ years of data engineering experience. Skills include PySpark, Python, AWS (S3, Glue, EMR, Redshift), SQL, ETL/ELT, and distributed data processing. Locations: Chennai, Hyderabad, Kolkata.","semantic_tags":["pyspark","data-engineering","aws","redshift","sql","etl","distributed-systems","senior-level"]}}
{"id":"job_ust_spark_databricks_developer_005","role":{"title":"Spark & Databricks Developer","level":"mid","department":"Data & Analytics","team":"Data Engineering","reports_to":"Data Engineering Manager"},"company":{"name":"UST","size":"30000+","industry":"Digital Transformation & IT Services","culture_keywords":["innovation","agility","collaboration","purpose-driven"]},"description":{"summary":"Design, develop, and optimize big data pipelines using Apache Spark and Databricks to deliver scalable, high-performance data solutions.","key_value_proposition":"Work on long-term, enterprise-scale big data projects using Spark and Databricks within a global digital transformation organization.","full_text":"As a Spark & Databricks Developer at UST, you will design, develop, and optimize big data pipelines using Apache Spark, build and maintain scalable data solutions on Databricks, collaborate with cross-functional teams, and ensure data quality, performance tuning, and job optimization."},"responsibilities":{"primary":["Design, develop, and optimize big data pipelines using Apache Spark","Build and maintain scalable data solutions on Databricks","Ensure data quality, performance tuning, and job optimization","Develop Spark workflows for batch and streaming processing","Write and optimize SQL and Python-based data transformations"],"secondary":["Collaborate with cross-functional teams for data integration and transformation","Manage version control and collaboration using GitHub","Participate in code reviews, testing, and documentation","Support best practices for scalable and maintainable data solutions"],"time_allocation":{"coding":60,"mentoring":10,"meetings":30}},"requirements":{"must_have":{"experience_years":4,"technical_skills":["Apache Spark","Databricks","Python","SQL","GitHub"],"domain_expertise":["Data Engineering","Big Data Processing","Data Lake and Data Warehouse Architecture"],"soft_skills":["problem-solving","collaboration","communication","attention to detail"]},"nice_to_have":{"education":[],"technical_skills":["Spark Streaming","Performance Tuning Tools","CI/CD"],"domain_expertise":["Streaming data pipelines","Enterprise data platforms"]},"deal_breakers":["No hands-on Apache Spark experience","Lack of Databricks or SQL proficiency"]},"technical_assessment_focus":{"coding":["Spark batch and streaming development","Python scripting","SQL query optimization"],"system_design":["Spark and Databricks-based data architectures","Scalable data pipeline design"],"domain_knowledge":["Data lake and warehouse concepts","Distributed data processing"],"problem_solving":["Performance optimization","Data quality and pipeline failures"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":null,"remote_policy":"onsite / hybrid","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Managerial discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Technical Panel"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Spark & Databricks Developer role at UST requiring 4–7 years of experience. Skills include Apache Spark (batch & streaming), Databricks, Python, SQL, GitHub, and enterprise data lake/warehouse architectures.","semantic_tags":["spark","databricks","data-engineering","python","sql","big-data","batch-processing","streaming","mid-level"]}}
{"id":"job_aws_pyspark_databricks_006","role":{"title":"AWS PySpark Databricks Developer","level":"senior","department":"Data & Analytics","team":"Data Engineering","reports_to":"Data Engineering Manager"},"company":{"name":null,"size":null,"industry":"Technology / Data Services","culture_keywords":["cloud-first","scalability","engineering-excellence","governance"]},"description":{"summary":"Design, develop, and optimize cloud-native data engineering solutions using AWS, Databricks, PySpark, and Delta Lake to support scalable analytics platforms.","key_value_proposition":"Build modern data lakehouse solutions on AWS using Databricks and PySpark with a strong focus on performance, governance, and CI/CD best practices.","full_text":"The AWS PySpark Databricks Developer role focuses on building and optimizing scalable data pipelines using Databricks and Apache Spark on AWS, leveraging services such as S3, Glue, EMR, Redshift, and CI/CD tools."},"responsibilities":{"primary":["Develop and optimize data pipelines using PySpark and Databricks","Build cloud-native data solutions on AWS using S3, Glue, EMR, and Redshift","Implement Delta Lake and lakehouse architectures","Develop and tune SQL queries for performance","Ensure data security, governance, and compliance standards"],"secondary":["Integrate orchestration tools such as Airflow or dbt","Support CI/CD pipelines using Jenkins, GitHub Actions, or CodePipeline","Monitor and optimize workloads using CloudWatch","Collaborate with cross-functional teams to deliver scalable data solutions"],"time_allocation":{"coding":60,"mentoring":15,"meetings":25}},"requirements":{"must_have":{"experience_years":5,"technical_skills":["PySpark","Apache Spark","Databricks","AWS S3","AWS Glue","AWS EMR","AWS Redshift","Python","SQL","Delta Lake"],"domain_expertise":["Data Engineering","Cloud Data Platforms","Lakehouse Architecture"],"soft_skills":["problem-solving","collaboration","communication","adaptability"]},"nice_to_have":{"education":[],"technical_skills":["Scala","Airflow","dbt","CI/CD Tools","AWS Lambda","CloudWatch","IAM"],"domain_expertise":["Data governance","Security and compliance frameworks"]},"deal_breakers":["No hands-on Databricks or Spark experience","Lack of AWS data platform expertise"]},"technical_assessment_focus":{"coding":["PySpark development","SQL performance tuning","Python scripting"],"system_design":["AWS lakehouse architecture","Databricks-based data platforms","Secure and governed data solutions"],"domain_knowledge":["Delta Lake concepts","AWS services stack","CI/CD for data pipelines"],"problem_solving":["Pipeline optimization","Cost and performance tuning","Security and governance issues"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":"Visakhapatnam, India","remote_policy":"onsite","required_onsite_days":5,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Managerial discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Technical Panel"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"AWS PySpark Databricks Developer role requiring 5–8 years of experience. Skills include Databricks, Apache Spark, PySpark, AWS (S3, Glue, EMR, Redshift), Delta Lake, SQL, and CI/CD. Location: Visakhapatnam.","semantic_tags":["aws","databricks","pyspark","spark","delta-lake","lakehouse","data-engineering","ci-cd","senior-level"]}}
{"id":"job_jpmc_senior_lead_software_engineer_007","role":{"title":"Senior Lead Software Engineer","level":"senior","department":"Corporate Data & Analytics Office (CDAO)","team":"Data & AI Engineering","reports_to":"Engineering Manager / Tech Lead"},"company":{"name":"JPMorganChase","size":"10000+","industry":"Financial Services","culture_keywords":["engineering-excellence","security","inclusion","innovation"]},"description":{"summary":"Lead the design and delivery of secure, scalable, and high-quality software and data platforms, leveraging cloud, Databricks, and modern SDLC practices.","key_value_proposition":"Drive significant business impact by building market-leading, secure technology solutions within a global financial institution.","full_text":"As a Senior Lead Software Engineer at JPMorganChase within the CDAO, you will provide technical leadership, develop production-grade code, influence architecture and design decisions, and contribute to firmwide engineering standards and practices."},"responsibilities":{"primary":["Provide technical guidance and direction to engineering teams and stakeholders","Develop secure, high-quality production code and review code written by others","Drive architectural and design decisions impacting applications and platforms","Act as a subject matter expert in data, cloud, and AI platforms","Advocate firmwide SDLC frameworks, tools, and best practices"],"secondary":["Influence adoption of leading-edge technologies","Support agile delivery, CI/CD, resiliency, and security practices","Contribute to an inclusive and collaborative engineering culture","Participate in code reviews, debugging, and technical mentoring"],"time_allocation":{"coding":50,"mentoring":25,"meetings":25}},"requirements":{"must_have":{"experience_years":5,"technical_skills":["Python","REST APIs","Databricks","Terraform","Cloud Platforms","CI/CD"],"domain_expertise":["Software Engineering","Cloud Solution Development","Data & AI Platforms"],"soft_skills":["technical leadership","problem-solving","communication","collaboration"]},"nice_to_have":{"education":[],"technical_skills":["Apache Spark","Infrastructure Observability","SCIM","SSO","Front-end Technologies"],"domain_expertise":["Security and resiliency engineering","AI and machine learning platforms"]},"deal_breakers":["Lack of hands-on Python or cloud experience","No experience with production-grade software systems"]},"technical_assessment_focus":{"coding":["Python development","API design and implementation","Code quality and reviews"],"system_design":["Secure and scalable cloud architectures","Databricks platform management","Infrastructure-as-code design"],"domain_knowledge":["Software Development Life Cycle","Agile and CI/CD practices","Application security and resiliency"],"problem_solving":["Complex system debugging","Design trade-off analysis","Production issue resolution"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":true,"target_percent":null}},"logistics":{"location":null,"remote_policy":"onsite / hybrid","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Architectural / leadership discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Engineering Leadership"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Senior Lead Software Engineer role at JPMorganChase (CDAO). 5+ years experience required. Skills include Python, REST APIs, Databricks, Terraform, cloud platforms, CI/CD, and secure enterprise software development.","semantic_tags":["senior-lead-engineer","software-engineering","python","databricks","cloud","terraform","ci-cd","financial-services"]}}
{"id":"job_asyva_azure_databricks_engineer_008","role":{"title":"Azure Databricks Engineer","level":"mid","department":"Data & Analytics","team":"Data Engineering","reports_to":"Data Engineering Lead / Project Manager"},"company":{"name":"ASYVA INFOTECH LLP","size":null,"industry":"IT Services and Data Engineering","culture_keywords":["collaboration","innovation-driven","stability","client-focus"]},"description":{"summary":"Design, develop, and optimize cloud-native data pipelines and analytics solutions using Azure and Databricks for global clients.","key_value_proposition":"Work on long-term, cutting-edge Azure & Databricks data platforms with global clients in a fully remote, contract-based role.","full_text":"ASYVA INFOTECH LLP is looking for a skilled Azure Databricks Engineer to build scalable ETL/ELT pipelines, implement modern data architectures, and optimize performance across Azure and Databricks environments in a long-term contract engagement."},"responsibilities":{"primary":["Design, develop, and maintain ETL/ELT pipelines using Databricks (PySpark, SQL, Python)","Implement and optimize data workflows using Azure services such as Data Factory, Data Lake, Synapse, and Event Hub","Develop data models using Bronze-Silver-Gold architecture","Optimize Spark job performance and cost","Ensure data quality, governance, and security"],"secondary":["Collaborate with data analysts and Power BI developers for reporting solutions","Integrate CI/CD pipelines using Azure DevOps or GitHub Actions","Document technical processes and participate in code reviews","Engage in agile ceremonies and cross-functional collaboration"],"time_allocation":{"coding":65,"mentoring":10,"meetings":25}},"requirements":{"must_have":{"experience_years":3,"technical_skills":["Azure Databricks","PySpark","Python","SQL","PL/SQL","Azure Data Factory","Azure Data Lake","Azure Synapse","Delta Lake"],"domain_expertise":["Data Engineering","Cloud Data Platforms","ETL/ELT Frameworks"],"soft_skills":["analytical thinking","debugging","communication","collaboration"]},"nice_to_have":{"education":[],"technical_skills":["Power BI","dbt","Snowflake","Apache Airflow","Azure Functions","Event Driven Architectures","Docker","Kubernetes","CI/CD"],"domain_expertise":["Data Vault 2.0","DevOps best practices"]},"deal_breakers":["Lack of hands-on Azure Databricks experience","No PySpark or SQL performance optimization experience"]},"technical_assessment_focus":{"coding":["PySpark and Python development","SQL and PL/SQL optimization","ETL pipeline implementation"],"system_design":["Azure Databricks architecture","Bronze-Silver-Gold data modeling","Secure and governed data platforms"],"domain_knowledge":["Delta Lake concepts","Spark architecture","Azure data services"],"problem_solving":["Performance tuning","Data quality issues","Pipeline failure troubleshooting"]},"compensation":{"salary_range":{"min":null,"max":106516.53,"currency":"INR","target":106516.53},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":"Remote","remote_policy":"remote","required_onsite_days":0,"relocation_assistance":false,"visa_sponsorship":false},"interview_process":{"stages":["HR screening","Technical interview","Client / managerial discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Technical Lead"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"contract","requisition_id":null,"hiring_urgency":"high","last_updated":null},"embedding_optimization":{"primary_text":"Azure Databricks Engineer contract role at ASYVA INFOTECH LLP. 3+ years experience required. Skills include Azure Databricks, PySpark, Python, SQL/PLSQL, Azure Data Factory, Synapse, Delta Lake, CI/CD. Fully remote with monthly pay up to ₹106,516.","semantic_tags":["azure-databricks","data-engineering","pyspark","delta-lake","azure-data-factory","ci-cd","remote","contract-role"]}}
{"id":"job_birlasoft_senior_databricks_powerbi_009","role":{"title":"Senior Databricks Developer with Power BI Expertise","level":"senior","department":"Data & Analytics","team":"Data Engineering & BI","reports_to":"Data Engineering Manager / Program Manager"},"company":{"name":"Birlasoft Limited","size":"10000+","industry":"IT Services and Consulting","culture_keywords":["client-focus","collaboration","analytics-driven","enterprise-delivery"]},"description":{"summary":"Lead enterprise data engineering and visualization initiatives using Databricks, Azure Synapse, advanced SQL, and Power BI to deliver scalable analytics solutions.","key_value_proposition":"Drive high-impact data engineering and BI solutions for enterprise clients by combining Databricks-based pipelines with advanced Power BI analytics.","full_text":"Birlasoft Limited is seeking a Senior Databricks Developer with Power BI expertise to design, implement, and manage enterprise-grade data pipelines, perform advanced SQL analytics, and deliver actionable insights through Power BI dashboards while collaborating closely with clients and cross-functional teams."},"responsibilities":{"primary":["Design, implement, and manage data pipelines using Azure Data Factory","Develop scalable data processing solutions using Databricks and Azure Synapse Analytics","Apply advanced SQL for data manipulation, querying, and performance optimization","Design and maintain data models, ETL processes, and data warehousing solutions","Create and manage Power BI reports and dashboards to deliver actionable insights"],"secondary":["Collaborate with clients to gather data requirements and provide solutions","Provide training and support to users on data platforms and BI tools","Develop and maintain technical documentation","Collaborate with cross-functional teams to deliver high-quality data solutions"],"time_allocation":{"coding":50,"mentoring":20,"meetings":30}},"requirements":{"must_have":{"experience_years":10,"technical_skills":["Databricks","Advanced SQL","Azure Data Factory","Azure Synapse Analytics","Power BI","ETL"],"domain_expertise":["Data Engineering","Data Warehousing","Business Intelligence"],"soft_skills":["communication","problem-solving","analytical thinking","client-interaction"]},"nice_to_have":{"education":["Bachelor’s degree in Computer Science or Information Technology"],"technical_skills":["Data Modeling","Query Optimization","Azure Certifications","Databricks Certifications"],"domain_expertise":["Enterprise analytics platforms"]},"deal_breakers":["Lack of hands-on Databricks experience","No Power BI or SQL expertise"]},"technical_assessment_focus":{"coding":["SQL query optimization","Databricks pipeline development","ETL implementation"],"system_design":["Azure-based data architecture","Enterprise data warehousing","BI-enabled analytics platforms"],"domain_knowledge":["Databricks and Synapse Analytics","Power BI visualization best practices","ETL and data modeling concepts"],"problem_solving":["Data performance tuning","Client-driven analytics challenges","Pipeline reliability issues"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":"Any Birlasoft Location, India","remote_policy":"onsite","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Client / managerial discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Technical Panel","Client Stakeholders"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Senior Databricks Developer with Power BI Expertise role at Birlasoft Limited. 10–14 years experience required. Skills include Databricks, Azure Data Factory, Azure Synapse Analytics, advanced SQL, Power BI, ETL, and enterprise data engineering.","semantic_tags":["databricks","power-bi","data-engineering","azure-synapse","advanced-sql","enterprise-analytics","senior-level"]}}
{"id":"job_senior_databricks_engineer_010","role":{"title":"Senior Databricks Engineer","level":"senior","department":"Data & Analytics","team":"Data Engineering","reports_to":"Data Engineering Manager / Architect"},"company":{"name":null,"size":null,"industry":"Technology / Healthcare Analytics","culture_keywords":["innovation-driven","collaboration","engineering-excellence","growth"]},"description":{"summary":"Design, build, and optimize scalable AWS-based data pipelines using Databricks, Apache Spark, Python, and SQL to deliver high-quality data products.","key_value_proposition":"Work on cutting-edge data engineering projects with a hybrid work model, leveraging Databricks and AWS to build scalable analytics platforms.","full_text":"The Senior Databricks Engineer role focuses on designing and optimizing scalable data pipelines on AWS using Databricks, Apache Spark, Python, and SQL, while ensuring automated deployments and collaboration across teams."},"responsibilities":{"primary":["Design, build, and optimize scalable data pipelines on AWS","Implement data ingestion, transformation, and integration solutions using Databricks and AWS Glue","Develop and optimize SQL-based transformations and analytics","Manage and optimize cloud storage and compute environments","Implement and maintain CI/CD pipelines using Jenkins and Terraform"],"secondary":["Collaborate with cross-functional teams to deliver data products","Ensure reliability, performance, and scalability of data platforms","Contribute to best practices for cloud data engineering","Support domain-specific analytics initiatives"],"time_allocation":{"coding":60,"mentoring":15,"meetings":25}},"requirements":{"must_have":{"experience_years":6,"technical_skills":["Python","Databricks","Apache Spark","SQL","AWS S3","AWS Glue","AWS Redshift","AWS EMR","AWS Lambda","CI/CD","Jenkins","Terraform"],"domain_expertise":["Data Engineering","Cloud Data Platforms","Big Data Processing"],"soft_skills":["problem-solving","collaboration","communication","ownership"]},"nice_to_have":{"education":[],"technical_skills":[],"domain_expertise":["Healthcare Analytics","Life Sciences Data"]},"deal_breakers":["No hands-on Databricks or Spark experience","Lack of AWS cloud data platform expertise"]},"technical_assessment_focus":{"coding":["PySpark development","SQL optimization","Python scripting"],"system_design":["AWS-based data pipeline architecture","Databricks-driven analytics platforms","Automated CI/CD deployments"],"domain_knowledge":["AWS data services","Databricks and Spark internals","Cloud cost and performance optimization"],"problem_solving":["Pipeline failures","Performance bottlenecks","Cloud infrastructure issues"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":"Hyderabad, India","remote_policy":"hybrid","required_onsite_days":3,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Managerial / architectural discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Technical Architect"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"high","last_updated":null},"embedding_optimization":{"primary_text":"Senior Databricks Engineer role in Hyderabad requiring 6–15 years experience. Skills include Databricks, Apache Spark, Python, SQL, AWS (S3, Glue, Redshift, EMR, Lambda), Jenkins, and Terraform. Hybrid work model.","semantic_tags":["databricks","aws","spark","python","sql","ci-cd","terraform","data-engineering","senior-level","hybrid"]}}
