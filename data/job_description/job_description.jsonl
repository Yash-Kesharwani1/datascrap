{"id":"job_deloitte_data_engineer_001","role":{"title":"Data Engineer","level":"senior","department":"Technology & Transformation","team":"Data Engineering","reports_to":"Project / Engagement Manager"},"company":{"name":"Deloitte","size":"10000+","industry":"Professional Services / Consulting","culture_keywords":["innovation","collaboration","ownership","impact"]},"description":{"summary":"Design, build, and optimize scalable data pipelines and data management solutions using modern big data and cloud technologies.","key_value_proposition":"Work on complex, real-world data engineering problems for global clients using cutting-edge cloud and big data platforms.","full_text":"The Data Engineer will work on data engineering projects for various business units, focusing on delivery of complex data management solutions by leveraging industry best practices."},"responsibilities":{"primary":["Design and build scalable data pipelines and ETL/ELT solutions","Develop data processing solutions using Python and PySpark","Work with Apache Spark, Hadoop, Kafka for large-scale data processing","Deploy and manage data workloads on cloud platforms","Optimize data pipelines and resolve performance bottlenecks","Ensure data quality, reliability, and scalability"],"secondary":["Collaborate with cross-functional and agile teams","Contribute to architectural decisions","Support production systems and resolve incidents","Participate in CoE and CoP community initiatives"],"time_allocation":{"coding":65,"mentoring":10,"meetings":25}},"requirements":{"must_have":{"experience_years":5,"technical_skills":["Python","PySpark","Apache Spark","SQL","ETL/ELT","Databricks","Azure Data Services"],"domain_expertise":["Data Engineering","Big Data Processing","Cloud Data Platforms"],"soft_skills":["problem-solving","communication","ownership","collaboration"]},"nice_to_have":{"education":[],"technical_skills":["AWS","Google Cloud Platform","Kafka","Hadoop","Kubernetes","CI/CD","JavaScript","REST APIs"],"domain_expertise":["High-volume data systems","Real-time data processing","Cloud-native architectures"]},"deal_breakers":["Lack of hands-on Python and PySpark experience","No experience with big data or cloud platforms"]},"technical_assessment_focus":{"coding":["Python and PySpark development","Data pipeline implementation","Unit and integration testing"],"system_design":["End-to-end ETL pipeline design","Cloud data architecture","Scalable data processing systems"],"domain_knowledge":["ETL/ELT concepts","Data warehousing","Performance optimization techniques"],"problem_solving":["Debugging production data issues","Resolving performance bottlenecks","Handling high volume and velocity data"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":true,"target_percent":null}},"logistics":{"location":"Bangalore, India","remote_policy":"onsite / hybrid","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Managerial / project discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Technical Panel"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Senior Data Engineer role at Deloitte in Bangalore. 5+ years experience required. Tech stack includes Python, PySpark, Databricks, Apache Spark, Azure data services. Focus on building scalable data pipelines and cloud-based big data solutions.","semantic_tags":["data-engineering","senior-engineer","python","pyspark","databricks","azure","big-data","etl","cloud-data"]}}
{"id":"job_mrf_data_engineer_001","role":{"title":"Data Engineer","level":"mid","department":"Digital Technology","team":"Data Engineering","reports_to":"Data / Application Manager"},"company":{"name":"MRF Limited","size":"10000+","industry":"Motor Vehicle Parts Manufacturing","culture_keywords":["quality","innovation","customer-focus","reliability"]},"description":{"summary":"Maintain and build Azure-based data engineering solutions, including SQL, Databricks, and real-time pipelines for managed enterprise applications.","key_value_proposition":"Work on large-scale enterprise data platforms supporting critical manufacturing and planning systems.","full_text":"Responsible to maintain the data required for managed applications like Advanced Planning System and Dealer Management System, building cloud-based data pipelines using Azure services and Python."},"responsibilities":{"primary":["Maintain and manage data for enterprise applications","Build and maintain real-time data pipelines from on-prem SAP systems to Azure cloud","Develop data extraction and transformation workflows using Azure Data Factory and Databricks","Work with Azure SQL and NoSQL databases for structured and unstructured data","Develop Python models and improve them through containerization","Manage user access for data upload and download"],"secondary":["Collaborate with functional teams for structured data management","Conduct exploratory data analysis for insights","Monitor and troubleshoot Azure data pipeline performance","Support data migration between on-prem and cloud environments"],"time_allocation":{"coding":60,"mentoring":5,"meetings":35}},"requirements":{"must_have":{"experience_years":2,"technical_skills":["Python","PySpark","SQL","Azure Data Factory","Azure Data Lake","Databricks","Azure SQL","Kafka"],"domain_expertise":["Data Engineering","Cloud Data Migration","Real-time Data Pipelines"],"soft_skills":["problem-solving","communication","ownership","troubleshooting"]},"nice_to_have":{"education":["BE CS","B.Tech IT","MCA"],"technical_skills":["Azure Machine Learning","Azure Cosmos DB","CI/CD","TensorFlow","Jupyter","Containerization"],"domain_expertise":["SAP data integration","Time-series databases","API development"]},"deal_breakers":["Lack of hands-on Azure data engineering experience","No SQL or Python proficiency"]},"technical_assessment_focus":{"coding":["Python and PySpark development","SQL query optimization","API development"],"system_design":["Azure-based data pipeline architecture","On-prem to cloud data migration","Real-time streaming pipelines"],"domain_knowledge":["ETL/ELT processes","Azure data services","Database performance tuning"],"problem_solving":["Pipeline performance troubleshooting","Production issue resolution","Data migration challenges"]},"compensation":{"salary_range":{"min":"Not Specified","max":"Not Specified","currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":null,"remote_policy":"onsite","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Managerial discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Technical Panel"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Data Engineer role at MRF Limited. 2–3 years experience required. Azure data engineering stack including Data Factory, Databricks, Azure SQL, Kafka, Python, PySpark. Focus on real-time pipelines and cloud data migration.","semantic_tags":["data-engineering","azure","databricks","python","pyspark","sql","kafka","cloud-migration","real-time-data"]}}
{"id":"job_infosys_ml_consultant_001","role":{"title":"Machine Learning Engineer / Consultant","level":"mid","department":"Consulting","team":"Digital Transformation","reports_to":"Project Manager / Delivery Manager"},"company":{"name":"Infosys","size":"10000+","industry":"IT Services & Consulting","culture_keywords":["client-focus","innovation","collaboration","ownership"]},"description":{"summary":"Design and deliver machine learning–driven solutions by diagnosing client problems, designing innovative models, and supporting deployment in large-scale consulting engagements.","key_value_proposition":"Work closely with global clients to design, propose, and deploy ML-driven solutions that enable digital transformation and business growth.","full_text":"As part of the Infosys consulting team, the role involves diagnosing customer issues, designing innovative machine learning solutions, supporting deployment, conducting POCs, and leading small projects to deliver high-quality value to clients."},"responsibilities":{"primary":["Design and implement machine learning solutions using Python","Diagnose client problem areas and design innovative technical solutions","Contribute to proposal creation and solution design","Configure solutions and support deployment activities","Conduct POCs, proof-of-technology workshops, and solution demonstrations","Develop value-creating strategies and analytical models for clients"],"secondary":["Assist in effort estimation and budgeting aligned with financial guidelines","Collaborate with cross-functional teams and clients","Lead small projects and contribute to organizational initiatives","Assess existing client processes and suggest technology improvements"],"time_allocation":{"coding":40,"mentoring":10,"meetings":50}},"requirements":{"must_have":{"experience_years":null,"technical_skills":["Python","Machine Learning","Software Configuration Management"],"domain_expertise":["Machine Learning Solutions","Consulting Delivery","Digital Transformation"],"soft_skills":["problem-solving","logical thinking","communication","client-interfacing","collaboration"]},"nice_to_have":{"education":[],"technical_skills":["Data Modeling","POC Development","Industry Tools and Frameworks"],"domain_expertise":["Industry domain knowledge","Financial project models","Pricing strategies"]},"deal_breakers":["Lack of Python or machine learning knowledge","Poor client communication skills"]},"technical_assessment_focus":{"coding":["Python-based machine learning implementation","Model design and optimization"],"system_design":["End-to-end ML solution design","POC and prototype architecture"],"domain_knowledge":["Machine learning fundamentals","Software configuration management","Industry technology trends"],"problem_solving":["Client issue diagnosis","Process improvement identification","Solution trade-off analysis"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":true,"target_percent":null}},"logistics":{"location":null,"remote_policy":"onsite / hybrid","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Managerial / client-facing discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Delivery Manager"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Machine Learning Engineer / Consultant role at Infosys. Python and machine learning focused consulting role involving client problem diagnosis, solution design, POCs, and deployment for digital transformation initiatives.","semantic_tags":["machine-learning","python","consulting","digital-transformation","ml-solutions","client-facing","mid-level"]}}
{"id":"job_exl_associate_data_scientist_001","role":{"title":"Associate Data Scientist","level":"entry","department":"Data Science & Analytics","team":"Analytics","reports_to":"Senior Data Scientist"},"company":{"name":"EXL","size":"10000+","industry":"Business Consulting and Services","culture_keywords":["collaboration","outcomes-driven","innovation","learning"]},"description":{"summary":"Assist senior data scientists in analyzing data, performing preprocessing, and generating data-driven insights using basic statistical and analytical techniques.","key_value_proposition":"Gain hands-on experience in data science within a global consulting organization focused on analytics-led business outcomes.","full_text":"Associate Data Scientists assist in analyzing data and developing data-driven insights. Responsibilities include data collection, preprocessing, and basic statistical analysis while working under the guidance of senior data scientists."},"responsibilities":{"primary":["Collect and preprocess data for analysis","Perform basic statistical analysis and exploratory data analysis","Support development of data-driven insights","Assist in reporting and documentation of analytical findings"],"secondary":["Support internal communications with analytical outputs","Collaborate with team members on analytics tasks","Follow programming and data consistency standards","Work under guidance of senior data scientists"],"time_allocation":{"coding":40,"mentoring":30,"meetings":30}},"requirements":{"must_have":{"experience_years":0,"technical_skills":["Data Analysis","Basic Statistics","Programming Fundamentals","Data Visualization"],"domain_expertise":["Analytics Support","Reporting","Data Preparation"],"soft_skills":["attention to detail","interpersonal skills","working under pressure","results orientation"]},"nice_to_have":{"education":[],"technical_skills":["D3 Data Visualization","Documentation Tools"],"domain_expertise":["Business analytics","Internal communications"]},"deal_breakers":["No basic data analysis or statistical knowledge","Inability to follow programming standards"]},"technical_assessment_focus":{"coding":["Basic data analysis scripts","Data preprocessing tasks"],"system_design":["Simple analytics workflows","Reporting pipelines"],"domain_knowledge":["Statistical methods","Data consistency principles","Visualization fundamentals"],"problem_solving":["Data quality issues","Basic analytical problem solving"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":null,"remote_policy":"onsite / hybrid","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical / analytical interview","Managerial discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Senior Data Scientist"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"low","last_updated":null},"embedding_optimization":{"primary_text":"Associate Data Scientist role at EXL. Entry-level analytics position focused on data collection, preprocessing, basic statistical analysis, reporting, and visualization support in a consulting environment.","semantic_tags":["associate-data-scientist","entry-level","data-analysis","statistics","analytics-support","consulting","visualization"]}}
{"id":"job_capgemini_digital_manufacturing_001","role":{"title":"Digital Manufacturing Engineer","level":"mid","department":"Engineering & Digital Manufacturing","team":"Digital Continuity","reports_to":"Project / Engineering Manager"},"company":{"name":"Capgemini","size":"10000+","industry":"IT Services and IT Consulting","culture_keywords":["innovation","collaboration","client-focus","transformation"]},"description":{"summary":"Develop and deploy industrial digital technologies such as PLM and MES to ensure digital continuity across engineering, manufacturing, and supply chain operations.","key_value_proposition":"Work on enterprise-scale digital manufacturing and continuity solutions for global clients using modern industrial digital technologies.","full_text":"This role involves the development and application of engineering practices in defining, configuring, and deploying industrial digital technologies for managing continuity of information across the engineering enterprise."},"responsibilities":{"primary":["Develop and deploy industrial digital technologies such as PLM and MES","Ensure digital continuity across design, industrialization, manufacturing, and supply chain","Manage manufacturing and engineering data","Interpret client needs and translate them into digital solutions","Work independently or with minimal supervision on assigned tasks"],"secondary":["Provide guidance and support to team members","Identify problems and propose solutions in straightforward situations","Collaborate with cross-functional teams and interact with customers","Contribute to teamwork and delivery excellence"],"time_allocation":{"coding":45,"mentoring":15,"meetings":40}},"requirements":{"must_have":{"experience_years":null,"technical_skills":["PLM","MES","Industrial Digital Technologies","Manufacturing Data Management"],"domain_expertise":["Digital Manufacturing","Digital Continuity","Engineering Enterprise Systems"],"soft_skills":["problem-solving","communication","collaboration","customer-interaction"]},"nice_to_have":{"education":[],"technical_skills":["Enterprise Systems Integration","Manufacturing IT Systems"],"domain_expertise":["Supply Chain Systems","Industrialization Processes"]},"deal_breakers":["No exposure to digital manufacturing or industrial systems"]},"technical_assessment_focus":{"coding":["Configuration and deployment of industrial digital systems"],"system_design":["Digital continuity architecture","Manufacturing data flow design"],"domain_knowledge":["PLM and MES concepts","Manufacturing and supply chain processes"],"problem_solving":["Identifying data continuity issues","Resolving manufacturing system challenges"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":null,"remote_policy":"onsite / hybrid","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Managerial discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Technical Lead"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Digital Manufacturing Engineer role at Capgemini focused on PLM, MES, and digital continuity across engineering, manufacturing, and supply chain using industrial digital technologies.","semantic_tags":["digital-manufacturing","plm","mes","digital-continuity","industrial-systems","engineering","consulting"]}}
{"id":"job_skillzenloop_python_developer_001","role":{"title":"Python Developer","level":"entry","department":"Engineering","team":"Backend Development","reports_to":"Engineering Lead"},"company":{"name":"SkillzenLoop","size":"11-50","industry":"IT Services and IT Consulting","culture_keywords":["learning","collaboration","growth","innovation"]},"description":{"summary":"Develop and maintain backend applications using Python, working on APIs, web services, and real-world client projects in a remote environment.","key_value_proposition":"Gain hands-on experience with real-world Python projects, mentorship, and clear growth paths into senior or full-stack roles.","full_text":"We are seeking a motivated Python Developer with a strong foundation in Python programming to work on backend development, API creation, database integration, and performance optimization."},"responsibilities":{"primary":["Develop, test, and maintain backend applications using Python","Build APIs and web services using Django, Flask, or FastAPI","Write clean, maintainable, and reusable code","Integrate applications with relational and NoSQL databases","Debug and resolve performance and functional issues"],"secondary":["Collaborate with frontend developers and designers","Participate in software planning, documentation, and deployment","Follow coding standards and best practices","Contribute to continuous improvement of development processes"],"time_allocation":{"coding":70,"mentoring":15,"meetings":15}},"requirements":{"must_have":{"experience_years":0,"technical_skills":["Python","Object-Oriented Programming","REST APIs","SQL","Git"],"domain_expertise":["Backend Development","API Development","Database Integration"],"soft_skills":["analytical thinking","problem-solving","communication","collaboration"]},"nice_to_have":{"education":["Bachelor’s degree in Computer Science or IT"],"technical_skills":["Django","Flask","FastAPI","Docker","AWS","Azure","JavaScript","HTML","CSS"],"domain_expertise":["Agile methodologies","Cloud-based deployments"]},"deal_breakers":["No basic Python programming knowledge","Lack of understanding of databases or APIs"]},"technical_assessment_focus":{"coding":["Python fundamentals","API development","Database queries"],"system_design":["Basic backend architecture","API-based application design"],"domain_knowledge":["OOP concepts","Web frameworks","Version control"],"problem_solving":["Debugging backend issues","Optimizing application performance"]},"compensation":{"salary_range":{"min":350000,"max":350000,"currency":"INR","target":350000},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":"Remote","remote_policy":"remote","required_onsite_days":0,"relocation_assistance":false,"visa_sponsorship":false},"interview_process":{"stages":["HR screening","Technical interview","Practical / coding round"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Engineering Lead"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Entry-level Python Developer role at SkillzenLoop. Remote position offering ₹3.5 LPA. Focus on backend development, APIs, Python frameworks, databases, and real-world project experience.","semantic_tags":["python-developer","entry-level","backend-development","api","remote","django","flask","fastapi"]}}
{"id":"job_genpact_senior_principal_databricks_001","role":{"title":"Senior Principal Consultant – Databricks Developer","level":"senior","department":"Advanced Analytics & AI","team":"Data Engineering / Databricks","reports_to":"Architect / Program Lead"},"company":{"name":"Genpact","size":"10000+","industry":"Advanced Technology Services and Consulting","culture_keywords":["innovation","curiosity","integrity","AI-first","inclusion"]},"description":{"summary":"Design, build, and optimize enterprise-scale Databricks Lakehouse solutions to support AI-first data engineering and analytics initiatives.","key_value_proposition":"Lead AI-driven data engineering transformations using Databricks and cloud-native technologies in Genpact’s AI Gigafactory ecosystem.","full_text":"Inviting applications for the role of Senior Principal Consultant – Databricks Developer. The role focuses on solving real-world data engineering problems using Databricks, cloud platforms, and modern Lakehouse architectures to meet functional and non-functional requirements."},"responsibilities":{"primary":["Design and implement enterprise-scale Databricks Lakehouse solutions","Develop complex batch and streaming data pipelines using Spark and PySpark","Build and optimize ETL workflows using Azure Data Factory and Databricks","Implement Delta Lake, Unity Catalog, and Databricks Workflows","Optimize performance and reduce cost through Spark and SQL tuning","Collaborate with architects and lead engineers to meet functional and non-functional requirements"],"secondary":["Integrate Databricks with tools such as DBT and Snowflake","Contribute to CI/CD pipelines for Databricks jobs","Stay current with emerging technologies and industry trends","Mentor team members and contribute to technical excellence","Participate in cloud migration and unified data platform initiatives"],"time_allocation":{"coding":60,"mentoring":15,"meetings":25}},"requirements":{"must_have":{"experience_years":null,"technical_skills":["Python","PySpark","Apache Spark","Spark SQL","SQL","Azure Data Factory","Azure Databricks","Delta Lake","Unity Catalog","ETL"],"domain_expertise":["Data Engineering","Databricks Lakehouse","Batch and Streaming Pipelines","Cloud Data Platforms"],"soft_skills":["analytical thinking","problem-solving","communication","team collaboration","learning mindset"]},"nice_to_have":{"education":["Bachelor’s degree in Computer Science or Engineering"],"technical_skills":["DBT","Databricks SQL Endpoints","CI/CD","Docker","Kubernetes","Snowflake"],"domain_expertise":["Data governance","Unified data platform migration","AI and analytics platforms"]},"deal_breakers":["No hands-on Databricks project experience","Lack of Spark or PySpark expertise"]},"technical_assessment_focus":{"coding":["PySpark and Spark SQL development","Custom ETL script development","Unit and integration testing"],"system_design":["Databricks Lakehouse architecture","Batch and streaming data pipeline design","Cloud-native data platforms"],"domain_knowledge":["Spark and Hive frameworks","Delta Lake and Unity Catalog","Cloud services on Azure/AWS/GCP"],"problem_solving":["Performance optimization","Cost optimization","Complex pipeline troubleshooting"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":true,"target_percent":null}},"logistics":{"location":null,"remote_policy":"onsite / hybrid","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Architectural / managerial discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Architect","Technical Panel"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"high","last_updated":null},"embedding_optimization":{"primary_text":"Senior Principal Consultant – Databricks Developer role at Genpact. Enterprise data engineering role focused on Databricks Lakehouse, Spark, PySpark, Azure Data Factory, Delta Lake, Unity Catalog, and cloud-native batch and streaming pipelines.","semantic_tags":["databricks","lakehouse","data-engineering","spark","pyspark","azure","delta-lake","unity-catalog","senior-consultant","ai-platforms"]}}
{"id":"job_azure_databricks_developer_001","role":{"title":"Azure Databricks Developer","level":"senior","department":"Data & Analytics","team":"Databricks / Data Engineering","reports_to":"Data Engineering Manager"},"company":{"name":"Not Specified","size":"Not Specified","industry":"Technology / Data Services","culture_keywords":["innovation","performance","scalability","learning"]},"description":{"summary":"Design and deliver cost-effective Azure Databricks solutions using Python, Delta Lake, and SQL to support scalable, secure, and high-performance data platforms.","key_value_proposition":"Lead the development and optimization of enterprise Databricks solutions with a strong focus on performance, scalability, and business impact.","full_text":"As an Azure Databricks Developer, the role involves providing innovative and cost-effective solutions using Databricks and Python, optimizing resources, and building scalable, secure, and high-availability data platforms aligned with business objectives."},"responsibilities":{"primary":["Design and implement Azure Databricks solutions using Delta Lake","Develop and optimize ETL pipelines to meet business requirements","Optimize resource usage to improve performance and reduce cost","Develop solutions aligned with business objectives and procedures","Work with relational databases and data warehouses using SQL"],"secondary":["Build operational excellence practices for scalable and secure platforms","Continuously learn and adapt to new technologies","Support high availability and performance of data systems","Collaborate with stakeholders to understand business needs"],"time_allocation":{"coding":60,"mentoring":15,"meetings":25}},"requirements":{"must_have":{"experience_years":7,"technical_skills":["Azure Databricks","Delta Lake","Python","SQL","ETL"],"domain_expertise":["Data Engineering","Cloud Data Platforms","Data Warehousing"],"soft_skills":["analytical thinking","problem-solving","adaptability","communication"]},"nice_to_have":{"education":[],"technical_skills":["Azure Architecture","Data Modeling","Performance Optimization"],"domain_expertise":["Operational excellence","Scalable platform design"]},"deal_breakers":["No hands-on Azure Databricks experience","Lack of Delta Lake or SQL expertise"]},"technical_assessment_focus":{"coding":["Python scripting","Databricks ETL development","SQL query optimization"],"system_design":["Azure Databricks architecture","Delta Lake-based data platforms","Scalable and secure data solutions"],"domain_knowledge":["ETL methodologies","Azure architecture and design","Data warehouse concepts"],"problem_solving":["Performance tuning","Resource optimization","Production issue resolution"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":null,"remote_policy":"onsite / hybrid","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Managerial discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Technical Panel"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Senior Azure Databricks Developer role requiring 7–10 years of experience with Databricks Delta Lake, Azure cloud, Python, SQL, and ETL for building scalable and cost-effective enterprise data solutions.","semantic_tags":["azure-databricks","delta-lake","data-engineering","python","sql","etl","cloud-data","senior-level"]}}
{"id":"job_databricks_data_engineer_003","role":{"title":"Databricks Data Engineer","level":"senior","department":"Data & Analytics","team":"Data Engineering","reports_to":"Data Engineering Lead / Architect"},"company":{"name":null,"size":null,"industry":"Technology / Data Services","culture_keywords":["scalability","engineering-excellence","collaboration","mentorship"]},"description":{"summary":"Design, build, and optimize scalable data engineering solutions using Databricks, PySpark, AWS, and Azure data services to support enterprise analytics and data platforms.","key_value_proposition":"Lead complex, large-scale data engineering initiatives leveraging Databricks and cloud platforms while mentoring teams and driving best practices.","full_text":"The role requires deep technical expertise in Databricks, PySpark, AWS, and Azure data services to design and deliver scalable batch and streaming data pipelines, optimize performance, and support enterprise-wide analytics initiatives."},"responsibilities":{"primary":["Design and develop scalable data pipelines using Databricks, PySpark, and cloud services","Build and optimize batch ETL pipelines; streaming pipelines are a plus","Develop solutions using Azure Synapse, Azure Data Factory, and SQL Server","Implement dimensional data models, data warehousing, and governance practices","Optimize performance and cost across data platforms","Perform root cause analysis on data and pipeline issues"],"secondary":["Mentor and coach data engineering team members","Collaborate with business analysts and solution architects on enterprise initiatives","Conduct design reviews and code reviews","Develop and maintain data engineering standards and documentation","Foster reuse, scalability, stability, and operational efficiency"],"time_allocation":{"coding":55,"mentoring":20,"meetings":25}},"requirements":{"must_have":{"experience_years":5,"technical_skills":["Databricks","PySpark","Python","Scala","SQL","Azure Synapse","Azure Data Factory","AWS","ETL"],"domain_expertise":["Data Engineering","Cloud Data Platforms","Data Warehousing","Dimensional Modeling"],"soft_skills":["problem-solving","mentorship","communication","collaboration"]},"nice_to_have":{"education":[],"technical_skills":["Boomi","CI/CD","Streaming Pipelines"],"domain_expertise":["Data governance","Enterprise data architecture"]},"deal_breakers":["Less than 5 years Databricks experience","Lack of PySpark or SQL expertise"]},"technical_assessment_focus":{"coding":["PySpark development","SQL and T-SQL query optimization","ETL pipeline implementation"],"system_design":["Enterprise data architecture","Batch and streaming pipeline design","Scalable cloud data platforms"],"domain_knowledge":["Dimensional modeling","Data warehousing","Azure and AWS data services"],"problem_solving":["Root cause analysis","Performance tuning","Complex data issue resolution"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":"Kolkata, India","remote_policy":"onsite","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Architectural / leadership discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Data Architect","Technical Panel"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Senior Databricks Data Engineer role in Kolkata requiring 5+ years experience with Databricks, PySpark, AWS, Azure Synapse, Azure Data Factory, SQL, and enterprise data warehousing. Focus on scalable batch pipelines, mentoring, and performance optimization.","semantic_tags":["databricks","data-engineering","pyspark","azure","aws","synapse","azure-data-factory","sql","senior-level"]}}
{"id":"job_pyspark_developer_004","role":{"title":"PySpark Developer","level":"senior","department":"Data & Analytics","team":"Data Engineering","reports_to":"Data Engineering Manager"},"company":{"name":null,"size":null,"industry":"Technology / Data Services","culture_keywords":["scalability","data-quality","collaboration","engineering-excellence"]},"description":{"summary":"Design, develop, and optimize scalable ETL/ELT data pipelines using PySpark, Python, and AWS cloud services for large-scale data processing and analytics.","key_value_proposition":"Work on distributed data engineering systems handling large and complex datasets with strong focus on performance, data quality, and analytics readiness.","full_text":"The PySpark Developer role involves building robust ETL/ELT pipelines, optimizing Spark jobs, working with AWS data services, and collaborating with cross-functional teams to deliver high-quality, analysis-ready datasets."},"responsibilities":{"primary":["Design and develop scalable ETL/ELT pipelines using PySpark","Build reusable and parameterized Spark jobs for batch and micro-batch processing","Transform raw data into analysis-ready datasets and data marts","Optimize PySpark performance for large and complex datasets","Ensure data quality, consistency, lineage, and documentation","Work with AWS services such as S3, Glue, EMR, and Redshift"],"secondary":["Collaborate with data architects, data modelers, and data scientists","Support CI/CD practices and version control workflows","Implement ingestion frameworks for structured, semi-structured, and unstructured data","Contribute to analytics-driven data architecture"],"time_allocation":{"coding":65,"mentoring":10,"meetings":25}},"requirements":{"must_have":{"experience_years":5,"technical_skills":["Python","PySpark","Apache Spark","Amazon Redshift","PostgreSQL","AWS S3","ETL/ELT","SQL"],"domain_expertise":["Data Engineering","Distributed Data Processing","Data Lake and Data Warehouse Architecture"],"soft_skills":["problem-solving","collaboration","communication","attention to detail"]},"nice_to_have":{"education":[],"technical_skills":["AWS Glue","AWS EMR","AWS Lambda","Delta Lake","Kafka","Spark Structured Streaming","CI/CD","Jenkins","Git"],"domain_expertise":["Data governance","Metadata and lineage management","Streaming data pipelines"]},"deal_breakers":["Lack of PySpark/Spark experience","No hands-on SQL or AWS data platform experience"]},"technical_assessment_focus":{"coding":["PySpark job development","Advanced SQL queries","Python data processing"],"system_design":["ETL/ELT pipeline architecture","AWS-based data platforms","Batch and micro-batch processing systems"],"domain_knowledge":["Data modeling concepts","Data lake and warehouse design","Distributed computing fundamentals"],"problem_solving":["Spark performance tuning","Data quality issues","Pipeline failure troubleshooting"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":"Chennai / Hyderabad / Kolkata, India","remote_policy":"onsite","required_onsite_days":5,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Managerial discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Technical Panel"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"high","last_updated":null},"embedding_optimization":{"primary_text":"Senior PySpark Developer role requiring 5+ years of data engineering experience. Skills include PySpark, Python, AWS (S3, Glue, EMR, Redshift), SQL, ETL/ELT, and distributed data processing. Locations: Chennai, Hyderabad, Kolkata.","semantic_tags":["pyspark","data-engineering","aws","redshift","sql","etl","distributed-systems","senior-level"]}}
{"id":"job_ust_spark_databricks_developer_005","role":{"title":"Spark & Databricks Developer","level":"mid","department":"Data & Analytics","team":"Data Engineering","reports_to":"Data Engineering Manager"},"company":{"name":"UST","size":"30000+","industry":"Digital Transformation & IT Services","culture_keywords":["innovation","agility","collaboration","purpose-driven"]},"description":{"summary":"Design, develop, and optimize big data pipelines using Apache Spark and Databricks to deliver scalable, high-performance data solutions.","key_value_proposition":"Work on long-term, enterprise-scale big data projects using Spark and Databricks within a global digital transformation organization.","full_text":"As a Spark & Databricks Developer at UST, you will design, develop, and optimize big data pipelines using Apache Spark, build and maintain scalable data solutions on Databricks, collaborate with cross-functional teams, and ensure data quality, performance tuning, and job optimization."},"responsibilities":{"primary":["Design, develop, and optimize big data pipelines using Apache Spark","Build and maintain scalable data solutions on Databricks","Ensure data quality, performance tuning, and job optimization","Develop Spark workflows for batch and streaming processing","Write and optimize SQL and Python-based data transformations"],"secondary":["Collaborate with cross-functional teams for data integration and transformation","Manage version control and collaboration using GitHub","Participate in code reviews, testing, and documentation","Support best practices for scalable and maintainable data solutions"],"time_allocation":{"coding":60,"mentoring":10,"meetings":30}},"requirements":{"must_have":{"experience_years":4,"technical_skills":["Apache Spark","Databricks","Python","SQL","GitHub"],"domain_expertise":["Data Engineering","Big Data Processing","Data Lake and Data Warehouse Architecture"],"soft_skills":["problem-solving","collaboration","communication","attention to detail"]},"nice_to_have":{"education":[],"technical_skills":["Spark Streaming","Performance Tuning Tools","CI/CD"],"domain_expertise":["Streaming data pipelines","Enterprise data platforms"]},"deal_breakers":["No hands-on Apache Spark experience","Lack of Databricks or SQL proficiency"]},"technical_assessment_focus":{"coding":["Spark batch and streaming development","Python scripting","SQL query optimization"],"system_design":["Spark and Databricks-based data architectures","Scalable data pipeline design"],"domain_knowledge":["Data lake and warehouse concepts","Distributed data processing"],"problem_solving":["Performance optimization","Data quality and pipeline failures"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":null,"remote_policy":"onsite / hybrid","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Managerial discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Technical Panel"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Spark & Databricks Developer role at UST requiring 4–7 years of experience. Skills include Apache Spark (batch & streaming), Databricks, Python, SQL, GitHub, and enterprise data lake/warehouse architectures.","semantic_tags":["spark","databricks","data-engineering","python","sql","big-data","batch-processing","streaming","mid-level"]}}
{"id":"job_aws_pyspark_databricks_006","role":{"title":"AWS PySpark Databricks Developer","level":"senior","department":"Data & Analytics","team":"Data Engineering","reports_to":"Data Engineering Manager"},"company":{"name":null,"size":null,"industry":"Technology / Data Services","culture_keywords":["cloud-first","scalability","engineering-excellence","governance"]},"description":{"summary":"Design, develop, and optimize cloud-native data engineering solutions using AWS, Databricks, PySpark, and Delta Lake to support scalable analytics platforms.","key_value_proposition":"Build modern data lakehouse solutions on AWS using Databricks and PySpark with a strong focus on performance, governance, and CI/CD best practices.","full_text":"The AWS PySpark Databricks Developer role focuses on building and optimizing scalable data pipelines using Databricks and Apache Spark on AWS, leveraging services such as S3, Glue, EMR, Redshift, and CI/CD tools."},"responsibilities":{"primary":["Develop and optimize data pipelines using PySpark and Databricks","Build cloud-native data solutions on AWS using S3, Glue, EMR, and Redshift","Implement Delta Lake and lakehouse architectures","Develop and tune SQL queries for performance","Ensure data security, governance, and compliance standards"],"secondary":["Integrate orchestration tools such as Airflow or dbt","Support CI/CD pipelines using Jenkins, GitHub Actions, or CodePipeline","Monitor and optimize workloads using CloudWatch","Collaborate with cross-functional teams to deliver scalable data solutions"],"time_allocation":{"coding":60,"mentoring":15,"meetings":25}},"requirements":{"must_have":{"experience_years":5,"technical_skills":["PySpark","Apache Spark","Databricks","AWS S3","AWS Glue","AWS EMR","AWS Redshift","Python","SQL","Delta Lake"],"domain_expertise":["Data Engineering","Cloud Data Platforms","Lakehouse Architecture"],"soft_skills":["problem-solving","collaboration","communication","adaptability"]},"nice_to_have":{"education":[],"technical_skills":["Scala","Airflow","dbt","CI/CD Tools","AWS Lambda","CloudWatch","IAM"],"domain_expertise":["Data governance","Security and compliance frameworks"]},"deal_breakers":["No hands-on Databricks or Spark experience","Lack of AWS data platform expertise"]},"technical_assessment_focus":{"coding":["PySpark development","SQL performance tuning","Python scripting"],"system_design":["AWS lakehouse architecture","Databricks-based data platforms","Secure and governed data solutions"],"domain_knowledge":["Delta Lake concepts","AWS services stack","CI/CD for data pipelines"],"problem_solving":["Pipeline optimization","Cost and performance tuning","Security and governance issues"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":"Visakhapatnam, India","remote_policy":"onsite","required_onsite_days":5,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Managerial discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Technical Panel"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"AWS PySpark Databricks Developer role requiring 5–8 years of experience. Skills include Databricks, Apache Spark, PySpark, AWS (S3, Glue, EMR, Redshift), Delta Lake, SQL, and CI/CD. Location: Visakhapatnam.","semantic_tags":["aws","databricks","pyspark","spark","delta-lake","lakehouse","data-engineering","ci-cd","senior-level"]}}
{"id":"job_jpmc_senior_lead_software_engineer_007","role":{"title":"Senior Lead Software Engineer","level":"senior","department":"Corporate Data & Analytics Office (CDAO)","team":"Data & AI Engineering","reports_to":"Engineering Manager / Tech Lead"},"company":{"name":"JPMorganChase","size":"10000+","industry":"Financial Services","culture_keywords":["engineering-excellence","security","inclusion","innovation"]},"description":{"summary":"Lead the design and delivery of secure, scalable, and high-quality software and data platforms, leveraging cloud, Databricks, and modern SDLC practices.","key_value_proposition":"Drive significant business impact by building market-leading, secure technology solutions within a global financial institution.","full_text":"As a Senior Lead Software Engineer at JPMorganChase within the CDAO, you will provide technical leadership, develop production-grade code, influence architecture and design decisions, and contribute to firmwide engineering standards and practices."},"responsibilities":{"primary":["Provide technical guidance and direction to engineering teams and stakeholders","Develop secure, high-quality production code and review code written by others","Drive architectural and design decisions impacting applications and platforms","Act as a subject matter expert in data, cloud, and AI platforms","Advocate firmwide SDLC frameworks, tools, and best practices"],"secondary":["Influence adoption of leading-edge technologies","Support agile delivery, CI/CD, resiliency, and security practices","Contribute to an inclusive and collaborative engineering culture","Participate in code reviews, debugging, and technical mentoring"],"time_allocation":{"coding":50,"mentoring":25,"meetings":25}},"requirements":{"must_have":{"experience_years":5,"technical_skills":["Python","REST APIs","Databricks","Terraform","Cloud Platforms","CI/CD"],"domain_expertise":["Software Engineering","Cloud Solution Development","Data & AI Platforms"],"soft_skills":["technical leadership","problem-solving","communication","collaboration"]},"nice_to_have":{"education":[],"technical_skills":["Apache Spark","Infrastructure Observability","SCIM","SSO","Front-end Technologies"],"domain_expertise":["Security and resiliency engineering","AI and machine learning platforms"]},"deal_breakers":["Lack of hands-on Python or cloud experience","No experience with production-grade software systems"]},"technical_assessment_focus":{"coding":["Python development","API design and implementation","Code quality and reviews"],"system_design":["Secure and scalable cloud architectures","Databricks platform management","Infrastructure-as-code design"],"domain_knowledge":["Software Development Life Cycle","Agile and CI/CD practices","Application security and resiliency"],"problem_solving":["Complex system debugging","Design trade-off analysis","Production issue resolution"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":true,"target_percent":null}},"logistics":{"location":null,"remote_policy":"onsite / hybrid","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Architectural / leadership discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Engineering Leadership"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Senior Lead Software Engineer role at JPMorganChase (CDAO). 5+ years experience required. Skills include Python, REST APIs, Databricks, Terraform, cloud platforms, CI/CD, and secure enterprise software development.","semantic_tags":["senior-lead-engineer","software-engineering","python","databricks","cloud","terraform","ci-cd","financial-services"]}}
{"id":"job_asyva_azure_databricks_engineer_008","role":{"title":"Azure Databricks Engineer","level":"mid","department":"Data & Analytics","team":"Data Engineering","reports_to":"Data Engineering Lead / Project Manager"},"company":{"name":"ASYVA INFOTECH LLP","size":null,"industry":"IT Services and Data Engineering","culture_keywords":["collaboration","innovation-driven","stability","client-focus"]},"description":{"summary":"Design, develop, and optimize cloud-native data pipelines and analytics solutions using Azure and Databricks for global clients.","key_value_proposition":"Work on long-term, cutting-edge Azure & Databricks data platforms with global clients in a fully remote, contract-based role.","full_text":"ASYVA INFOTECH LLP is looking for a skilled Azure Databricks Engineer to build scalable ETL/ELT pipelines, implement modern data architectures, and optimize performance across Azure and Databricks environments in a long-term contract engagement."},"responsibilities":{"primary":["Design, develop, and maintain ETL/ELT pipelines using Databricks (PySpark, SQL, Python)","Implement and optimize data workflows using Azure services such as Data Factory, Data Lake, Synapse, and Event Hub","Develop data models using Bronze-Silver-Gold architecture","Optimize Spark job performance and cost","Ensure data quality, governance, and security"],"secondary":["Collaborate with data analysts and Power BI developers for reporting solutions","Integrate CI/CD pipelines using Azure DevOps or GitHub Actions","Document technical processes and participate in code reviews","Engage in agile ceremonies and cross-functional collaboration"],"time_allocation":{"coding":65,"mentoring":10,"meetings":25}},"requirements":{"must_have":{"experience_years":3,"technical_skills":["Azure Databricks","PySpark","Python","SQL","PL/SQL","Azure Data Factory","Azure Data Lake","Azure Synapse","Delta Lake"],"domain_expertise":["Data Engineering","Cloud Data Platforms","ETL/ELT Frameworks"],"soft_skills":["analytical thinking","debugging","communication","collaboration"]},"nice_to_have":{"education":[],"technical_skills":["Power BI","dbt","Snowflake","Apache Airflow","Azure Functions","Event Driven Architectures","Docker","Kubernetes","CI/CD"],"domain_expertise":["Data Vault 2.0","DevOps best practices"]},"deal_breakers":["Lack of hands-on Azure Databricks experience","No PySpark or SQL performance optimization experience"]},"technical_assessment_focus":{"coding":["PySpark and Python development","SQL and PL/SQL optimization","ETL pipeline implementation"],"system_design":["Azure Databricks architecture","Bronze-Silver-Gold data modeling","Secure and governed data platforms"],"domain_knowledge":["Delta Lake concepts","Spark architecture","Azure data services"],"problem_solving":["Performance tuning","Data quality issues","Pipeline failure troubleshooting"]},"compensation":{"salary_range":{"min":null,"max":106516.53,"currency":"INR","target":106516.53},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":"Remote","remote_policy":"remote","required_onsite_days":0,"relocation_assistance":false,"visa_sponsorship":false},"interview_process":{"stages":["HR screening","Technical interview","Client / managerial discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Technical Lead"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"contract","requisition_id":null,"hiring_urgency":"high","last_updated":null},"embedding_optimization":{"primary_text":"Azure Databricks Engineer contract role at ASYVA INFOTECH LLP. 3+ years experience required. Skills include Azure Databricks, PySpark, Python, SQL/PLSQL, Azure Data Factory, Synapse, Delta Lake, CI/CD. Fully remote with monthly pay up to ₹106,516.","semantic_tags":["azure-databricks","data-engineering","pyspark","delta-lake","azure-data-factory","ci-cd","remote","contract-role"]}}
{"id":"job_birlasoft_senior_databricks_powerbi_009","role":{"title":"Senior Databricks Developer with Power BI Expertise","level":"senior","department":"Data & Analytics","team":"Data Engineering & BI","reports_to":"Data Engineering Manager / Program Manager"},"company":{"name":"Birlasoft Limited","size":"10000+","industry":"IT Services and Consulting","culture_keywords":["client-focus","collaboration","analytics-driven","enterprise-delivery"]},"description":{"summary":"Lead enterprise data engineering and visualization initiatives using Databricks, Azure Synapse, advanced SQL, and Power BI to deliver scalable analytics solutions.","key_value_proposition":"Drive high-impact data engineering and BI solutions for enterprise clients by combining Databricks-based pipelines with advanced Power BI analytics.","full_text":"Birlasoft Limited is seeking a Senior Databricks Developer with Power BI expertise to design, implement, and manage enterprise-grade data pipelines, perform advanced SQL analytics, and deliver actionable insights through Power BI dashboards while collaborating closely with clients and cross-functional teams."},"responsibilities":{"primary":["Design, implement, and manage data pipelines using Azure Data Factory","Develop scalable data processing solutions using Databricks and Azure Synapse Analytics","Apply advanced SQL for data manipulation, querying, and performance optimization","Design and maintain data models, ETL processes, and data warehousing solutions","Create and manage Power BI reports and dashboards to deliver actionable insights"],"secondary":["Collaborate with clients to gather data requirements and provide solutions","Provide training and support to users on data platforms and BI tools","Develop and maintain technical documentation","Collaborate with cross-functional teams to deliver high-quality data solutions"],"time_allocation":{"coding":50,"mentoring":20,"meetings":30}},"requirements":{"must_have":{"experience_years":10,"technical_skills":["Databricks","Advanced SQL","Azure Data Factory","Azure Synapse Analytics","Power BI","ETL"],"domain_expertise":["Data Engineering","Data Warehousing","Business Intelligence"],"soft_skills":["communication","problem-solving","analytical thinking","client-interaction"]},"nice_to_have":{"education":["Bachelor’s degree in Computer Science or Information Technology"],"technical_skills":["Data Modeling","Query Optimization","Azure Certifications","Databricks Certifications"],"domain_expertise":["Enterprise analytics platforms"]},"deal_breakers":["Lack of hands-on Databricks experience","No Power BI or SQL expertise"]},"technical_assessment_focus":{"coding":["SQL query optimization","Databricks pipeline development","ETL implementation"],"system_design":["Azure-based data architecture","Enterprise data warehousing","BI-enabled analytics platforms"],"domain_knowledge":["Databricks and Synapse Analytics","Power BI visualization best practices","ETL and data modeling concepts"],"problem_solving":["Data performance tuning","Client-driven analytics challenges","Pipeline reliability issues"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":"Any Birlasoft Location, India","remote_policy":"onsite","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Client / managerial discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Technical Panel","Client Stakeholders"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Senior Databricks Developer with Power BI Expertise role at Birlasoft Limited. 10–14 years experience required. Skills include Databricks, Azure Data Factory, Azure Synapse Analytics, advanced SQL, Power BI, ETL, and enterprise data engineering.","semantic_tags":["databricks","power-bi","data-engineering","azure-synapse","advanced-sql","enterprise-analytics","senior-level"]}}
{"id":"job_senior_databricks_engineer_010","role":{"title":"Senior Databricks Engineer","level":"senior","department":"Data & Analytics","team":"Data Engineering","reports_to":"Data Engineering Manager / Architect"},"company":{"name":null,"size":null,"industry":"Technology / Healthcare Analytics","culture_keywords":["innovation-driven","collaboration","engineering-excellence","growth"]},"description":{"summary":"Design, build, and optimize scalable AWS-based data pipelines using Databricks, Apache Spark, Python, and SQL to deliver high-quality data products.","key_value_proposition":"Work on cutting-edge data engineering projects with a hybrid work model, leveraging Databricks and AWS to build scalable analytics platforms.","full_text":"The Senior Databricks Engineer role focuses on designing and optimizing scalable data pipelines on AWS using Databricks, Apache Spark, Python, and SQL, while ensuring automated deployments and collaboration across teams."},"responsibilities":{"primary":["Design, build, and optimize scalable data pipelines on AWS","Implement data ingestion, transformation, and integration solutions using Databricks and AWS Glue","Develop and optimize SQL-based transformations and analytics","Manage and optimize cloud storage and compute environments","Implement and maintain CI/CD pipelines using Jenkins and Terraform"],"secondary":["Collaborate with cross-functional teams to deliver data products","Ensure reliability, performance, and scalability of data platforms","Contribute to best practices for cloud data engineering","Support domain-specific analytics initiatives"],"time_allocation":{"coding":60,"mentoring":15,"meetings":25}},"requirements":{"must_have":{"experience_years":6,"technical_skills":["Python","Databricks","Apache Spark","SQL","AWS S3","AWS Glue","AWS Redshift","AWS EMR","AWS Lambda","CI/CD","Jenkins","Terraform"],"domain_expertise":["Data Engineering","Cloud Data Platforms","Big Data Processing"],"soft_skills":["problem-solving","collaboration","communication","ownership"]},"nice_to_have":{"education":[],"technical_skills":[],"domain_expertise":["Healthcare Analytics","Life Sciences Data"]},"deal_breakers":["No hands-on Databricks or Spark experience","Lack of AWS cloud data platform expertise"]},"technical_assessment_focus":{"coding":["PySpark development","SQL optimization","Python scripting"],"system_design":["AWS-based data pipeline architecture","Databricks-driven analytics platforms","Automated CI/CD deployments"],"domain_knowledge":["AWS data services","Databricks and Spark internals","Cloud cost and performance optimization"],"problem_solving":["Pipeline failures","Performance bottlenecks","Cloud infrastructure issues"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":"Hyderabad, India","remote_policy":"hybrid","required_onsite_days":3,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Managerial / architectural discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Technical Architect"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"high","last_updated":null},"embedding_optimization":{"primary_text":"Senior Databricks Engineer role in Hyderabad requiring 6–15 years experience. Skills include Databricks, Apache Spark, Python, SQL, AWS (S3, Glue, Redshift, EMR, Lambda), Jenkins, and Terraform. Hybrid work model.","semantic_tags":["databricks","aws","spark","python","sql","ci-cd","terraform","data-engineering","senior-level","hybrid"]}}
{"id":"job_aws_databricks_palantir_011","role":{"title":"AWS Databricks Developer","level":"senior","department":"Data & Analytics","team":"Data Engineering","reports_to":"Data Engineering Manager / Architect"},"company":{"name":null,"size":null,"industry":"Technology / Data Services","culture_keywords":["engineering-excellence","scalability","security","continuous-learning"]},"description":{"summary":"Design, develop, and maintain scalable data processing and analytics solutions using AWS, Databricks, Cloudera, and Palantir platforms.","key_value_proposition":"Work on complex, enterprise-grade data engineering and analytics solutions integrating Databricks with AWS and Palantir to deliver secure, high-performance data platforms.","full_text":"The AWS Databricks Developer role focuses on building and optimizing scalable data pipelines and analytics solutions using Databricks, AWS services, Cloudera ecosystem tools, and Palantir while ensuring performance, security, and alignment with business goals."},"responsibilities":{"primary":["Design and develop data engineering solutions using Databricks and AWS","Build and optimize Databricks jobs, workloads, clusters, and notebooks","Develop Python-based automation scripts for workflow optimization","Integrate Databricks with AWS services such as S3, Glue, Athena, Redshift, and Lambda","Design architectures for data-driven and analytics solutions","Implement data pipelines and analytics solutions using Palantir"],"secondary":["Work with CI/CD pipelines using Jenkins","Ensure scalability, security, and performance of deployed solutions","Collaborate with business teams, data engineers, and developers","Stay updated with emerging technologies and adapt quickly","Provide solutioning support for web applications and data pipelines"],"time_allocation":{"coding":55,"mentoring":15,"meetings":30}},"requirements":{"must_have":{"experience_years":6,"technical_skills":["Databricks","PySpark","Scala","Python","AWS S3","AWS Glue","AWS Athena","AWS Redshift","AWS Lambda","Cloudera","Palantir","CI/CD","Jenkins"],"domain_expertise":["Data Engineering","Big Data Processing","Cloud Data Platforms","Analytics Platforms"],"soft_skills":["communication","problem-solving","adaptability","collaboration"]},"nice_to_have":{"education":["Bachelor’s degree in Computer Science or IT"],"technical_skills":["Docker","Kubernetes","Kafka","Spark Streaming","MLOps"],"domain_expertise":["Regulated industries","Data compliance and governance"]},"deal_breakers":["No hands-on Databricks experience","Lack of AWS or big data ecosystem exposure"]},"technical_assessment_focus":{"coding":["PySpark and Scala development","Python automation scripts","SQL-based data transformations"],"system_design":["AWS-Databricks architecture","Palantir-based analytics workflows","Secure and scalable data platforms"],"domain_knowledge":["Cloudera ecosystem (Spark, Hive, Impala, HDFS, Kafka, HBase)","Databricks workload optimization","Cloud integration patterns"],"problem_solving":["Performance tuning","Cluster optimization","Complex data pipeline failures"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":null,"remote_policy":"onsite","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Architectural / managerial discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Technical Architect","Engineering Leadership"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Senior AWS Databricks Developer role requiring 6+ years of data engineering experience. Skills include Databricks, PySpark, Scala, Python, AWS (S3, Glue, Athena, Redshift, Lambda), Cloudera ecosystem, Palantir, and CI/CD.","semantic_tags":["aws","databricks","data-engineering","pyspark","scala","cloudera","palantir","big-data","senior-level"]}}
{"id":"job_azure_databricks_engineer_012","role":{"title":"Databricks Developer","level":"senior","department":"Data & Analytics","team":"Data Engineering","reports_to":"Data Engineering Manager / Architect"},"company":{"name":null,"size":null,"industry":"Technology / Data Services","culture_keywords":["operational-excellence","collaboration","scalability","mentorship"]},"description":{"summary":"Design, build, and support scalable Azure Databricks data engineering solutions, including batch and real-time pipelines, in a 24x7 production environment.","key_value_proposition":"Work on enterprise-scale Azure Databricks platforms handling complex data pipelines, advanced analytics, and production-critical workloads across multiple locations.","full_text":"This Databricks Developer role requires deep expertise in Azure Databricks, Python, Scala, Synapse, and Azure Data Factory to design, optimize, and support scalable data engineering solutions while mentoring teams and supporting 24x7 production workloads."},"responsibilities":{"primary":["Design and develop scalable data pipelines using Azure Databricks, Python, and Scala","Build and optimize batch ETL pipelines; support real-time pipelines where applicable","Develop solutions using Azure Synapse, Azure Data Factory, and SQL Server","Implement dimensional modeling, data warehousing, and data governance practices","Perform root cause analysis on data and pipeline issues","Support 24x7 production environments including night shifts"],"secondary":["Mentor and coach data engineers on standards and best practices","Collaborate with business analysts and solution architects on enterprise initiatives","Conduct design reviews, code reviews, and documentation","Foster reuse, scalability, and operational efficiency of data solutions","Partner with cross-functional teams to deliver strategic data platforms"],"time_allocation":{"coding":50,"mentoring":20,"meetings":30}},"requirements":{"must_have":{"experience_years":6,"technical_skills":["Databricks","Python","Scala","Azure Synapse","Azure Data Factory","SQL Server","T-SQL","ETL","CI/CD"],"domain_expertise":["Data Engineering","Cloud Data Platforms","Data Warehousing","Dimensional Modeling"],"soft_skills":["problem-solving","communication","mentorship","collaboration"]},"nice_to_have":{"education":[],"technical_skills":["Boomi","Streaming Pipelines"],"domain_expertise":["Data governance","Enterprise data architecture"]},"deal_breakers":["No hands-on Azure Databricks experience","Lack of Synapse or SQL Server expertise"]},"technical_assessment_focus":{"coding":["PySpark and Scala development","Advanced SQL and T-SQL","ETL pipeline implementation"],"system_design":["Azure Databricks data architecture","Batch and real-time pipeline design","Enterprise data warehousing solutions"],"domain_knowledge":["Dimensional modeling","Data governance","Azure data services"],"problem_solving":["Production issue troubleshooting","Root cause analysis","Performance optimization"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":"Kolkata / Hyderabad / Pune, India","remote_policy":"onsite","required_onsite_days":5,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Architectural / managerial discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Data Architect","Technical Panel"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"high","last_updated":null},"embedding_optimization":{"primary_text":"Senior Databricks Developer role requiring 6–8 years experience with Azure Databricks, Python, Scala, Azure Synapse, Azure Data Factory, SQL Server, and enterprise data warehousing. Locations: Kolkata, Hyderabad, Pune. 24x7 production support environment.","semantic_tags":["databricks","azure","data-engineering","pyspark","scala","synapse","azure-data-factory","sql-server","senior-level","production-support"]}}
{"id":"job_seaspan_databricks_developer_013","role":{"title":"Databricks Developer","level":"senior","department":"Data Development","team":"Data Engineering","reports_to":"Team Lead, Data Development"},"company":{"name":"Seaspan","size":null,"industry":"Maritime & Logistics / Enterprise Services","culture_keywords":["collaboration","data-quality","engineering-excellence","enterprise-focus"]},"description":{"summary":"Design, build, and optimize scalable Databricks-based data pipelines to support enterprise analytics and reporting in a modern Azure cloud environment.","key_value_proposition":"Work on enterprise-scale Azure Databricks lakehouse platforms delivering high-quality, analytics-ready datasets with both batch and real-time processing.","full_text":"This Databricks Developer role focuses on implementing robust, scalable data pipelines using Apache Spark on Databricks, integrating multiple data sources, optimizing Delta Lake performance, and supporting enterprise analytics and reporting needs."},"responsibilities":{"primary":["Design, build, and maintain scalable data pipelines using Databricks (SQL, PySpark, Delta Lake)","Develop ETL/ELT pipelines using Azure Data Factory and Databricks notebooks","Integrate and transform large-scale structured and semi-structured datasets","Optimize Spark jobs and Delta Lake performance using partitioning, Z-ordering, caching, and broadcast joins","Implement data ingestion pipelines for REST APIs and streaming sources"],"secondary":["Perform data validation and quality checks","Build orchestration pipelines for ADLS, Databricks, and SQL Server","Collaborate with data scientists, analysts, architects, and business teams","Contribute to data governance, metadata documentation, and quality standards","Manage code and deployments using Git and CI/CD pipelines"],"time_allocation":{"coding":60,"mentoring":10,"meetings":30}},"requirements":{"must_have":{"experience_years":5,"technical_skills":["Databricks","Apache Spark","PySpark","SQL","Python","Delta Lake","Azure Data Factory","Azure Data Lake","ETL/ELT"],"domain_expertise":["Data Engineering","Lakehouse Architecture","Batch and Streaming Data Processing"],"soft_skills":["problem-solving","communication","collaboration","attention to detail"]},"nice_to_have":{"education":["Bachelor’s degree in Computer Science or related field"],"technical_skills":["dbt","Azure Synapse","Microsoft Fabric","Unity Catalog","Kafka","Event Hub","MQTT","CI/CD"],"domain_expertise":["IoT data processing","Predictive modeling","Anomaly detection"]},"deal_breakers":["No hands-on Databricks or Spark experience","Lack of Azure Data Factory exposure"]},"technical_assessment_focus":{"coding":["PySpark and SQL development","Python-based data validation","ETL pipeline implementation"],"system_design":["Azure Databricks lakehouse architecture","Batch and real-time pipeline design","API-based data ingestion architectures"],"domain_knowledge":["Delta Lake internals","Data modeling and warehousing","Streaming data frameworks"],"problem_solving":["Spark performance tuning","Pipeline reliability issues","Data quality failures"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":null,"remote_policy":"onsite / hybrid","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Team / architectural discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Team Lead","Technical Panel"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Senior Databricks Developer role focused on Azure Databricks, Apache Spark, Delta Lake, Azure Data Factory, and scalable batch and streaming pipelines for enterprise analytics.","semantic_tags":["databricks","spark","azure","delta-lake","data-engineering","etl","streaming","lakehouse","senior-level"]}}
{"id":"job_azure_databricks_delta_014","role":{"title":"Azure Databricks Delta Lake Developer","level":"mid","department":"Data & Analytics","team":"Data Engineering","reports_to":"Data Engineering Manager"},"company":{"name":null,"size":null,"industry":"Technology / Data Services","culture_keywords":["collaboration","engineering-excellence","cloud-first","delivery-focus"]},"description":{"summary":"Design and implement Azure-based data engineering solutions using Databricks Delta Lake, Python, SQL, and ETL methodologies for enterprise data platforms.","key_value_proposition":"Work on Azure-centric Databricks Delta Lake projects involving dimensional data models, data warehouses, and scalable cloud architectures.","full_text":"This role focuses on leveraging Databricks Delta Lake on Azure to build and optimize ETL pipelines, implement Python-based data processing, and support analytics solutions using relational databases and dimensional data models."},"responsibilities":{"primary":["Develop and optimize data pipelines using Databricks Delta Lake on Azure","Implement Python scripting for data processing and automation","Build ETL pipelines to extract data from dimensional data models and data warehouses","Use SparkSQL and SQL for data transformation and analytics","Collaborate with teams to design Azure-based data architectures"],"secondary":["Support performance tuning and optimization of data pipelines","Work with relational databases for data access and integration","Document data engineering processes and standards","Collaborate with cross-functional teams for solution delivery"],"time_allocation":{"coding":60,"mentoring":10,"meetings":30}},"requirements":{"must_have":{"experience_years":5,"technical_skills":["Databricks","Delta Lake","Python","SparkSQL","SQL","ETL","Azure"],"domain_expertise":["Data Engineering","Cloud Data Platforms","Data Warehousing","Dimensional Modeling"],"soft_skills":["problem-solving","communication","collaboration"]},"nice_to_have":{"education":[],"technical_skills":["Azure Architecture Design"],"domain_expertise":["Enterprise analytics platforms"]},"deal_breakers":["No hands-on Databricks Delta Lake experience","Lack of Azure or SQL expertise"]},"technical_assessment_focus":{"coding":["PySpark and SparkSQL development","Python scripting","SQL-based transformations"],"system_design":["Azure Databricks data architecture","ETL and dimensional data modeling"],"domain_knowledge":["Delta Lake architecture","Data warehousing concepts","Azure cloud services"],"problem_solving":["Data pipeline failures","Performance optimization","Data extraction challenges"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":null,"remote_policy":"onsite / hybrid","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Managerial discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Technical Panel"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Azure Databricks Delta Lake Developer role requiring 5–8 years experience. Skills include Databricks Delta Lake, Python, SparkSQL, SQL, ETL, dimensional data models, data warehousing, and Azure architecture.","semantic_tags":["azure","databricks","delta-lake","data-engineering","python","sql","etl","mid-level"]}}
{"id":"job_cgi_senior_databricks_015","role":{"title":"Senior Databricks Developer","level":"senior","department":"Software Development","team":"Data Engineering / Lakehouse","reports_to":"Data Engineering Manager / Lead Architect"},"company":{"name":"CGI","size":"90000+","industry":"IT Services and Business Consulting","culture_keywords":["ownership","teamwork","innovation","belonging"]},"description":{"summary":"Lead the design, development, and deployment of enterprise-scale data products on the Databricks Lakehouse platform with deep AWS integration and governance.","key_value_proposition":"Drive high-impact data products and lakehouse architectures at scale within a global consulting organization, combining Databricks, AWS, governance, and BI.","full_text":"As a Senior Databricks Developer at CGI, you will lead the design and delivery of data products on the Databricks Lakehouse platform, leveraging Delta Lake, Unity Catalog, AWS services, and modern data engineering best practices to deliver scalable, governed, and high-performance solutions."},"responsibilities":{"primary":["Design and manage data products using Databricks Lakehouse principles","Develop end-to-end data pipelines using PySpark and Databricks SQL","Implement medallion architecture (Bronze/Silver/Gold) using Delta Lake","Lead Databricks on AWS integration including S3, IAM, and VPC networking","Implement governance using Unity Catalog with fine-grained access and lineage","Optimize Spark workloads for performance and cost efficiency"],"secondary":["Drive DevOps and CI/CD automation using Git, Databricks Repos, CLI, and SDK","Lead code reviews, performance reviews, and engineering best practices","Operationalize ML and analytics using MLflow, Feature Store, and Model Serving","Build and optimize Power BI dashboards and data models","Evaluate and adopt new Databricks features into the technical roadmap"],"time_allocation":{"coding":50,"mentoring":25,"meetings":25}},"requirements":{"must_have":{"experience_years":5,"technical_skills":["Databricks","PySpark","Python","Databricks SQL","Delta Lake","Unity Catalog","AWS S3","AWS Glue","AWS Lambda","AWS EMR","Terraform","CI/CD"],"domain_expertise":["Data Engineering","Lakehouse Architecture","Data Products","Cloud Data Platforms"],"soft_skills":["ownership","technical leadership","communication","architecture influence"]},"nice_to_have":{"education":["Bachelor’s or Master’s degree in Computer Science or Engineering"],"technical_skills":["MLflow","Feature Store","Model Serving","Power BI","DAX","Photon","Serverless Compute"],"domain_expertise":["Data Mesh","Governance and security frameworks"]},"deal_breakers":["No production Databricks experience","Lack of AWS cloud integration knowledge"]},"technical_assessment_focus":{"coding":["PySpark pipeline development","Databricks SQL optimization","Terraform-based infrastructure code"],"system_design":["Databricks Lakehouse architecture","AWS-integrated data platforms","Governed data product design"],"domain_knowledge":["Delta Lake internals","Unity Catalog governance","Spark performance tuning"],"problem_solving":["Cost optimization","Pipeline scalability issues","Governance and security challenges"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":true,"target_percent":null}},"logistics":{"location":"Bangalore / Chennai, India","remote_policy":"hybrid","required_onsite_days":3,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Architectural discussion","Managerial round"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Lead Architect","Engineering Leadership"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":"J1125-2184","hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Senior Databricks Developer role at CGI requiring 5–10 years experience. Skills include Databricks Lakehouse, PySpark, Delta Lake, Unity Catalog, AWS services, Terraform, CI/CD, and Power BI. Hybrid role in Bangalore or Chennai.","semantic_tags":["databricks","lakehouse","aws","delta-lake","unity-catalog","data-products","terraform","power-bi","senior-level","hybrid"]}}
{"id":"job_newscape_senior_lead_databricks_016","role":{"title":"Senior / Lead Databricks Developer","level":"senior","department":"Data & Analytics","team":"Data Engineering","reports_to":"Data Engineering Director / Architect"},"company":{"name":"Newscape Consulting","size":null,"industry":"Digital Services & Healthcare Technology","culture_keywords":["innovation","healthcare-impact","technical-leadership","collaboration"]},"description":{"summary":"Architect, design, and optimize large-scale Databricks Lakehouse data pipelines while providing technical leadership in healthcare-focused data engineering projects.","key_value_proposition":"Lead end-to-end Databricks implementations in healthcare data platforms, combining deep technical expertise with hands-on leadership in a fast-growing digital services company.","full_text":"Newscape Consulting is hiring a Senior or Lead Databricks Developer to architect, build, and optimize enterprise-scale data pipelines using Databricks, Apache Spark, Delta Lake, and cloud platforms, while mentoring engineers and collaborating closely with business and technical stakeholders."},"responsibilities":{"primary":["Architect, design, and maintain large-scale data pipelines using Databricks, PySpark, and Spark SQL","Lead implementation of Databricks components including Jobs, DLT, Repos, and Unity Catalog","Build and optimize Delta Lake solutions following Lakehouse and Medallion architectures","Drive CI/CD automation for Databricks deployments using Azure DevOps or GitHub Actions","Ensure data governance, quality, metadata management, and lineage tracking"],"secondary":["Collaborate with architects and stakeholders to translate business requirements into technical solutions","Perform Spark performance tuning and cluster cost optimization","Guide data modeling, SQL transformations, and data warehousing practices","Mentor junior and mid-level data engineers","Participate in architecture reviews and roadmap planning"],"time_allocation":{"coding":45,"mentoring":30,"meetings":25}},"requirements":{"must_have":{"experience_years":6,"technical_skills":["Databricks","PySpark","Spark SQL","Delta Lake","Unity Catalog","CI/CD","Cloud Platforms","SQL"],"domain_expertise":["Data Engineering","Lakehouse Architecture","Enterprise Data Platforms"],"soft_skills":["technical leadership","stakeholder management","communication","problem-solving"]},"nice_to_have":{"education":[],"technical_skills":["Scala","Apache Airflow","Azure Data Factory","Databricks Workflows","Power BI"],"domain_expertise":["Healthcare Data Engineering","Data Governance Frameworks"]},"deal_breakers":["No hands-on Databricks expertise","Lack of leadership or senior SME experience"]},"technical_assessment_focus":{"coding":["PySpark and Spark SQL development","Delta Lake optimization","CI/CD pipeline implementation"],"system_design":["Databricks Lakehouse architecture","Medallion architecture","Governed enterprise data platforms"],"domain_knowledge":["Unity Catalog governance","Spark performance tuning","Cloud-native data services"],"problem_solving":["Pipeline scalability issues","Cost optimization","Data quality and governance challenges"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":"Baner, Pune, India","remote_policy":"onsite","required_onsite_days":5,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","In-person technical interview","Architectural / leadership discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Technical Leadership"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"high","last_updated":null},"embedding_optimization":{"primary_text":"Senior / Lead Databricks Developer role at Newscape Consulting in Pune. Onsite role requiring deep expertise in Databricks, Delta Lake, Unity Catalog, CI/CD, cloud platforms, and leadership in healthcare data engineering projects.","semantic_tags":["databricks","lakehouse","delta-lake","unity-catalog","data-engineering","lead-role","healthcare","onsite","pune","senior-level"]}}
{"id":"job_tcs_data_engineer_017","role":{"title":"Data Engineer (Python, PySpark, Databricks)","level":"senior","department":"Data & Analytics","team":"Data Engineering","reports_to":"Project Manager / Data Engineering Lead"},"company":{"name":"Tata Consultancy Services (TCS)","size":"100000+","industry":"IT Services and Consulting","culture_keywords":["learning","scale","collaboration","innovation"]},"description":{"summary":"Design, build, and optimize scalable cloud-based data pipelines using Python, PySpark, Databricks, SQL, and AWS big data technologies.","key_value_proposition":"Work on large-scale enterprise data platforms at a global IT leader, building secure, high-performance data pipelines that drive analytics and business insights.","full_text":"TCS is hiring a Data Engineer with strong experience in Python, PySpark, Databricks, and SQL to design and maintain optimal data pipeline architectures, build cloud-native big data solutions, and support analytics and data science teams across the organization."},"responsibilities":{"primary":["Design and maintain optimal data pipeline architectures","Build and optimize data ingestion, transformation, and publishing pipelines using Python, PySpark, and Databricks","Assemble large, complex datasets meeting business requirements","Build cloud-based big data pipelines and architectures on AWS","Develop analytics tools to deliver actionable business insights","Ensure data security and segregation across regions"],"secondary":["Automate manual processes and improve data delivery efficiency","Collaborate with executive, product, data, and design teams","Support analytics and data science teams with scalable data tools","Perform root cause analysis on data issues","Work with cross-functional teams in a dynamic environment"],"time_allocation":{"coding":60,"mentoring":10,"meetings":30}},"requirements":{"must_have":{"experience_years":6,"technical_skills":["Python","PySpark","Databricks","SQL","AWS Big Data","Data Modeling"],"domain_expertise":["Data Engineering","Cloud Big Data Platforms","Analytics Enablement"],"soft_skills":["analytical thinking","problem-solving","communication","collaboration"]},"nice_to_have":{"education":[],"technical_skills":["PostgreSQL","Cassandra","Apache NiFi","AWS Step Functions","Oozie","Azkaban","Luigi","Airflow","AWS EMR","AWS Redshift","AWS DMS","Kinesis","Spark Streaming","Scala","Java"],"domain_expertise":["Streaming data processing","Metadata and workload management"]},"deal_breakers":["No Databricks experience","Lack of Python or SQL proficiency"]},"technical_assessment_focus":{"coding":["PySpark development","Advanced SQL queries","Python scripting"],"system_design":["Cloud big data pipeline architecture","AWS-based data platforms","Secure multi-region data architectures"],"domain_knowledge":["Structured and unstructured data processing","Data modeling","Streaming and batch data systems"],"problem_solving":["Root cause analysis","Pipeline optimization","Large-scale data handling issues"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":"Hyderabad, India","remote_policy":"onsite / hybrid","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["Virtual technical interview","Managerial discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Technical Panel"]},"metadata":{"posted_date":null,"application_deadline":"2025-12-13","job_type":"full-time","requisition_id":null,"hiring_urgency":"high","last_updated":null},"embedding_optimization":{"primary_text":"Senior Data Engineer role at TCS requiring 6–8 years experience. Skills include Python, PySpark, Databricks, SQL, AWS big data services, data modeling, and large-scale enterprise data pipeline development. Location: Hyderabad.","semantic_tags":["tcs","data-engineering","databricks","pyspark","python","sql","aws","big-data","senior-level"]}}
{"id":"job_beyondkey_senior_data_engineer_018","role":{"title":"Senior Data Engineer (Snowflake, Databricks, Azure)","level":"senior","department":"Data & Analytics","team":"Data Engineering","reports_to":"Data Engineering Manager"},"company":{"name":"Beyond Key","size":"350+","industry":"IT Consulting and Software Services","culture_keywords":["happy-team","client-first","collaboration","innovation"]},"description":{"summary":"Design and own end-to-end cloud data pipelines and analytics solutions using Snowflake, Databricks, Python, and Azure.","key_value_proposition":"Work with global clients to build production-grade, scalable cloud data platforms while mentoring engineers in a Microsoft Gold Partner organization.","full_text":"Beyond Key is seeking a Senior Data Engineer to design and own end-to-end data pipelines and Snowflake data models, process large-scale data using Databricks, and deliver robust ETL/ELT frameworks on Azure cloud platforms."},"responsibilities":{"primary":["Design and own end-to-end data pipelines and Snowflake data models","Process large-scale datasets using Databricks (PySpark and Spark SQL)","Translate business requirements into scalable data architectures and ETL/ELT frameworks","Optimize Azure cloud infrastructure and manage data integration tools","Deliver production-grade, analytics-ready datasets"],"secondary":["Collaborate with cross-functional teams to deliver data solutions","Mentor junior data engineers and promote best practices","Support data modeling, performance tuning, and cost optimization","Contribute to documentation and data engineering standards"],"time_allocation":{"coding":55,"mentoring":20,"meetings":25}},"requirements":{"must_have":{"experience_years":5,"technical_skills":["Snowflake","Databricks","PySpark","Spark SQL","Python","Azure","ETL/ELT","Data Modeling"],"domain_expertise":["Data Engineering","Cloud Data Platforms","Analytics Enablement"],"soft_skills":["communication","mentorship","problem-solving","collaboration"]},"nice_to_have":{"education":[],"technical_skills":["Fivetran","Apache Airflow","Kafka","Terraform"],"domain_expertise":["Streaming data pipelines","Infrastructure as Code"]},"deal_breakers":["No hands-on Snowflake or Databricks experience","Lack of Azure cloud exposure"]},"technical_assessment_focus":{"coding":["PySpark and Spark SQL development","Python-based ETL logic","Snowflake SQL modeling"],"system_design":["Cloud-native data architecture on Azure","Snowflake and Databricks integration","Scalable ETL/ELT frameworks"],"domain_knowledge":["Data modeling best practices","Cloud cost and performance optimization","Batch and streaming data concepts"],"problem_solving":["Pipeline performance issues","Data integration failures","Schema and model optimization"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":null,"remote_policy":"onsite / hybrid","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Managerial discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Technical Panel"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Senior Data Engineer role at Beyond Key requiring 5+ years experience. Skills include Snowflake, Databricks, PySpark, Spark SQL, Python, Azure, ETL/ELT, and data modeling for enterprise cloud data platforms.","semantic_tags":["snowflake","databricks","azure","data-engineering","pyspark","etl","cloud-data","senior-level"]}}
{"id":"job_gainwell_senior_principal_data_engineer_019","role":{"title":"Senior Data Engineer / Data Architect (Databricks, Spark, AWS)","level":"senior","department":"Data Analytics","team":"Product Data Engineering","reports_to":"Engineering Manager / Data Architecture Lead"},"company":{"name":"Gainwell Technologies","size":null,"industry":"Healthcare Technology & Data Analytics","culture_keywords":["mission-driven","product-focus","agile","quality-first"]},"description":{"summary":"Design, develop, and optimize cloud-native data frameworks using Databricks, Apache Spark, and AWS to support analytics-focused product development.","key_value_proposition":"Lead high-impact product data engineering initiatives using Databricks and AWS, focusing on scalable analytics frameworks without exposure to PHI or PII.","full_text":"Gainwell Technologies is seeking experienced Data Engineers to design, develop, and optimize data frameworks on AWS using Databricks and Apache Spark. The role is focused on product development and analytics enablement, explicitly excluding handling of PHI, PII, or other sensitive client data."},"responsibilities":{"primary":["Design and develop data frameworks using Databricks, PySpark, and AWS","Build and optimize ETL pipelines for data collection, transformation, storage, and reporting","Collaborate in Agile teams and participate in scrum ceremonies","Work with testers to ensure defect-free data solutions","Participate in technical reviews and inspections to ensure design intent is met","Provide time estimates and support project planning activities"],"secondary":["Coordinate documentation reviews with SMEs and implementation teams","Assist in defining methods to align data solutions with business objectives","Mentor and guide less experienced data engineers","Contribute to strategic planning across multiple engagements"],"time_allocation":{"coding":45,"mentoring":25,"meetings":30}},"requirements":{"must_have":{"experience_years":14,"technical_skills":["Databricks","Apache Spark","PySpark","Python","AWS","ETL","Data Architecture"],"domain_expertise":["Data Engineering","Big Data Frameworks","Cloud Data Platforms","Product Analytics"],"soft_skills":["leadership","strategic thinking","communication","project planning"]},"nice_to_have":{"education":["Bachelor’s degree in Computer Science or related field"],"technical_skills":["Scala","Azure","GCP","Databricks Certifications","AWS Certifications"],"domain_expertise":["Enterprise data architecture","Agile delivery at scale"]},"deal_breakers":["No hands-on Databricks or Spark experience","Lack of cloud-based big data expertise"]},"technical_assessment_focus":{"coding":["PySpark and Spark job development","Python-based ETL logic"],"system_design":["AWS-based data architecture","Databricks-centric analytics frameworks"],"domain_knowledge":["Transactional vs data warehouse systems","Big data processing patterns"],"problem_solving":["Pipeline optimization","Design validation across project lifecycle"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":null,"remote_policy":"onsite / remote","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Leadership / architectural discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Data Architecture Leadership"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Senior Data Engineer / Data Architect role at Gainwell Technologies requiring 14+ years experience. Skills include Databricks, Apache Spark, PySpark, Python, AWS, ETL, and cloud data architecture for analytics-focused product development.","semantic_tags":["gainwell","databricks","spark","aws","data-engineering","data-architecture","product-analytics","senior-level"]}}
{"id":"job_astellas_bi_etl_developer_020","role":{"title":"BI & ETL Developer (Qlik, Power BI, Databricks)","level":"mid","department":"FoundationX / Data & Analytics","team":"Business Intelligence & Data Engineering","reports_to":"Data & Analytics Manager"},"company":{"name":"Astellas Pharma Inc.","size":"10000+","industry":"Pharmaceuticals & Life Sciences","culture_keywords":["innovation","ethics","patient-centric","collaboration","work-life-balance"]},"description":{"summary":"Design, develop, and optimize business intelligence and ETL solutions using Qlik, Power BI, Databricks, and modern data warehousing practices in a regulated pharmaceutical environment.","key_value_proposition":"Enable data-driven decision-making in a global pharmaceutical organization by delivering high-quality BI, ETL, and analytics solutions while adhering to strict regulatory and governance standards.","full_text":"Astellas is seeking a BI & ETL Developer to design and optimize data solutions leveraging Qlik Sense, Power BI, Databricks, and ETL technologies. The role focuses on building scalable analytics, ensuring data quality and governance, and collaborating across commercial, manufacturing, and medical domains in a regulated life sciences environment."},"responsibilities":{"primary":["Design, develop, and maintain BI applications using Qlik Sense, Tableau, and Power BI","Build and optimize ETL processes using Databricks, Talend, dbt, or equivalent tools","Design logical and physical data models for BI consumption","Develop interactive dashboards, reports, and visualizations with pixel-perfect quality","Ensure data quality, consistency, lineage, and governance compliance"],"secondary":["Collaborate with end users to translate business requirements into analytical solutions","Perform performance tuning of BI models and reports","Support data validation, testing, and troubleshooting of BI systems","Create and maintain technical documentation","Participate in DevOps and CI/CD pipelines for analytics delivery"],"time_allocation":{"coding":45,"mentoring":15,"meetings":40}},"requirements":{"must_have":{"experience_years":3,"technical_skills":["Qlik Sense","Power BI","DAX","SQL","Databricks","ETL","Data Modeling"],"domain_expertise":["Business Intelligence","Data Warehousing","Pharmaceutical / Life Sciences Analytics"],"soft_skills":["analytical thinking","communication","collaboration","problem-solving"]},"nice_to_have":{"education":["Bachelor’s or Master’s degree in Computer Science, IT, or related field"],"technical_skills":["Tableau","Talend","dbt","Python","R","CI/CD","Power Automate"],"domain_expertise":["GxP compliance","HIPAA","Regulated industry analytics"]},"deal_breakers":["No BI tool experience (Qlik/Power BI)","Lack of SQL or data modeling expertise"]},"technical_assessment_focus":{"coding":["SQL queries and optimization","BI data model development","ETL logic implementation"],"system_design":["BI and data warehouse architecture","ETL and analytics integration","Governed analytics platforms"],"domain_knowledge":["Data governance and lineage","Regulatory compliance in pharma","Data quality frameworks"],"problem_solving":["Data quality issues","Performance bottlenecks","Reporting accuracy challenges"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":true,"target_percent":null}},"logistics":{"location":"Bangalore, India","remote_policy":"hybrid","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Stakeholder / behavioral discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Data & Analytics Leadership"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"BI & ETL Developer role at Astellas Pharma in Bangalore. Skills include Qlik Sense, Power BI, DAX, SQL, Databricks, ETL, data modeling, and regulated life sciences analytics. Hybrid work model.","semantic_tags":["bi-developer","etl","databricks","qlik","power-bi","pharma","life-sciences","hybrid","mid-level"]}}
{"id":"job_pradeepit_python_sql_databricks_021","role":{"title":"Python, SQL & Databricks Developer","level":"mid","department":"Engineering","team":"Data Engineering","reports_to":"Technical Lead / Project Manager"},"company":{"name":"Pradeepit Global Consulting Private Limited","size":null,"industry":"IT Consulting and Services","culture_keywords":["collaboration","continuous-learning","delivery-focus"]},"description":{"summary":"Develop, optimize, and maintain data-driven applications using Python, SQL, and Databricks in a fast-paced, US time zone–aligned environment.","key_value_proposition":"Work on scalable data solutions leveraging Python, SQL, and Databricks while collaborating with cross-functional teams to deliver business-critical insights.","full_text":"Pradeepit Global Consulting is hiring a Python, SQL, and Databricks Developer with 4–5 years of experience to design, build, and optimize data solutions. The role involves Databricks-based ETL development, SQL performance tuning, reporting support, and close collaboration with analytics and business teams."},"responsibilities":{"primary":["Design, develop, test, and deploy scalable data solutions using Python","Develop and maintain ETL pipelines using Databricks notebooks","Write, optimize, and maintain complex SQL queries","Implement and optimize data pipelines for transformation and integration","Collaborate with data scientists and analysts to deliver actionable insights"],"secondary":["Perform database performance tuning and optimization","Manage code using Git and participate in code reviews","Create and maintain technical documentation","Support reporting and dashboard development","Continuously learn and adopt new tools and best practices"],"time_allocation":{"coding":60,"mentoring":10,"meetings":30}},"requirements":{"must_have":{"experience_years":4,"technical_skills":["Python","SQL","Databricks","ETL","Git"],"domain_expertise":["Data Engineering","Data Processing","Analytics Support"],"soft_skills":["problem-solving","communication","attention to detail","collaboration"]},"nice_to_have":{"education":["Bachelor’s degree in Computer Science, IT, or related field"],"technical_skills":["Reporting tools","Dashboard development"],"domain_expertise":["Big data analytics"]},"deal_breakers":["No hands-on Databricks experience","Weak SQL proficiency"]},"technical_assessment_focus":{"coding":["Python development","SQL query optimization","Databricks ETL implementation"],"system_design":["Databricks-based data pipelines","Relational database design"],"domain_knowledge":["ETL processes","Data transformation and integration"],"problem_solving":["Pipeline optimization","Database performance issues","Data consistency challenges"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":"Bangalore / Pune, India","remote_policy":"onsite","required_onsite_days":5,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Managerial discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Technical Panel"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"high","last_updated":null},"embedding_optimization":{"primary_text":"Mid-level Python, SQL, and Databricks Developer role at Pradeepit Global Consulting requiring 4–5 years experience. Skills include Python, SQL, Databricks, ETL, and data pipeline development. Location: Bangalore or Pune, US time zone aligned.","semantic_tags":["python","sql","databricks","data-engineering","mid-level","onsite","us-timezone","bangalore","pune"]}}
{"id":"job_generic_databricks_engineer_023","role":{"title":"Databricks Engineer","level":"mid","department":"Data & Analytics","team":"Data Engineering","reports_to":"Data Engineering Manager"},"company":{"name":null,"size":null,"industry":"Technology / Data Platforms","culture_keywords":["collaboration","engineering-excellence","growth-oriented","innovation"]},"description":{"summary":"Design, build, and optimize scalable data pipelines using Databricks, Apache Spark, and cloud platforms to support analytics and machine learning initiatives.","key_value_proposition":"Work on modern cloud-native big data platforms, delivering reliable and high-performance Databricks pipelines while collaborating with analytics and ML teams.","full_text":"This Databricks Engineer role focuses on building scalable data pipelines, optimizing Spark workloads, managing Delta Lake tables, and supporting analytics and machine learning use cases across Azure, AWS, or GCP environments."},"responsibilities":{"primary":["Design, develop, and maintain scalable data pipelines using Databricks and Apache Spark","Build and optimize ETL/ELT workflows for large-scale datasets","Develop and manage Delta Lake tables ensuring data quality and performance","Integrate data from APIs, databases, and cloud storage","Collaborate with data scientists and analysts to support analytics and ML workloads"],"secondary":["Implement CI/CD, version control, and automation best practices","Monitor Databricks clusters and jobs and troubleshoot performance issues","Apply data warehousing and data lake architectural best practices","Contribute to documentation and engineering standards"],"time_allocation":{"coding":60,"mentoring":10,"meetings":30}},"requirements":{"must_have":{"experience_years":3,"technical_skills":["Databricks","Apache Spark","PySpark","Scala","Delta Lake","SQL","Cloud Platforms","CI/CD","Git"],"domain_expertise":["Data Engineering","Big Data Processing","Data Warehousing","Data Lake Architecture"],"soft_skills":["problem-solving","communication","collaboration","analytical thinking"]},"nice_to_have":{"education":[],"technical_skills":["Unity Catalog","MLflow","Kafka","Structured Streaming","Airflow","Azure Data Factory","AWS Glue"],"domain_expertise":["Machine Learning enablement","Streaming data pipelines"]},"deal_breakers":["No hands-on Databricks experience","Weak Spark or SQL proficiency"]},"technical_assessment_focus":{"coding":["PySpark / Scala development","SQL transformations","Delta Lake implementation"],"system_design":["Databricks-based data platform architecture","ETL/ELT pipeline design"],"domain_knowledge":["Spark internals","Delta Lake reliability and performance","Cloud-native data architectures"],"problem_solving":["Pipeline failures","Cluster performance tuning","Data quality issues"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":null,"remote_policy":"onsite / hybrid","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Managerial discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Technical Panel"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Databricks Engineer role requiring 3+ years experience. Skills include Databricks, Apache Spark, PySpark/Scala, Delta Lake, SQL, cloud platforms (Azure/AWS/GCP), and CI/CD.","semantic_tags":["databricks","spark","delta-lake","data-engineering","mid-level","cloud","etl"]}}
{"id":"job_databricks_lakehouse_data_engineer_024","role":{"title":"Databricks Data Engineer","level":"senior","department":"Data & Analytics","team":"Data Engineering","reports_to":"Data Engineering Manager / Lead"},"company":{"name":null,"size":null,"industry":"Technology / Data Platforms","culture_keywords":["ownership","high-performance","innovation","user-centric"]},"description":{"summary":"Design, build, and optimize scalable Databricks Lakehouse data pipelines powering analytics, AI/ML models, and business insights.","key_value_proposition":"Own end-to-end Databricks Lakehouse implementations, transforming raw data into high-value datasets with strong governance, performance, and scalability.","full_text":"This role focuses on building scalable ETL/ELT pipelines on the Databricks Lakehouse platform using PySpark, SQL, Delta Lake, and streaming frameworks, while enforcing governance, security, and CI/CD best practices across cloud environments."},"responsibilities":{"primary":["Develop scalable ETL/ELT pipelines using PySpark, SQL, and Delta Lake","Build and maintain batch and streaming pipelines using Databricks Workflows","Design and optimize Lakehouse and Delta Lake architectures","Integrate data from APIs, databases, cloud storage, and third-party platforms","Implement data quality, reliability, and monitoring using Delta Live Tables"],"secondary":["Implement CI/CD pipelines using Git, Databricks Repos, and DevOps tools","Enforce governance, security, and compliance using Unity Catalog and RBAC","Optimize Spark workloads for performance and cost efficiency","Collaborate with analytics, AI/ML, and business teams","Lead by example with high-quality, maintainable code standards"],"time_allocation":{"coding":55,"mentoring":20,"meetings":25}},"requirements":{"must_have":{"experience_years":4,"technical_skills":["Databricks","PySpark","Apache Spark","SQL","Delta Lake","Databricks Workflows","Unity Catalog","CI/CD","Cloud Platforms"],"domain_expertise":["Data Engineering","Lakehouse Architecture","Distributed Systems","Streaming Data Processing"],"soft_skills":["problem-solving","ownership","leadership-by-example","communication"]},"nice_to_have":{"education":[],"technical_skills":["Databricks Photon","Serverless SQL","ML Feature Engineering","Kafka","Structured Streaming","Auto Loader","Power BI","Tableau","Looker"],"domain_expertise":["Machine Learning enablement","Advanced performance optimization"]},"deal_breakers":["No hands-on Databricks or Spark experience","Weak Delta Lake or SQL fundamentals"]},"technical_assessment_focus":{"coding":["PySpark and Spark SQL development","Delta Lake table design","Streaming pipeline implementation"],"system_design":["Databricks Lakehouse architecture","Batch and streaming data platforms","Governed cloud data systems"],"domain_knowledge":["Distributed systems","Delta Live Tables","Unity Catalog governance"],"problem_solving":["Pipeline reliability issues","Performance bottlenecks","Streaming data failures"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":null,"remote_policy":"onsite / hybrid","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","System design discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Technical Leadership"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Databricks Data Engineer role focused on Lakehouse architecture, PySpark, SQL, Delta Lake, streaming pipelines, Unity Catalog, and CI/CD across Azure, AWS, or GCP.","semantic_tags":["databricks","lakehouse","delta-lake","pyspark","streaming","unity-catalog","data-engineering","senior-level"]}}
{"id":"job_zodiac_senior_data_engineer_azure_databricks_025","role":{"title":"Senior Data Engineer – Azure Databricks","level":"senior","department":"Data & Analytics","team":"Data Engineering","reports_to":"Data Engineering Manager / Architect"},"company":{"name":"Zodiac Maritime","size":null,"industry":"Maritime & Shipping","culture_keywords":["data-transformation","collaboration","engineering-excellence","quality-first"]},"description":{"summary":"Design, build, and optimize scalable Azure Databricks data pipelines to enable faster, data-driven decision-making as part of an enterprise data transformation journey.","key_value_proposition":"Lead Azure Databricks Lakehouse implementations with strong focus on Medallion architecture, data quality, governance, and performance optimization across the Azure data stack.","full_text":"Zodiac Maritime is seeking a Senior Data Engineer – Azure Databricks to design and implement scalable batch and streaming data pipelines using Azure Databricks, Delta Lake, and Data Factory. The role emphasizes Medallion architecture, data quality, governance, and close collaboration with analytics and business teams."},"responsibilities":{"primary":["Design and implement robust batch and streaming data pipelines using Azure Databricks and Apache Spark","Apply Medallion architecture to structure raw, enriched, and curated data layers","Ensure data quality, consistency, and governance using Azure Purview and Unity Catalog","Optimize Spark jobs, Delta Lake tables, and SQL queries for performance and cost efficiency","Collaborate with analysts, architects, and business teams to deliver end-to-end data solutions"],"secondary":["Build scalable ETL/ELT pipelines and data transformations","Implement CI/CD pipelines using Git and DevOps tools","Support data visualization and analytics use cases","Contribute to documentation and data engineering standards","Stay aware of emerging data technologies and patterns"],"time_allocation":{"coding":55,"mentoring":15,"meetings":30}},"requirements":{"must_have":{"experience_years":5,"technical_skills":["Azure Databricks","Delta Lake","Azure Data Factory","Python","PySpark","SQL","CI/CD","Git"],"domain_expertise":["Data Engineering","Lakehouse Architecture","Medallion Design Patterns","Data Quality and Governance"],"soft_skills":["problem-solving","analytical thinking","communication","collaboration"]},"nice_to_have":{"education":["Degree in Computer Science, Data Engineering, Information Systems, or related field"],"technical_skills":["Power BI","Kafka","Azure Event Hubs","Azure Purview"],"domain_expertise":["IoT data pipelines","Streaming architectures","Knowledge Graphs"]},"deal_breakers":["No hands-on Azure Databricks experience","Weak Spark or SQL optimization skills"]},"technical_assessment_focus":{"coding":["PySpark and Spark SQL development","Delta Lake optimization","SQL query tuning"],"system_design":["Azure Databricks Lakehouse architecture","Medallion data architecture","Batch and streaming pipeline design"],"domain_knowledge":["Data governance frameworks","Azure data services","ETL/ELT best practices"],"problem_solving":["Performance bottlenecks","Data quality issues","Pipeline reliability challenges"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":"Nagpur / Pune / Chennai / Bangalore, India","remote_policy":"onsite / hybrid","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Architectural discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Data Engineering Leadership"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Senior Data Engineer – Azure Databricks role at Zodiac Maritime requiring 5+ years experience. Skills include Azure Databricks, Delta Lake, Data Factory, PySpark, SQL, Medallion architecture, data quality, and governance.","semantic_tags":["azure-databricks","delta-lake","medallion-architecture","data-engineering","senior-level","azure","governance"]}}
{"id":"job_pune_azure_databricks_developer_026","role":{"title":"Developer (Azure Databricks)","level":"mid","department":"Technology","team":"Data Engineering","reports_to":"Engineering Manager"},"company":{"name":null,"size":null,"industry":"Technology / Financial Services","culture_keywords":["engineering-excellence","cloud-first","collaboration"]},"description":{"summary":"Develop and migrate cloud data warehouse solutions using Azure Databricks, Azure Data Factory, and modern data warehousing practices.","key_value_proposition":"Work on hands-on Azure Databricks development and migration initiatives, building scalable data platforms with strong engineering and data warehousing foundations.","full_text":"This role focuses on Azure Databricks and Azure Data Factory development, applying object-oriented programming principles, data warehousing concepts, and cloud migration experience to deliver scalable data solutions. The position emphasizes hands-on development, source control best practices, and Azure-native services."},"responsibilities":{"primary":["Develop and migrate data solutions using Azure Databricks and Azure Data Factory","Apply object-oriented programming, data structures, and multi-threading concepts","Build and support cloud data warehouse solutions on Microsoft Azure","Implement data warehousing concepts and ETL/data integration patterns","Use Git for source control including branches, pull requests, and repositories"],"secondary":["Work with Azure services including SQL, T-SQL, Storage, Service Fabric, and Purview","Contribute to Azure migration or cloud-native development initiatives","Analyze data and support data lake population strategies","Create clear and concise technical documentation","Collaborate with cross-functional technology teams"],"time_allocation":{"coding":60,"mentoring":10,"meetings":30}},"requirements":{"must_have":{"experience_years":3,"technical_skills":["Azure Databricks","Azure Data Factory","Azure SQL","T-SQL","Git","Object-Oriented Programming","Data Warehousing"],"domain_expertise":["Cloud Data Warehousing","ETL / Data Integration","Azure Cloud Platforms"],"soft_skills":["problem-solving","analytical thinking","communication"]},"nice_to_have":{"education":["Bachelor of Engineering"],"technical_skills":["ETL Tools","Azure Purview","Service Fabric"],"domain_expertise":["Banking Domain","Data Lake Architecture"]},"deal_breakers":["No Azure Databricks or ADF experience","Lack of Git-based development experience"]},"technical_assessment_focus":{"coding":["Object-oriented programming","SQL and T-SQL development","Azure Databricks pipelines"],"system_design":["Azure cloud data warehouse architecture","ADF and Databricks integration"],"domain_knowledge":["Data warehousing concepts","Azure migration strategies"],"problem_solving":["Data integration issues","Migration challenges","Performance tuning"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":"Pune, India","remote_policy":"onsite","required_onsite_days":5,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Managerial discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Technical Panel"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":"384099","hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Azure Databricks Developer role in Pune requiring experience in Azure Data Factory, Databricks, data warehousing, Git, and cloud migration. Skills include OOP, SQL/T-SQL, and Azure services.","semantic_tags":["azure-databricks","azure-data-factory","data-warehousing","developer","mid-level","pune","cloud-migration"]}}
{"id":"job_deloitte_analyst_databricks_027","role":{"title":"Analyst / Senior Analyst – Databricks & PySpark","level":"mid","department":"Information Technology Services","team":"Solutions Delivery – Data Engineering","reports_to":"Data Engineering Manager"},"company":{"name":"Deloitte","size":"100000+","industry":"Professional Services & Consulting","culture_keywords":["impact","inclusion","learning","client-excellence"]},"description":{"summary":"Design, build, and optimize enterprise-grade Azure Databricks data pipelines using PySpark, SQL, and ADF while supporting large-scale cloud migration initiatives.","key_value_proposition":"Work on complex enterprise data engineering and cloud migration projects at Deloitte, gaining exposure to SAP HANA migrations, governance, DevOps, and SAFe Agile delivery.","full_text":"Deloitte is seeking an Analyst / Senior Analyst with strong experience in PySpark and Databricks to design, develop, and optimize ETL/ELT pipelines on Azure. The role involves enterprise data lake development, SAP HANA migrations, performance tuning, governance, and collaboration within SAFe Agile teams."},"responsibilities":{"primary":["Design and build scalable ETL/ELT pipelines using Databricks and Apache Spark","Integrate data from on-prem, cloud, APIs, and third-party systems into enterprise data lakes","Support migration of legacy systems including SAP HANA to Azure Databricks and Delta Lake","Cleanse, transform, and prepare datasets for analytics and machine learning","Develop and optimize SQL-based transformations and data models"],"secondary":["Optimize Spark workloads for performance, scalability, and cost","Support data governance, security, lineage, and auditability initiatives","Implement automation and CI/CD using Azure DevOps","Document pipeline logic, ETL processes, and data flows","Collaborate within SAFe Agile teams and provide transparent status updates"],"time_allocation":{"coding":60,"mentoring":10,"meetings":30}},"requirements":{"must_have":{"experience_years":4,"technical_skills":["Databricks","PySpark","Apache Spark","SQL","Azure Data Factory","Azure DevOps","Delta Lake","ETL"],"domain_expertise":["Data Engineering","Cloud Migration","Enterprise Data Warehousing","ETL/ELT"],"soft_skills":["communication","problem-solving","collaboration","ownership"]},"nice_to_have":{"education":["Computer Science degree or equivalent experience"],"technical_skills":["SAP HANA","Scala","Kafka","DataStage"],"domain_expertise":["Data Governance","SAFe Agile","Banking or enterprise domains"]},"deal_breakers":["No Databricks or PySpark experience","Lack of Azure data engineering exposure"]},"technical_assessment_focus":{"coding":["PySpark transformations","SQL optimization","ADF pipeline development"],"system_design":["Azure Databricks lakehouse architecture","Enterprise data lake migration design"],"domain_knowledge":["SAP HANA to cloud migration","Data governance and security","Batch and streaming ETL patterns"],"problem_solving":["Pipeline failures","Performance bottlenecks","Data quality and validation issues"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":true,"target_percent":null}},"logistics":{"location":"Hyderabad, India","remote_policy":"onsite / hybrid","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Managerial discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Technical Panel"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Analyst / Senior Analyst Databricks role at Deloitte requiring 4–5 years experience. Skills include PySpark, Databricks, Azure Data Factory, SQL, Delta Lake, SAP HANA migration, and enterprise ETL pipelines. Location: Hyderabad.","semantic_tags":["deloitte","databricks","pyspark","azure","etl","sap-hana","data-engineering","mid-level"]}}
{"id":"job_databricks_data_engineer_028","role":{"title":"Data Engineer + Databricks Developer","level":"senior","department":"Engineering","team":"Data Engineering","reports_to":"Data Engineering Manager"},"company":{"name":null,"size":null,"industry":"Technology","culture_keywords":["modern-data-stack","automation","scalability","clean-code"]},"description":{"summary":"Design, build, and optimize scalable batch and streaming data pipelines using Databricks, PySpark, Delta Lake, and cloud-native services across Azure and AWS.","key_value_proposition":"Lead modern lakehouse-based data engineering initiatives using Databricks, Unity Catalog, CI/CD automation, and cloud-native architectures.","full_text":"The Databricks Developer role focuses on building high-performance, scalable ETL/ELT pipelines using Databricks, Apache Spark, PySpark, and Delta Lake. The position involves real-time and batch processing, cloud integration with Azure and AWS, data governance using Unity Catalog, CI/CD automation, and collaboration with analytics and ML teams."},"responsibilities":{"primary":["Design, build, and optimize ETL/ELT pipelines using Databricks, PySpark, SQL, and Delta Lake","Develop reusable notebooks, jobs, and workflows for batch and streaming workloads","Implement data lakehouse and medallion architecture on Azure and AWS","Apply performance tuning, partitioning strategies, and Spark optimization techniques","Manage metadata, lineage, access control, and governance using Unity Catalog"],"secondary":["Integrate Databricks with orchestration tools such as Azure Data Factory, AWS Glue, or Airflow","Build and maintain CI/CD pipelines using GitLab or similar DevOps tools","Collaborate with data scientists and business users for analytics and ML pipelines","Monitor Databricks jobs, clusters, and compute usage for cost optimization","Maintain documentation for pipelines, job configurations, and data logic"],"time_allocation":{"coding":65,"mentoring":10,"meetings":25}},"requirements":{"must_have":{"experience_years":5,"technical_skills":["Databricks","PySpark","Apache Spark","Delta Lake","Python","SQL","Azure Data Factory","Azure Synapse","AWS S3","AWS Glue","CI/CD","GitLab","Unity Catalog"],"domain_expertise":["Data Engineering","Lakehouse Architecture","ETL/ELT","Distributed Computing"],"soft_skills":["communication","problem-solving","collaboration","ownership"]},"nice_to_have":{"education":["Bachelor’s or Master’s in Computer Science or related field"],"technical_skills":["MLflow","Databricks Workflows","Job Clusters","Power BI","Tableau","Data Observability Tools"],"domain_expertise":["Machine Learning Pipelines","Data Governance","Analytics Enablement"]},"deal_breakers":["No Databricks or PySpark experience","Lack of Spark performance tuning knowledge"]},"technical_assessment_focus":{"coding":["PySpark transformations","Delta Lake optimizations","SQL analytics queries"],"system_design":["Databricks lakehouse architecture","Batch and streaming pipeline design","CI/CD for data platforms"],"domain_knowledge":["Spark internals","Delta Lake ACID properties","Unity Catalog governance"],"problem_solving":["Pipeline failures","Performance bottlenecks","Cost optimization"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":true,"target_percent":null}},"logistics":{"location":"Hyderabad, India","remote_policy":"onsite / hybrid","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical Databricks interview","Managerial discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Data Engineering Lead"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Senior Data Engineer + Databricks Developer role in Hyderabad requiring 5+ years experience. Skills include Databricks, PySpark, Delta Lake, Apache Spark, Azure Data Factory, AWS S3/Glue, Unity Catalog, CI/CD, and lakehouse architecture.","semantic_tags":["databricks","data-engineer","pyspark","delta-lake","azure","aws","lakehouse","senior"]}}
{"id":"job_databricks_engineer_adf_029","role":{"title":"Databricks Engineer","level":"mid-senior","department":"Engineering","team":"Data Engineering","reports_to":"Data Engineering Manager"},"company":{"name":null,"size":null,"industry":"Technology / Financial Services","culture_keywords":["data-driven","collaboration","delivery-focused"]},"description":{"summary":"Design and develop scalable ETL pipelines using Azure Data Factory and Databricks, enabling robust data ingestion, transformation, and analytics on Azure.","key_value_proposition":"Work on enterprise-grade Azure data engineering solutions combining Databricks, ADF, and Synapse to translate business requirements into scalable data platforms.","full_text":"As a Databricks Engineer, the role focuses on designing and developing ETL pipelines using Azure Data Factory and Databricks. Responsibilities include collaborating with Azure data services, writing SQL, Python, and PySpark code, translating business requirements into technical designs, and ensuring smooth project execution through stakeholder communication."},"responsibilities":{"primary":["Design and develop ETL pipelines using Azure Data Factory for data ingestion and transformation","Build data solutions using Azure Data Lake and Azure SQL / Synapse","Write SQL, Python, and PySpark code for efficient data processing","Translate business requirements into technical designs and data models","Develop mapping documents and transformation rules"],"secondary":["Collaborate with stakeholders to communicate project status and progress","Support CI/CD deployments for data pipelines","Ensure data quality and reliability across pipelines","Participate in design discussions and documentation"],"time_allocation":{"coding":60,"mentoring":10,"meetings":30}},"requirements":{"must_have":{"experience_years":4,"technical_skills":["Azure Databricks","Azure Data Factory","PySpark","Python","SQL","Azure Data Lake Storage","Azure Synapse","Azure SQL Database"],"domain_expertise":["Data Engineering","ETL Pipelines","Big Data Processing","Azure Cloud"],"soft_skills":["communication","requirements-analysis","stakeholder-management"]},"nice_to_have":{"education":["Bachelor’s degree in Computer Science or related field"],"technical_skills":["CI/CD","DevOps Tools"],"domain_expertise":["Insurance","Financial Services"]},"deal_breakers":["No Azure Databricks experience","No hands-on ETL pipeline development"]},"technical_assessment_focus":{"coding":["PySpark transformations","SQL queries","ADF pipeline development"],"system_design":["Azure-based ETL architecture","Databricks and ADF integration"],"domain_knowledge":["Big data ingestion","Relational and analytical data processing"],"problem_solving":["Pipeline failures","Data transformation issues","Deployment issues"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":true,"target_percent":null}},"logistics":{"location":"India","remote_policy":"onsite / hybrid","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Managerial discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Data Engineering Lead"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Databricks Engineer role requiring 4–10 years experience with Azure Databricks, Azure Data Factory, PySpark, Python, SQL, ADLS, and Synapse. Focus on ETL pipelines, data ingestion, and transformation.","semantic_tags":["databricks","azure-data-factory","pyspark","etl","azure","data-engineering","mid-senior"]}}
{"id":"job_virtusa_databricks_engineer_030","role":{"title":"Databricks Engineer","level":"mid-senior","department":"Engineering","team":"Data Engineering","reports_to":"Data Engineering Manager"},"company":{"name":"Virtusa","size":"27000+","industry":"IT Services and Consulting","culture_keywords":["collaboration","growth","innovation","inclusion"]},"description":{"summary":"Design, develop, and optimize scalable big data pipelines using Apache Spark on Databricks, delivering high-quality, governed datasets for analytics and ML use cases.","key_value_proposition":"Work on enterprise-scale Databricks lakehouse platforms using Spark, Delta Lake, Unity Catalog, and modern CI/CD practices in a collaborative global environment.","full_text":"Virtusa is seeking a Databricks Engineer with strong Spark and big data expertise to build and optimize scalable ETL pipelines. The role focuses on PySpark/Scala development, Delta Lake implementation, data governance, performance optimization, and collaboration with analytics and data science teams."},"responsibilities":{"primary":["Design, develop, and maintain scalable data pipelines using Apache Spark on Databricks","Write production-grade PySpark or Scala code for ETL and data transformations","Implement Delta Lake features including schema evolution and data versioning","Integrate structured and unstructured data into unified analytics platforms","Optimize Spark workflows for performance, scalability, and cost efficiency"],"secondary":["Implement data quality checks, validation routines, and logging","Monitor and debug production Databricks jobs, notebooks, and clusters","Ensure data security, privacy, and compliance across pipelines","Collaborate with data scientists, analysts, and business stakeholders","Mentor and guide junior team members"],"time_allocation":{"coding":65,"mentoring":10,"meetings":25}},"requirements":{"must_have":{"experience_years":4,"technical_skills":["Databricks","Apache Spark","PySpark","Scala","Delta Lake","SQL","Unity Catalog","Delta Live Tables"],"domain_expertise":["Big Data Engineering","ETL Pipelines","Lakehouse Architecture","Distributed Computing"],"soft_skills":["collaboration","communication","problem-solving","mentorship"]},"nice_to_have":{"education":["Bachelor’s degree in Computer Science or related field"],"technical_skills":["Azure Data Lake","Azure Data Factory","Azure Synapse","AWS S3","AWS Glue","Redshift","CI/CD","GitHub Actions","Azure DevOps"],"domain_expertise":["Machine Learning Workflows","Streaming Systems","Agile/Scrum"]},"deal_breakers":["No hands-on Databricks experience","Lack of Spark-based ETL development"]},"technical_assessment_focus":{"coding":["PySpark/Scala transformations","SQL analytics queries","Delta Lake implementation"],"system_design":["Databricks lakehouse architecture","Batch and streaming data pipelines"],"domain_knowledge":["Delta Lake internals","Data governance and cataloging","Cloud-native data platforms"],"problem_solving":["Performance tuning","Job failures","Schema evolution issues"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":true,"target_percent":null}},"logistics":{"location":"India","remote_policy":"onsite / hybrid","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical Databricks interview","Managerial discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Data Engineering Lead"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Databricks Engineer role at Virtusa requiring 4–6 years experience. Skills include Apache Spark, Databricks, PySpark/Scala, Delta Lake, Unity Catalog, CI/CD, and cloud platforms Azure or AWS.","semantic_tags":["virtusa","databricks","spark","pyspark","delta-lake","big-data","mid-senior","data-engineering"]}}
{"id":"job_mindcurv_senior_data_engineer_031","role":{"title":"Senior Data Engineer","level":"senior","department":"Data & Analytics","team":"Data Engineering","reports_to":"Data Engineering Lead"},"company":{"name":"Mindcurv (Part of Accenture Song)","size":null,"industry":"Digital Transformation & Consulting","culture_keywords":["innovation","agile","transparency","continuous-learning"]},"description":{"summary":"Design and build scalable cloud-native data pipelines and lakehouse architectures using Databricks to enable advanced analytics and machine learning.","key_value_proposition":"Work on cutting-edge Databricks lakehouse solutions in a global, agile environment with strong focus on governance, performance, and innovation.","full_text":"Mindcurv is seeking a Senior Data Engineer with deep expertise in Databricks and cloud data warehousing. The role focuses on building scalable ETL pipelines, architecting lakehouse solutions, enforcing governance and security, and mentoring engineers while collaborating with global stakeholders."},"responsibilities":{"primary":["Design, build, and maintain scalable ETL pipelines using Databricks","Architect and manage cloud-based data warehousing solutions using Lakehouse architecture","Develop optimized data lake architectures for analytics and ML use cases","Optimize data pipelines for performance and cost efficiency","Implement data governance, access control, security, and compliance practices"],"secondary":["Collaborate with stakeholders to gather requirements and deliver high-quality data solutions","Monitor and troubleshoot data pipelines for reliability and accuracy","Lead and mentor junior engineers","Support ML and advanced analytics workflows","Communicate effectively with global client teams"],"time_allocation":{"coding":55,"mentoring":20,"meetings":25}},"requirements":{"must_have":{"experience_years":5,"technical_skills":["Databricks","Apache Spark","Delta Lake","Unity Catalog","Python","SQL","PySpark","Lakehouse Architecture","CI/CD","Git"],"domain_expertise":["Data Engineering","Cloud Data Warehousing","Analytics Enablement","Data Governance"],"soft_skills":["communication","leadership","collaboration","ownership"]},"nice_to_have":{"education":["Bachelor’s or Master’s degree in Computer Science or related field"],"technical_skills":["AWS","Airflow","MLOps","Power BI"],"domain_expertise":["Machine Learning Workflows","GDPR","CCPA Compliance"]},"deal_breakers":["No hands-on Databricks experience","Lack of cloud-based data engineering background"]},"technical_assessment_focus":{"coding":["PySpark ETL pipelines","SQL transformations","Delta Lake optimizations"],"system_design":["Databricks Lakehouse architecture","Cloud data warehouse design"],"domain_knowledge":["Data modeling","Governance and security","Distributed data processing"],"problem_solving":["Pipeline performance issues","Cost optimization","Data reliability"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"EUR/INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":true,"target_percent":null}},"logistics":{"location":"India / Europe (Hybrid)","remote_policy":"hybrid","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical Databricks interview","Client / Managerial discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Data Engineering Lead"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Senior Data Engineer role at Mindcurv requiring 5+ years experience. Skills include Databricks, Spark, Delta Lake, Unity Catalog, AWS, Python, SQL, lakehouse architecture, and data governance.","semantic_tags":["mindcurv","accenture-song","databricks","senior-data-engineer","lakehouse","aws","data-engineering"]}}
{"id":"job_databricks_developer_multi_cloud_032","role":{"title":"Databricks Developer","level":"mid-senior","department":"Engineering","team":"Data Engineering","reports_to":"Data Engineering Manager"},"company":{"name":null,"size":null,"industry":"Technology","culture_keywords":["scalability","cloud-native","performance","data-driven"]},"description":{"summary":"Build and optimize scalable batch and streaming data pipelines using Databricks, Apache Spark, and Delta Lake across multi-cloud environments.","key_value_proposition":"Work on modern lakehouse architectures using Databricks across AWS, Azure, and GCP, focusing on performance, governance, and cost-efficient data engineering.","full_text":"The Databricks Developer role focuses on designing and developing high-performance data pipelines using Databricks, Spark, and Delta Lake. The position involves batch and streaming ETL/ELT development, multi-cloud integration, data modeling, governance, and collaboration with cross-functional teams."},"responsibilities":{"primary":["Develop scalable and high-performance data pipelines using Databricks (PySpark/Scala)","Implement Delta Lake storage layers and advanced Lakehouse architecture","Build batch and streaming ETL/ELT pipelines for ingestion and transformation","Create and maintain Databricks notebooks, clusters, jobs, and workflows","Perform data modeling, schema design, and BI integration"],"secondary":["Optimize pipelines for performance, reliability, and cost efficiency","Ensure data quality, monitoring, and reliability standards","Apply security, governance, and cluster optimization best practices","Collaborate with data engineers, architects, and business stakeholders","Support CI/CD and version control workflows"],"time_allocation":{"coding":65,"mentoring":10,"meetings":25}},"requirements":{"must_have":{"experience_years":5,"technical_skills":["Databricks","Apache Spark","PySpark","Scala","Delta Lake","SQL","ETL/ELT","Git","CI/CD"],"domain_expertise":["Data Engineering","Lakehouse Architecture","Distributed Systems","Cloud Data Platforms"],"soft_skills":["collaboration","communication","problem-solving","ownership"]},"nice_to_have":{"education":["Bachelor’s degree in Computer Science or related field"],"technical_skills":["AWS Glue","AWS S3","AWS EMR","AWS Lambda","Azure Data Factory","Azure Synapse","ADLS","GCP BigQuery","Dataproc"],"domain_expertise":["Streaming Data Processing","BI Integration","Multi-cloud Architecture"]},"deal_breakers":["No hands-on Databricks experience","Lack of Spark-based data pipeline development"]},"technical_assessment_focus":{"coding":["PySpark/Scala ETL pipelines","SQL optimization","Delta Lake implementation"],"system_design":["Multi-cloud Databricks lakehouse architecture","Batch and streaming pipeline design"],"domain_knowledge":["Delta Lake internals","ETL orchestration","Cloud-native data services"],"problem_solving":["Performance tuning","Pipeline failures","Cost optimization"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":true,"target_percent":null}},"logistics":{"location":"Hyderabad, India","remote_policy":"onsite / hybrid","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical Databricks interview","Managerial discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Data Engineering Lead"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"high","last_updated":null},"embedding_optimization":{"primary_text":"Databricks Developer role in Hyderabad requiring 5–11 years experience. Skills include Databricks, Apache Spark, PySpark/Scala, Delta Lake, SQL, ETL/ELT, and multi-cloud platforms AWS, Azure, and GCP.","semantic_tags":["databricks","spark","pyspark","delta-lake","multi-cloud","aws","azure","gcp","data-engineering"]}}
{"id":"job_databricks_senior_developer_chennai_033","role":{"title":"Senior Databricks Developer","level":"senior","department":"Engineering","team":"Data Engineering","reports_to":"Technical Lead / Engineering Manager"},"company":{"name":null,"size":null,"industry":"IT Software & Services","culture_keywords":["teamwork","long-term-engagement","mentorship","problem-solving"]},"description":{"summary":"Design, develop, and maintain scalable big data solutions using Databricks, Apache Spark, and modern data engineering tools in a long-term offshore engagement.","key_value_proposition":"Long-term senior Databricks role focused on solution design, mentoring, and large-scale Spark-based data engineering projects.","full_text":"This role is for an offshore Senior Databricks Developer responsible for designing and developing data-driven solutions using Databricks, Spark, and Big Data technologies. The position emphasizes teamwork, solution design, code reviews, mentoring junior engineers, and long-term project ownership."},"responsibilities":{"primary":["Design and develop scalable data-driven solutions using Databricks and Apache Spark","Write production-grade PySpark, Scala, and SQL code","Work on batch and streaming data pipelines using Spark Streaming and Kafka","Perform solution design, code reviews, and technical guidance","Mentor junior engineers and support team skill development"],"secondary":["Collaborate with developers, project managers, and engineers across teams","Troubleshoot complex data and performance issues","Ensure best practices in coding, testing, and documentation","Work with Linux-based environments and big data toolsets"],"time_allocation":{"coding":60,"mentoring":20,"meetings":20}},"requirements":{"must_have":{"experience_years":5,"technical_skills":["Databricks","Apache Spark","PySpark","Scala","SQL","Spark Streaming","Kafka","Big Data Tools","Linux","Python"],"domain_expertise":["Big Data Engineering","Data-Driven Applications","Backend Development"],"soft_skills":["analytical-thinking","problem-solving","teamwork","communication"]},"nice_to_have":{"education":["BCA","BSc","BE"],"technical_skills":["Advanced Kafka Streaming","Cloud Platforms"],"domain_expertise":["Offshore Delivery Models","Long-Term Support Projects"]},"deal_breakers":["No Databricks experience","Lack of Spark or Big Data background"]},"technical_assessment_focus":{"coding":["PySpark and Scala transformations","SQL query optimization","Spark Streaming jobs"],"system_design":["Big data solution design","Batch and streaming architectures"],"domain_knowledge":["Kafka integration","Linux environments","Distributed processing"],"problem_solving":["Performance tuning","Streaming failures","Data consistency issues"]},"compensation":{"salary_range":{"min":600000,"max":2000000,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":true,"target_percent":null}},"logistics":{"location":"Chennai, India","remote_policy":"offshore / onsite","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Managerial discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Technical Lead"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Senior Databricks Developer role in Chennai requiring 5–11 years experience. Skills include Databricks, PySpark, Scala, Spark Streaming, Kafka, SQL, Linux, and big data toolsets.","semantic_tags":["databricks","senior-developer","spark","pyspark","scala","kafka","big-data","chennai"]}}
{"id":"job_tenjumps_databricks_data_engineer_036","role":{"title":"Data Engineer / Databricks Developer","level":"mid","department":"Engineering","team":"Data Engineering","reports_to":"Data Engineering Lead"},"company":{"name":"Tenjumps Softech","size":null,"industry":"IT Services & Data Platforms","culture_keywords":["automation","cloud-native","ownership","continuous-improvement"]},"description":{"summary":"Build and operate scalable cloud-native data platforms using Databricks, Delta Lake, and modern ETL/ELT frameworks to deliver mission-critical data pipelines.","key_value_proposition":"Own end-to-end data pipelines on Databricks while working across engineering and data science teams to enable analytics and ML at scale.","full_text":"Tenjumps Softech is looking for a Data Engineer / Databricks Developer to design and build scalable ETL/ELT pipelines using Databricks and Delta Lake. The role involves cloud-native data platform development, data lake and warehouse modeling, ML pipeline enablement, and CI/CD-driven automation."},"responsibilities":{"primary":["Design and build ETL/ELT pipelines using Databricks and Delta Lake","Develop scalable data lakes and data warehouse models","Manage Databricks clusters, jobs, and performance tuning","Integrate data from databases, APIs, and enterprise systems (Salesforce/SAP)","Enable data quality, metadata management, and master data structures"],"secondary":["Collaborate with data scientists to operationalize and monitor ML models","Work with Azure and AWS data services such as ADLS, S3, and Redshift","Maintain technical documentation and follow engineering best practices","Champion automation, CI/CD, and continuous improvement initiatives"],"time_allocation":{"coding":65,"mentoring":5,"meetings":30}},"requirements":{"must_have":{"experience_years":2,"technical_skills":["Databricks","Delta Lake","Python","PySpark","SQL","Talend","DataStage","Azure DevOps","CI/CD","Git"],"domain_expertise":["Data Engineering","ETL/ELT","Distributed Systems","Cloud Data Platforms"],"soft_skills":["collaboration","ownership","problem-solving","communication"]},"nice_to_have":{"education":["Bachelor’s degree in Computer Science or related field"],"technical_skills":["AWS Redshift","Star Schema Modeling","ML Pipelines"],"domain_expertise":["Life Sciences","Pharma Analytics"],"certifications":["Databricks","Azure","AWS"]},"deal_breakers":["No Databricks or Delta Lake experience","No Talend or DataStage exposure"]},"technical_assessment_focus":{"coding":["PySpark ETL pipelines","SQL transformations","Data ingestion logic"],"system_design":["Databricks lakehouse architecture","Cloud-native data platform design"],"domain_knowledge":["Distributed systems","Data modeling","Metadata and data quality"],"problem_solving":["Pipeline failures","Performance tuning","Integration issues"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":true,"target_percent":null}},"logistics":{"location":"India","remote_policy":"onsite / hybrid","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical Databricks interview","Managerial discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Data Engineering Lead"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Data Engineer / Databricks Developer role at Tenjumps Softech requiring 2–4 years experience. Skills include Databricks, Delta Lake, PySpark, SQL, Talend, DataStage, Azure DevOps, and cloud data platforms.","semantic_tags":["tenjumps","databricks","data-engineer","delta-lake","pyspark","etl","azure-devops","mid-level"]}}
{"id":"job_infosys_databricks_consultant_038","role":{"title":"Databricks Consultant","level":"mid-senior","department":"Consulting","team":"Digital Transformation","reports_to":"Project Manager"},"company":{"name":"Infosys","size":"enterprise","industry":"IT Services & Consulting","culture_keywords":["client-centric","digital-transformation","innovation","collaboration"]},"description":{"summary":"Consulting-focused Databricks role at Infosys involving solution design, client engagement, proposal development, and delivery of data engineering solutions.","key_value_proposition":"Opportunity to work in a consulting role combining Databricks expertise with client-facing solution design and digital transformation initiatives.","full_text":"As part of the Infosys consulting team, the Databricks Consultant will diagnose customer problems, design innovative data engineering solutions, and support deployment to drive client success. The role includes proposal development, solution demonstrations, POCs, effort estimation, and leading small projects as part of digital transformation programs."},"responsibilities":{"primary":["Diagnose customer problem areas and design Databricks-based solutions","Develop proposal documents and contribute to solution design","Configure solutions as per design and conduct conference room pilots","Conduct solution demonstrations, POCs, and proof-of-technology workshops","Prepare effort estimates aligned with customer budgets and organizational guidelines"],"secondary":["Lead small projects and contribute to organizational initiatives","Collaborate with clients to resolve requirement and solution design queries","Assess current processes and identify improvement opportunities","Provide client-facing consulting and stakeholder communication","Support digital transformation initiatives"],"time_allocation":{"client_work":50,"solution_design":30,"project_leadership":20}},"requirements":{"must_have":{"experience_years":null,"technical_skills":["Databricks","Data Engineering"],"domain_expertise":["Consulting","Digital Transformation"],"soft_skills":["client-interfacing","problem-solving","collaboration","logical-thinking"]},"nice_to_have":{"education":["Bachelor’s degree or equivalent"],"technical_skills":["Software Configuration Management"],"domain_expertise":["Industry domain knowledge","Project Management"],"certifications":[]},"deal_breakers":["No Databricks exposure","Lack of client-facing experience"]},"technical_assessment_focus":{"coding":["Databricks fundamentals"],"system_design":["Data engineering solution design"],"domain_knowledge":["Consulting delivery models","Digital transformation"],"problem_solving":["Process improvement","Client requirement analysis"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":true,"target_percent":null}},"logistics":{"location":"India","remote_policy":"hybrid","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical Databricks interview","Consulting/Managerial round"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Practice Lead"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Infosys Databricks Consultant role focused on data engineering, solution design, client consulting, and digital transformation initiatives.","semantic_tags":["infosys","databricks","consultant","data-engineering","digital-transformation","client-facing"]}}
{"id":"job_crescendo_databricks_data_engineer_010","role":{"title":"Databricks Data Engineer","level":"mid","department":"Data & Analytics","team":"Data Engineering","reports_to":"Data Engineering Manager"},"company":{"name":"Crescendo Global (Client Confidential)","size":null,"industry":"Recruitment / Client-facing Data Engineering","culture_keywords":["diversity","inclusion","collaboration","growth"]},"description":{"summary":"Design, build, and optimize scalable data pipelines and analytics solutions using the Databricks Lakehouse Platform for advanced analytics and business intelligence.","key_value_proposition":"Work on modern Databricks Lakehouse architectures supporting large-scale analytics, ML workloads, and cross-functional business intelligence initiatives.","full_text":"An exciting opportunity for an experienced Databricks Data Engineer to design, build, and optimize scalable data pipelines and analytics solutions using Spark, PySpark, Delta Lake, and modern cloud data engineering practices."},"responsibilities":{"primary":["Develop and maintain scalable data pipelines using Databricks and Apache Spark (PySpark)","Build and optimize Delta Lake tables for analytics and machine learning workloads","Integrate data from APIs, cloud storage, databases, and streaming platforms such as Kafka","Implement ETL/ELT best practices to transform raw data into structured formats","Automate workflows using Databricks Workflows, job clusters, and schedulers","Monitor and optimize Spark jobs for performance and cost-efficiency"],"secondary":["Implement data quality checks for accuracy and consistency","Collaborate with data scientists, analysts, and business stakeholders","Document data models, pipeline designs, and technical workflows","Support analytics and BI use cases using structured datasets"],"time_allocation":{"coding":65,"mentoring":10,"meetings":25}},"requirements":{"must_have":{"experience_years":3,"technical_skills":["Databricks","Apache Spark","PySpark","Delta Lake","SQL","ETL/ELT","Kafka","Git","CI/CD"],"domain_expertise":["Data Engineering","Lakehouse Architecture","Cloud Data Platforms"],"soft_skills":["problem-solving","communication","collaboration","analytical thinking"]},"nice_to_have":{"education":["Bachelor’s degree in Computer Science or Engineering"],"technical_skills":["MLflow","Unity Catalog","Purview","Power BI","Tableau"],"domain_expertise":["Data governance","Machine learning pipelines"]},"deal_breakers":["No hands-on Databricks or Spark experience","Lack of PySpark or SQL proficiency"]},"technical_assessment_focus":{"coding":["PySpark pipeline development","SQL transformations","Delta Lake operations"],"system_design":["Databricks Lakehouse architecture","ETL/ELT pipeline design","Streaming and batch data integration"],"domain_knowledge":["Data warehousing concepts","Cloud-native data engineering","Governance and lineage"],"problem_solving":["Performance tuning","Pipeline failure debugging","Cost optimization"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":"Mumbai, Maharashtra, India","remote_policy":"onsite / hybrid","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["Recruiter screening","Technical interview","Client discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Client Technical Panel"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Databricks Data Engineer role in Mumbai requiring 3+ years experience. Skills include Databricks, Spark, PySpark, Delta Lake, ETL/ELT, Kafka, cloud platforms, and Lakehouse architecture.","semantic_tags":["databricks","data-engineering","pyspark","spark","delta-lake","etl","kafka","lakehouse","mid-level","mumbai"]}}
{"id":"job_greymatter_lead_aws_databricks_011","role":{"title":"Lead AWS Cloud Engineer – Databricks Platform","level":"senior","department":"Cloud Engineering","team":"Databricks & Data Platforms","reports_to":"Cloud Architecture Head"},"company":{"name":"Greymatter Innovationz","size":null,"industry":"IT Services & Cloud Solutions","culture_keywords":["leadership","innovation","mentorship","engineering-excellence"]},"description":{"summary":"Lead the design, deployment, and optimization of AWS cloud infrastructure and Databricks-based data platforms supporting analytics and machine learning workloads.","key_value_proposition":"Own end-to-end AWS and Databricks platform delivery while leading teams and driving best practices for scalable, secure, and cost-efficient data solutions.","full_text":"Greymatter Innovationz is seeking a Lead AWS Cloud Engineer with deep Databricks expertise to design, manage, and optimize AWS cloud infrastructure and Databricks platforms while mentoring engineers and delivering enterprise-grade data solutions."},"responsibilities":{"primary":["Design and optimize scalable AWS cloud architectures","Build and manage Databricks clusters, notebooks, and Spark workloads","Integrate Databricks with AWS services such as S3, Redshift, and Glue","Lead infrastructure automation using Terraform or CloudFormation","Monitor and optimize performance, security, and cost across AWS and Databricks"],"secondary":["Lead and mentor cloud engineers","Collaborate with architecture, data science, and business teams","Troubleshoot production issues and performance bottlenecks","Document cloud infrastructure and Databricks configurations"],"time_allocation":{"coding":45,"mentoring":25,"meetings":30}},"requirements":{"must_have":{"experience_years":5,"technical_skills":["AWS","Databricks","Apache Spark","Python","Terraform","CloudFormation","Docker","Kubernetes"],"domain_expertise":["Cloud Engineering","Databricks Platform","Data Engineering","Machine Learning Workloads"],"soft_skills":["leadership","communication","mentorship","problem-solving"]},"nice_to_have":{"education":["Bachelor’s degree in Computer Science or Engineering"],"technical_skills":["AWS Certified Solutions Architect","Databricks Certification"],"domain_expertise":["Agile delivery","Cost optimization"]},"deal_breakers":["No Databricks production experience","Lack of AWS architecture exposure"]},"technical_assessment_focus":{"coding":["Spark and Databricks development","Infrastructure automation scripts"],"system_design":["AWS cloud architecture","Databricks platform design","Secure and scalable data platforms"],"domain_knowledge":["Cloud security best practices","Lakehouse concepts"],"problem_solving":["Performance tuning","Cost optimization","Production issue resolution"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"USD","target":null},"equity":{"offered":null,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":"Princeton, New Jersey, USA","remote_policy":"hybrid","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Leadership interview"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Cloud Architect"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"contract / full-time","requisition_id":null,"hiring_urgency":"high","last_updated":null},"embedding_optimization":{"primary_text":"Lead AWS Cloud Engineer – Databricks Platform role at Greymatter Innovationz. 5+ years AWS, 3+ years Databricks, leadership experience, Spark, Terraform, Docker, Kubernetes.","semantic_tags":["aws","databricks","cloud-engineering","spark","terraform","lead-role","lakehouse"]}}
{"id":"job_azure_databricks_engineer_012","role":{"title":"Databricks Engineer","level":"senior","department":"Data Engineering","team":"Azure Lakehouse","reports_to":"Data Engineering Manager"},"company":{"name":null,"size":null,"industry":"Technology / Data Platforms","culture_keywords":["collaboration","innovation","delivery-focused","learning"]},"description":{"summary":"Design and implement scalable Azure Databricks data pipelines and Lakehouse solutions using Spark, PySpark, and SQL.","key_value_proposition":"Work on enterprise-grade Azure Lakehouse architectures supporting analytics, BI, and data science workloads.","full_text":"This role involves building scalable data pipelines using Azure Databricks, Spark-SQL, PySpark, and Azure Data Lake Storage while collaborating with DevOps, BI, and Data Science teams."},"responsibilities":{"primary":["Develop scalable data pipelines using Spark-SQL and PySpark in Azure Databricks","Build and maintain Lakehouse architecture using ADLS and Databricks","Perform data cleaning, normalization, deduplication, and transformations","Collaborate with DevOps for production deployments","Support data migrations and system upgrades"],"secondary":["Partner with BI and Data Science teams","Document data processes and pipelines","Support training and change management initiatives"],"time_allocation":{"coding":60,"mentoring":10,"meetings":30}},"requirements":{"must_have":{"experience_years":6,"technical_skills":["Azure Databricks","PySpark","Spark-SQL","Python","SQL","Azure Data Factory","Azure Data Lake Storage"],"domain_expertise":["Data Engineering","Lakehouse Architecture","Distributed Computing"],"soft_skills":["communication","collaboration","problem-solving"]},"nice_to_have":{"education":[],"technical_skills":["Delta Lake","Git","CI/CD","Pandas"],"domain_expertise":["On-prem to cloud migration","BI enablement"]},"deal_breakers":["No Azure Databricks experience","Lack of Spark-SQL or PySpark skills"]},"technical_assessment_focus":{"coding":["PySpark development","Spark-SQL transformations"],"system_design":["Azure Lakehouse architecture","ETL pipeline design"],"domain_knowledge":["Distributed data processing","Data modeling"],"problem_solving":["Pipeline optimization","Production support issues"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":"New Delhi / Bangalore / Hyderabad, India","remote_policy":"onsite","required_onsite_days":5,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical round 1","Technical round 2","Final HR"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Technical Panel"]},"metadata":{"posted_date":"2025-06-30","application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Senior Databricks Engineer role on Azure requiring 6–8 years experience. Skills include Azure Databricks, Spark-SQL, PySpark, ADLS, Delta Lake, CI/CD.","semantic_tags":["azure-databricks","spark","pyspark","lakehouse","data-engineering","senior-level","india"]}}
{"id":"job_azure_databricks_data_engineer_013","role":{"title":"Azure Databricks Data Engineer","level":"mid","department":"Data & Analytics","team":"Azure Lakehouse Engineering","reports_to":"Data Engineering Manager"},"company":{"name":null,"size":null,"industry":"Technology / Data Engineering","culture_keywords":["scalability","governance","collaboration","engineering-excellence"]},"description":{"summary":"Design and build scalable Azure Databricks ETL pipelines using PySpark and Delta Lake to power enterprise Lakehouse architectures and BI reporting.","key_value_proposition":"Own end-to-end Azure Lakehouse implementations including Bronze–Silver–Gold modeling, performance optimization, governance, and CI/CD automation.","full_text":"This role involves building ETL pipelines in Azure Databricks, implementing Lakehouse architecture on ADLS Gen2, integrating with Azure services, optimizing Spark performance, and enabling reporting via Synapse and Power BI."},"responsibilities":{"primary":["Design and build ETL pipelines using Azure Databricks (PySpark, Delta Lake)","Implement Bronze–Silver–Gold Lakehouse architecture on ADLS Gen2","Develop dimensional and star schema data models for Synapse and Power BI","Integrate Databricks with ADF, Key Vault, Event Hub, Synapse, Purview, and Logic Apps","Build and manage CI/CD pipelines using Azure DevOps","Optimize Spark performance through tuning, caching, Z-ordering, and Delta optimization","Ensure data security, compliance, and auditability"],"secondary":["Collaborate with data architects and analysts","Handle schema drift and debugging issues","Support governance and lineage implementation","Document data models and pipelines"],"time_allocation":{"coding":65,"mentoring":10,"meetings":25}},"requirements":{"must_have":{"experience_years":3,"technical_skills":["Azure Databricks","PySpark","Python","SQL","Delta Lake","Azure Data Factory","Azure Data Lake Gen2","Azure DevOps"],"domain_expertise":["Data Engineering","Lakehouse Architecture","ETL/ELT","Data Warehousing"],"soft_skills":["problem-solving","collaboration","communication","analytical thinking"]},"nice_to_have":{"education":[],"technical_skills":["Unity Catalog","Databricks ML Runtime","Power BI","Synapse Analytics"],"domain_expertise":["Healthcare data","Financial data","Data governance"]},"deal_breakers":["No Azure Databricks experience","Lack of PySpark or Delta Lake knowledge"]},"technical_assessment_focus":{"coding":["PySpark ETL development","SQL transformations","Delta Lake optimizations"],"system_design":["Azure Lakehouse architecture","CI/CD pipeline design","Secure data platform design"],"domain_knowledge":["Star schema modeling","Governance and lineage","Azure ecosystem integration"],"problem_solving":["Performance tuning","Schema drift handling","Security compliance issues"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":"Bhopal, Madhya Pradesh, India","remote_policy":"onsite","required_onsite_days":5,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Managerial discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Technical Lead"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Azure Databricks Data Engineer role in Bhopal requiring 3–5 years experience. Skills include PySpark, Delta Lake, Azure Data Factory, ADLS Gen2, Azure DevOps, Power BI, Synapse.","semantic_tags":["azure-databricks","lakehouse","pyspark","delta-lake","etl","power-bi","mid-level","bhopal"]}}
{"id":"job_data_support_engineer_014","role":{"title":"Senior Data Support Engineer","level":"senior","department":"Data Operations","team":"Data Platform Support","reports_to":"Data Operations Manager"},"company":{"name":null,"size":null,"industry":"Technology / Data Platforms","culture_keywords":["stability","reliability","customer-focus","operational-excellence"]},"description":{"summary":"Provide L3/L4 production support for enterprise data platforms, ensuring pipeline stability, data quality, and rapid issue resolution across Azure and Databricks environments.","key_value_proposition":"Own critical data platform stability through proactive monitoring, root cause analysis, and close collaboration with engineering and business teams.","full_text":"This role serves as the escalation point for data ingestion, pipeline, and consumption issues, focusing on monitoring, root cause analysis, data quality checks, and operational excellence across Azure and Databricks platforms."},"responsibilities":{"primary":["Act as escalation point for data ingestion and pipeline issues","Proactively monitor data pipeline health and system stability","Perform root cause analysis on recurring data issues","Support data quality checks and corrective actions","Troubleshoot complex issues using SQL, Python, and PySpark","Work with Azure services including Databricks, ADF, ADLS, Azure Workbooks, and KQL"],"secondary":["Collaborate with engineering and business teams","Maintain incident documentation and long-term fixes","Use ticketing tools such as ServiceNow or JIRA","Support Grafana dashboards and monitoring solutions"],"time_allocation":{"coding":35,"mentoring":15,"meetings":50}},"requirements":{"must_have":{"experience_years":8,"technical_skills":["SQL","Python","ETL/ELT","Azure Databricks","PySpark","Azure Data Factory","ADLS","KQL","Grafana"],"domain_expertise":["Data Operations","Production Support","Cloud Data Platforms"],"soft_skills":["problem-solving","communication","prioritization","stakeholder-management"]},"nice_to_have":{"education":[],"technical_skills":["Azure Monitor","Azure Workbooks"],"domain_expertise":["SRE practices","Incident management"]},"deal_breakers":["No production data support experience","Lack of SQL troubleshooting skills"]},"technical_assessment_focus":{"coding":["SQL debugging","Python troubleshooting","Pipeline diagnostics"],"system_design":["Monitoring and alerting design","Operational data platform architecture"],"domain_knowledge":["Data pipeline concepts","Cloud monitoring tools"],"problem_solving":["Root cause analysis","Incident resolution","Data quality remediation"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":"Remote (WFH)","remote_policy":"remote","required_onsite_days":0,"relocation_assistance":false,"visa_sponsorship":false},"interview_process":{"stages":["HR screening","Technical support interview","Final discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Operations Lead"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"high","last_updated":null},"embedding_optimization":{"primary_text":"Senior Data Support Engineer role (WFH) requiring 8+ years experience. Skills include SQL, Python, Azure Databricks, PySpark, ADF, ADLS, monitoring, and incident management.","semantic_tags":["data-support","production-support","azure","databricks","sql","pyspark","senior-level","remote"]}}
{"id":"job_valuemomentum_databricks_engineer_015","role":{"title":"Databricks Engineer","level":"mid","department":"Data Engineering","team":"Azure Data Platforms","reports_to":"Data Engineering Manager"},"company":{"name":"ValueMomentum","size":null,"industry":"Insurance & Financial Technology Services","culture_keywords":["client-focused","delivery-oriented","collaboration"]},"description":{"summary":"Design and develop Azure Databricks-based ETL pipelines using PySpark, SQL, and Azure Data Factory to support enterprise data solutions.","key_value_proposition":"Work on Azure Databricks solutions within insurance and financial services, delivering scalable and analytics-ready data platforms.","full_text":"This role focuses on building ETL pipelines using Azure Databricks, ADF, and ADLS, translating business requirements into technical designs, and collaborating with stakeholders to deliver robust data engineering solutions."},"responsibilities":{"primary":["Design and develop ETL pipelines using Azure Data Factory","Build data solutions using Azure Databricks, ADLS, and Synapse","Write SQL, Python, and PySpark code for data transformation","Translate business requirements into technical designs","Develop mapping documents and transformation rules"],"secondary":["Collaborate with stakeholders and project teams","Communicate project status and risks","Support DevOps and CI/CD deployments"],"time_allocation":{"coding":60,"mentoring":10,"meetings":30}},"requirements":{"must_have":{"experience_years":4,"technical_skills":["Azure Databricks","PySpark","SQL","Python","Azure Data Factory","ADLS","Azure Synapse"],"domain_expertise":["Data Engineering","ETL/ELT","Azure Cloud Platforms"],"soft_skills":["communication","problem-solving","stakeholder-management"]},"nice_to_have":{"education":[],"technical_skills":["Advanced SQL","CI/CD"],"domain_expertise":["Insurance","Financial services"]},"deal_breakers":["No Azure Databricks experience","Lack of SQL or PySpark skills"]},"technical_assessment_focus":{"coding":["PySpark transformations","SQL optimization"],"system_design":["Azure-based data pipeline design"],"domain_knowledge":["Data ingestion and processing"],"problem_solving":["ETL debugging","Performance tuning"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":"Hyderabad, Telangana, India","remote_policy":"onsite","required_onsite_days":5,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Client discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Technical Panel"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Databricks Engineer role at ValueMomentum Hyderabad requiring 4–10 years experience with Azure Databricks, PySpark, SQL, ADF, and ADLS.","semantic_tags":["azure-databricks","pyspark","etl","data-engineering","hyderabad","mid-level"]}}
{"id":"job_gmg_aws_databricks_data_engineer_016","role":{"title":"Senior Data Engineer","level":"senior","department":"Data Engineering","team":"Cloud Data Platforms","reports_to":"Data Engineering Lead"},"company":{"name":"GMG","size":"10000+","industry":"Retail, Food & Health","culture_keywords":["global","innovation","well-being","collaboration"]},"description":{"summary":"Design, build, and optimize scalable AWS and Databricks-based data pipelines supporting analytics and real-time data processing.","key_value_proposition":"Work on global-scale AWS and Databricks platforms enabling real-time and batch analytics across multiple business verticals.","full_text":"GMG is seeking a Senior Data Engineer with strong AWS and Databricks expertise to develop ETL pipelines, manage cloud infrastructure, optimize performance, and enforce governance and security best practices."},"responsibilities":{"primary":["Develop ETL pipelines using AWS Glue, PySpark, and SQL","Handle real-time and batch data ingestion","Manage and optimize Databricks clusters and workloads","Integrate Databricks with AWS services such as S3, Athena, and Lambda","Implement CI/CD pipelines and testing frameworks"],"secondary":["Optimize performance and cost across AWS and Databricks","Ensure security, governance, and compliance","Collaborate with cross-functional teams"],"time_allocation":{"coding":55,"mentoring":15,"meetings":30}},"requirements":{"must_have":{"experience_years":6,"technical_skills":["AWS Glue","PySpark","SQL","Databricks","Athena","Lambda","S3"],"domain_expertise":["Data Engineering","Cloud Platforms","Real-time Data Processing"],"soft_skills":["problem-solving","communication","collaboration"]},"nice_to_have":{"education":[],"technical_skills":["SNS","CI/CD","Unit Testing"],"domain_expertise":["Retail analytics","Cost optimization"]},"deal_breakers":["No AWS data engineering experience","No Databricks exposure"]},"technical_assessment_focus":{"coding":["PySpark and SQL development"],"system_design":["AWS-based data architectures"],"domain_knowledge":["Streaming and batch processing"],"problem_solving":["Performance tuning","Production issue resolution"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"AED","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":"Middle East / Asia (GMG Regions)","remote_policy":"hybrid","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Managerial discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Engineering Leadership"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Senior Data Engineer role at GMG requiring AWS, Databricks, PySpark, SQL, Glue, Athena, and CI/CD experience.","semantic_tags":["aws","databricks","data-engineering","pyspark","senior-level","real-time-processing"]}}
{"id":"job_rearc_lead_data_engineer_017","role":{"title":"Lead Data Engineer","level":"lead","department":"Data Engineering","team":"Platform & Architecture","reports_to":"Head of Engineering"},"company":{"name":"Rearc","size":null,"industry":"Cloud & Data Engineering","culture_keywords":["ownership","innovation","freedom","engineering-led"]},"description":{"summary":"Lead large-scale data engineering initiatives, designing and optimizing cloud-native data pipelines and architectures using Databricks, Spark, and modern orchestration tools.","key_value_proposition":"Drive technical excellence and mentor teams while building scalable, modern data platforms across cloud ecosystems.","full_text":"Rearc is seeking a Lead Data Engineer with deep expertise in data architecture, ETL, Databricks, Spark, and orchestration tools to lead data initiatives and mentor engineers."},"responsibilities":{"primary":["Lead and deliver complex data engineering projects","Design scalable data architectures and pipelines","Implement DataOps practices using Airflow, Databricks, and DBT","Mentor data engineers and provide technical leadership","Collaborate with stakeholders to translate requirements into solutions"],"secondary":["Promote best practices and knowledge sharing","Write technical blogs and documentation","Drive innovation in data tooling and architecture"],"time_allocation":{"coding":40,"mentoring":30,"meetings":30}},"requirements":{"must_have":{"experience_years":10,"technical_skills":["Databricks","Apache Spark","Python","Java","Airflow","DBT","Delta Lake"],"domain_expertise":["Data Architecture","ETL","Data Warehousing","Cloud Platforms"],"soft_skills":["leadership","strategic thinking","communication","mentorship"]},"nice_to_have":{"education":[],"technical_skills":["AWS Redshift","Azure Synapse","Google BigQuery"],"domain_expertise":["DataOps","Enterprise-scale platforms"]},"deal_breakers":["No leadership experience","Lack of large-scale data architecture exposure"]},"technical_assessment_focus":{"coding":["Python/Java data pipelines"],"system_design":["Enterprise data architectures"],"domain_knowledge":["Modern data stacks","Cloud-native services"],"problem_solving":["Scalability challenges","Complex data workflows"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"USD","target":null},"equity":{"offered":null,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":"Remote / Global","remote_policy":"remote","required_onsite_days":0,"relocation_assistance":false,"visa_sponsorship":false},"interview_process":{"stages":["HR screening","Technical leadership interview","Final discussion"],"total_duration_weeks":null,"decision_makers":["Engineering Leadership"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Lead Data Engineer role at Rearc requiring 10+ years experience with Databricks, Spark, Airflow, DBT, cloud data architectures, and team leadership.","semantic_tags":["lead-data-engineer","databricks","spark","airflow","dbt","cloud-data","leadership"]}}
{"id":"job_gcp_databricks_data_engineer_018","role":{"title":"Data Engineer","level":"senior","department":"Data Engineering","team":"Cloud Data Platforms","reports_to":"Engineering Manager"},"company":{"name":"Confidential (GCP-focused organization)","size":null,"industry":"Cloud & Data Infrastructure","culture_keywords":["cloud-native","automation","scalability"]},"description":{"summary":"Design, develop, and maintain scalable data pipelines using Databricks, PySpark, and GCP services with strong governance and automation practices.","key_value_proposition":"Work on Databricks with GCP infrastructure using Terraform and CI/CD to deliver governed, scalable data platforms.","full_text":"This role focuses on building scalable ETL pipelines using Databricks and PySpark, implementing Unity Catalog for governance, automating GCP infrastructure using Terraform, and maintaining CI/CD pipelines for data engineering workflows."},"responsibilities":{"primary":["Design and optimize ETL pipelines using Databricks and PySpark","Implement Unity Catalog for governance, lineage, and access control","Develop infrastructure as code using Terraform on GCP","Build and maintain CI/CD pipelines","Ensure data quality, security, and availability"],"secondary":["Collaborate with data scientists and DevOps teams","Troubleshoot and optimize performance"],"time_allocation":{"coding":60,"mentoring":10,"meetings":30}},"requirements":{"must_have":{"experience_years":5,"technical_skills":["Databricks","PySpark","SQL","Terraform","Unity Catalog","GCP BigQuery","Cloud Build","Firestore"],"domain_expertise":["Data Engineering","Cloud Infrastructure","Governance"],"soft_skills":["problem-solving","collaboration","communication"]},"nice_to_have":{"education":[],"technical_skills":["Git","Agile"],"domain_expertise":["DataOps"]},"deal_breakers":["No Databricks experience","No GCP exposure"]},"technical_assessment_focus":{"coding":["PySpark ETL","SQL optimization"],"system_design":["GCP-based data platforms"],"domain_knowledge":["Governance and lineage"],"problem_solving":["Pipeline performance tuning"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"USD","target":null},"equity":{"offered":null,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":"India / GCP Hub","remote_policy":"hybrid","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Managerial round"],"total_duration_weeks":null,"decision_makers":["Engineering Manager"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Senior Data Engineer role using Databricks, PySpark, Unity Catalog, Terraform, and GCP services such as BigQuery and Cloud Build.","semantic_tags":["databricks","gcp","terraform","unity-catalog","data-engineering","senior"]}}
{"id":"job_aws_databricks_data_engineer_019","role":{"title":"AWS Data Engineer","level":"mid-senior","department":"Data Engineering","team":"Cloud Modernization","reports_to":"Data Engineering Lead"},"company":{"name":"Confidential Global IT Services Firm","size":"50000+","industry":"IT Services & Consulting","culture_keywords":["agile","cloud-first","enterprise-scale"]},"description":{"summary":"Design and maintain scalable AWS Databricks data pipelines using PySpark and SQL for enterprise analytics workloads.","key_value_proposition":"Work on enterprise-grade AWS Databricks platforms with strict performance, governance, and scalability requirements.","full_text":"This role involves building and optimizing ETL pipelines using AWS Databricks and PySpark, collaborating with architects and stakeholders, ensuring data quality and governance, and mentoring junior engineers."},"responsibilities":{"primary":["Develop ETL pipelines using AWS Databricks and PySpark","Optimize data workflows for scalability and performance","Collaborate with business stakeholders and architects","Ensure data quality, security, and governance","Provide technical guidance to junior engineers"],"secondary":["Monitor and troubleshoot pipeline issues","Participate in Agile ceremonies"],"time_allocation":{"coding":55,"mentoring":15,"meetings":30}},"requirements":{"must_have":{"experience_years":5,"technical_skills":["AWS Databricks","PySpark","SQL","Git","CI/CD"],"domain_expertise":["Data Engineering","Distributed Systems","Cloud Architecture"],"soft_skills":["communication","mentorship","problem-solving"]},"nice_to_have":{"education":[],"technical_skills":["DevOps","Automation"],"domain_expertise":["Large-scale data platforms"]},"deal_breakers":["Less than 5 years experience","No AWS Databricks exposure","Weak SQL skills"]},"technical_assessment_focus":{"coding":["PySpark transformations","Complex SQL queries"],"system_design":["AWS cloud data pipelines"],"domain_knowledge":["ETL and distributed processing"],"problem_solving":["Production issue debugging"]},"compensation":{"salary_range":{"min":2100000,"max":2600000,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":true,"target_percent":null}},"logistics":{"location":"PAN India","remote_policy":"hybrid","required_onsite_days":2,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Final managerial round"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Tech Lead"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"high","last_updated":null},"embedding_optimization":{"primary_text":"AWS Data Engineer role requiring AWS Databricks, PySpark, SQL, and strong ETL experience across PAN India locations.","semantic_tags":["aws-databricks","pyspark","sql","data-engineering","pan-india","mid-senior"]}}
{"id":"job_accenture_databricks_consultant_020","role":{"title":"Databricks Data Engineer Consultant","level":"mid","department":"Data & AI Consulting","team":"Client Delivery","reports_to":"Consulting Manager"},"company":{"name":"Accenture","size":"775000+","industry":"IT Services & Consulting","culture_keywords":["inclusion","innovation","client-centric","consulting"]},"description":{"summary":"Deliver scalable Databricks-based data pipelines for cloud and hybrid client environments with a strong consulting focus.","key_value_proposition":"Work on high-impact consulting projects using Databricks, DLT, Unity Catalog, and cloud-native data services across ANZ.","full_text":"This consulting role involves designing and implementing ETL pipelines using Databricks, handling batch and streaming data, applying DataOps practices, and collaborating closely with clients and architects."},"responsibilities":{"primary":["Design and implement ETL pipelines using Databricks","Ingest and process batch and streaming data","Apply data quality and observability practices","Collaborate with business users and architects","Monitor and tune pipeline performance"],"secondary":["Support CI/CD and DataOps workflows","Participate in troubleshooting and production support"],"time_allocation":{"coding":55,"mentoring":5,"meetings":40}},"requirements":{"must_have":{"experience_years":2,"technical_skills":["Databricks","PySpark","SQL","Python","Unity Catalog","DLT"],"domain_expertise":["Consulting","Cloud Data Engineering"],"soft_skills":["client-communication","problem-solving","adaptability"]},"nice_to_have":{"education":[],"technical_skills":["Glue","ADF","Qlik","CI/CD"],"domain_expertise":["Hybrid data platforms"]},"deal_breakers":["No Databricks experience","Poor communication skills"]},"technical_assessment_focus":{"coding":["Databricks ETL pipelines"],"system_design":["Client data architectures"],"domain_knowledge":["Batch and streaming ingestion"],"problem_solving":["Production data issues"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"AUD","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":true,"target_percent":null}},"logistics":{"location":"Australia (Sydney, Melbourne, Brisbane, Canberra)","remote_policy":"hybrid","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Client-facing assessment"],"total_duration_weeks":null,"decision_makers":["Consulting Manager","Tech Lead"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Databricks Data Engineer Consultant role at Accenture ANZ using PySpark, SQL, DLT, Unity Catalog, and cloud-native services.","semantic_tags":["databricks","consulting","anz","pyspark","unity-catalog","dataops"]}}
{"id":"job_wurthit_software_engineer_databricks_021","role":{"title":"Software Engineer – Databricks Platform","level":"senior","department":"Software Engineering","team":"Global Databricks Engineering","reports_to":"Engineering Manager"},"company":{"name":"Würth IT India","size":null,"industry":"Enterprise IT & Digital Solutions","culture_keywords":["global-collaboration","engineering-excellence","autonomy"]},"description":{"summary":"Develop and maintain enterprise-grade software applications on Databricks using Java, Python, SQL, and cloud platforms.","key_value_proposition":"Work as a core software engineer using Databricks as an application and ML platform, not just a data pipeline tool.","full_text":"This role focuses on software development on Databricks, implementing business logic in Java, MLflow-based applications, Databricks workflows, CI/CD automation, and orchestration using Airflow across multi-cloud environments."},"responsibilities":{"primary":["Develop and maintain applications using Java, Python, SQL, and Linux on Databricks","Design and implement data integration workflows with business logic in Java","Implement Databricks workflows and MLflow-based applications","Build and manage CI/CD pipelines using GitHub Enterprise Actions","Implement and manage data pipelines using Airflow"],"secondary":["Optimize application performance","Write unit tests and perform debugging","Collaborate with global cross-functional teams"],"time_allocation":{"coding":65,"mentoring":10,"meetings":25}},"requirements":{"must_have":{"experience_years":7,"technical_skills":["Databricks","Java","Python","SQL","MLflow","Airflow","GitHub Actions"],"domain_expertise":["Software Engineering","Databricks Application Development","CI/CD"],"soft_skills":["autonomy","problem-solving","communication"]},"nice_to_have":{"education":[],"technical_skills":["C++","C#","Asset Bundles"],"domain_expertise":["Multi-cloud platforms"]},"deal_breakers":["No Java experience","No Databricks platform exposure"]},"technical_assessment_focus":{"coding":["Java business logic","Python unit testing"],"system_design":["Databricks-based applications"],"domain_knowledge":["MLflow workflows","CI/CD automation"],"problem_solving":["Performance optimization"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":true,"target_percent":null}},"logistics":{"location":"Pune, Maharashtra, India","remote_policy":"onsite","required_onsite_days":5,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Managerial round"],"total_duration_weeks":null,"decision_makers":["Engineering Manager","Global Tech Lead"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Senior Software Engineer role at Würth IT India using Databricks as an application platform with Java, Python, MLflow, Airflow, and CI/CD.","semantic_tags":["software-engineer","databricks","java","mlflow","airflow","ci-cd","pune"]}}
{"id":"job_aws_databricks_data_engineer_022","role":{"title":"Databricks Data Engineer","level":"mid-senior","department":"Data Engineering","team":"Cloud Data Platforms","reports_to":"Data Engineering Manager"},"company":{"name":"Confidential AWS-Focused Organization","size":null,"industry":"Cloud & Data Engineering","culture_keywords":["cloud-first","data-driven","continuous-learning"]},"description":{"summary":"Build and optimize scalable data pipelines using AWS services and Databricks for large-scale data processing and migration workloads.","key_value_proposition":"Work on AWS-centric Databricks environments handling ETL, data migration, and distributed processing at scale.","full_text":"This role involves designing ETL pipelines using Databricks notebooks, leveraging AWS services such as S3, Glue, Redshift, and Lambda, implementing Unity Catalog, and processing batch and real-time datasets using PySpark."},"responsibilities":{"primary":["Develop ETL pipelines using Databricks notebooks","Build and optimize data pipelines using PySpark","Leverage AWS services such as S3, Glue, Redshift, and Lambda","Implement and maintain Unity Catalog","Write complex SQL queries for data analysis"],"secondary":["Support batch and real-time processing","Share knowledge and contribute to continuous improvement"],"time_allocation":{"coding":60,"mentoring":10,"meetings":30}},"requirements":{"must_have":{"experience_years":4,"technical_skills":["Databricks","AWS","PySpark","Python","SQL","Unity Catalog"],"domain_expertise":["Data Engineering","Big Data Processing","Cloud Migration"],"soft_skills":["analytical-thinking","collaboration","communication"]},"nice_to_have":{"education":[],"technical_skills":["Redshift Optimization","AWS Certifications"],"domain_expertise":["Streaming pipelines"]},"deal_breakers":["No AWS experience","Weak SQL skills"]},"technical_assessment_focus":{"coding":["PySpark ETL pipelines","SQL optimization"],"system_design":["AWS Databricks architectures"],"domain_knowledge":["Batch and real-time processing"],"problem_solving":["Pipeline performance tuning"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":true,"target_percent":null}},"logistics":{"location":"India","remote_policy":"hybrid","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Final discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Tech Lead"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Databricks Data Engineer role using AWS services, PySpark, SQL, Unity Catalog, and ETL pipelines.","semantic_tags":["aws","databricks","pyspark","sql","unity-catalog","data-engineering","mid-senior"]}}
{"id":"job_azure_databricks_banking_engineer_023","role":{"title":"Azure Databricks Engineer","level":"senior","department":"Data Engineering","team":"Enterprise Data Platforms","reports_to":"Data Engineering Lead"},"company":{"name":"Confidential (Banking / Financial Services)","size":null,"industry":"Banking & Financial Services","culture_keywords":["data-governance","regulatory-compliance","scalability","security-first"]},"description":{"summary":"Design and build scalable batch and real-time data pipelines on Azure Databricks supporting banking, risk, and regulatory reporting use cases.","key_value_proposition":"Work on mission-critical banking data platforms leveraging CDC, Delta Live Tables, and streaming architectures on Azure Databricks.","full_text":"This role focuses on developing scalable data pipelines using Azure Databricks, PySpark, and Delta Lake, implementing CDC and Autoloader for near-real-time ingestion, integrating Kafka streams, and enforcing governance and security for financial data workloads."},"responsibilities":{"primary":["Design and optimize batch and streaming data pipelines using Azure Databricks","Implement CDC, Autoloader, and Delta Live Tables","Integrate data sources using ADF, ADLS Gen2, and Confluent Kafka","Develop CI/CD pipelines using GitHub Actions or Azure DevOps","Ensure data quality, lineage, governance, and security"],"secondary":["Collaborate with analysts and data scientists on risk and compliance use cases","Participate in code reviews and architectural discussions","Document data pipelines and governance processes"],"time_allocation":{"coding":55,"mentoring":10,"meetings":35}},"requirements":{"must_have":{"experience_years":6,"technical_skills":["Azure Databricks","PySpark","SQL","Python","ADF","ADLS Gen2","Confluent Kafka","CDC","Delta Live Tables"],"domain_expertise":["Banking Data","Risk & Compliance Reporting","Streaming Architectures"],"soft_skills":["stakeholder-collaboration","problem-solving","communication"]},"nice_to_have":{"education":[],"technical_skills":["Unity Catalog","Lakehouse Architecture"],"domain_expertise":["Regulatory reporting","Market and credit risk"]},"deal_breakers":["No Azure Databricks experience","No PySpark or SQL proficiency"]},"technical_assessment_focus":{"coding":["PySpark transformations","SQL optimization"],"system_design":["Streaming and CDC-based lakehouse architectures"],"domain_knowledge":["Financial data models","Governance and compliance"],"problem_solving":["Pipeline reliability and performance tuning"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":true,"target_percent":null}},"logistics":{"location":"India","remote_policy":"hybrid","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Architectural discussion"],"total_duration_weeks":null,"decision_makers":["Data Engineering Lead","Architecture Panel"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"high","last_updated":null},"embedding_optimization":{"primary_text":"Senior Azure Databricks Engineer role in banking using CDC, Delta Live Tables, Autoloader, Kafka, and governed lakehouse architecture.","semantic_tags":["azure-databricks","banking-data","cdc","delta-live-tables","kafka","streaming","governance","senior"]}}
{"id":"job_senior_databricks_tech_lead_024","role":{"title":"Senior Databricks Engineer / Tech Lead","level":"lead","department":"Data Engineering","team":"Innovation & Advanced Analytics","reports_to":"Head of Data Engineering"},"company":{"name":"Confidential (Innovation Team)","size":null,"industry":"Advanced Analytics & AI Platforms","culture_keywords":["innovation-driven","platform-ownership","engineering-excellence","governance-first"]},"description":{"summary":"Lead the design, implementation, and operationalization of scalable Databricks Lakehouse platforms to support advanced analytics and AI workloads.","key_value_proposition":"Own and evolve enterprise Databricks platforms using Asset Bundles, Unity Catalog, Lakehouse architecture, and AI-ready data foundations.","full_text":"This role focuses on architecting and leading end-to-end Databricks solutions, implementing governed Lakehouse architectures, CI/CD automation using Databricks Asset Bundles, enforcing Unity Catalog, and enabling advanced analytics and AI use cases."},"responsibilities":{"primary":["Architect and productionize end-to-end Databricks solutions","Implement and optimize ETL/ELT pipelines for structured and semi-structured data","Drive Lakehouse architecture adoption using Delta Lake","Implement CI/CD using Databricks Repos and Asset Bundles","Configure and enforce Unity Catalog for governance and security","Lead Spark performance tuning and optimization","Integrate streaming and batch pipelines using Kafka, Event Hub, and APIs"],"secondary":["Design data quality and validation frameworks","Collaborate with data scientists on AI/ML-ready datasets","Mentor junior engineers and define best practices","Troubleshoot complex production issues"],"time_allocation":{"coding":45,"mentoring":25,"meetings":30}},"requirements":{"must_have":{"experience_years":7,"technical_skills":["Databricks","Apache Spark","PySpark","SQL","Delta Lake","Unity Catalog","Databricks Asset Bundles","Structured Streaming","CI/CD"],"domain_expertise":["Lakehouse Architecture","Data Platform Engineering","AI/ML Data Enablement","Data Governance"],"soft_skills":["technical-leadership","stakeholder-collaboration","problem-solving"]},"nice_to_have":{"education":[],"technical_skills":["Mosaic AI","Databricks ML","Kafka","Event Hub","Data Observability Tools"],"domain_expertise":["AI Feature Engineering","Real-time Analytics"]},"deal_breakers":["No hands-on Databricks platform experience","No Spark performance tuning experience"]},"technical_assessment_focus":{"coding":["PySpark optimization","Delta Lake implementations"],"system_design":["Enterprise Databricks Lakehouse architecture"],"domain_knowledge":["Governed data platforms","AI/ML data foundations"],"problem_solving":["Performance bottlenecks","Production reliability issues"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":true,"target_percent":null}},"logistics":{"location":"India","remote_policy":"hybrid","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical deep-dive","Architecture & leadership round"],"total_duration_weeks":null,"decision_makers":["Head of Data Engineering","Platform Architects"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"high","last_updated":null},"embedding_optimization":{"primary_text":"Certified Senior Databricks Engineer / Tech Lead role focusing on Lakehouse architecture, Asset Bundles, Unity Catalog, Spark optimization, and AI/ML enablement.","semantic_tags":["databricks-tech-lead","asset-bundles","unity-catalog","lakehouse","spark-optimization","ai-ml-data","streaming","governance"]}}
{"id":"job_azure_dwh_engineer_trivandrum_025","role":{"title":"Azure Data Engineer / Data Warehouse Engineer","level":"mid-senior","department":"Data Engineering","team":"Enterprise Data & Analytics","reports_to":"Data Engineering Manager"},"company":{"name":"Confidential","size":null,"industry":"IT Services & Data Analytics","culture_keywords":["data-driven","hybrid-work","engineering-excellence","architecture-focused"]},"description":{"summary":"Design and develop enterprise-grade data warehouse solutions using Azure Synapse Analytics, Databricks, ADLS, ADF, and Power BI for large-scale analytics workloads.","key_value_proposition":"Work on end-to-end Azure data warehouse design with strong exposure to Synapse, SSIS, Power BI, DevOps automation, and Infrastructure as Code.","full_text":"This role involves building and optimizing Azure-based data warehouse solutions using Synapse Analytics, ADLS, ADF, Databricks, Power BI, and Azure Analysis Services. Responsibilities include ETL development, SQL optimization, SSIS workflows, CI/CD automation with Terraform and Jenkins, and collaboration in onshore-offshore delivery models."},"responsibilities":{"primary":["Design and develop data warehouse solutions using Azure Synapse Analytics","Build and optimize ETL pipelines using ADF, Databricks, and SSIS","Develop and optimize complex SQL queries for analytics workloads","Implement CI/CD automation using DevOps frameworks","Manage data ingestion, transformation, validation, and performance tuning"],"secondary":["Collaborate on architecture frameworks and best practices","Work with large datasets across Azure and AWS environments","Participate in onshore-offshore delivery coordination"],"time_allocation":{"coding":55,"architecture":25,"meetings":20}},"requirements":{"must_have":{"experience_years":4,"technical_skills":["Azure Synapse Analytics","Azure Data Factory","ADLS","Databricks","Power BI","Azure Analysis Services","SQL","SSIS","Python","Terraform","Jenkins"],"domain_expertise":["Data Warehousing","ETL/ELT","Analytics","Data Lifecycle Management"],"soft_skills":["problem-solving","working-with-ambiguity","stakeholder-collaboration"]},"nice_to_have":{"technical_skills":["AWS","CI/CD automation","IaC"],"domain_expertise":["Hybrid cloud data platforms"]},"deal_breakers":["No hands-on Azure Synapse experience","No ETL or data warehouse background"]},"technical_assessment_focus":{"coding":["SQL optimization","Python ETL logic"],"system_design":["Azure data warehouse architecture"],"devops":["CI/CD with Terraform and Jenkins"],"problem_solving":["Large dataset performance tuning"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"contract":{"duration_months":6,"extension_possible":true}},"logistics":{"location":"Trivandrum, India","remote_policy":"hybrid","work_shift":"IST","job_type":"contract"},"interview_process":{"stages":["HR screening","Technical interview","Client discussion"],"decision_makers":["Data Architect","Engineering Manager"]},"metadata":{"posted_date":null,"application_deadline":null,"hiring_urgency":"high"},"embedding_optimization":{"primary_text":"Azure Data Engineer contract role focused on Synapse Analytics, SSIS, Databricks, Power BI, and CI/CD automation with Terraform.","semantic_tags":["azure-synapse","data-warehouse","ssis","power-bi","terraform","ci-cd","databricks","etl"]}}
{"id":"job_digitrix_databricks_platform_engineer_010","role":{"title":"Databricks Platform Engineer / Administrator","level":"mid","department":"Platform Engineering","team":"Data Platform","reports_to":"Platform Lead"},"company":{"name":"Digitrix Software LLP","size":null,"industry":"Software & IT Services","culture_keywords":["cross-functional","governance-driven","cost-aware","platform-first"]},"description":{"summary":"Manage, optimize, and govern Databricks environments on AWS with a strong focus on cost optimization, security, and performance.","key_value_proposition":"Opportunity to own Databricks platform operations, FinOps, and governance for large-scale AWS-based data platforms.","full_text":"As a Databricks Platform Engineer / Administrator, you will oversee AWS-based Databricks environments, manage infrastructure, optimize performance and cost, enforce governance and security, and automate platform operations using infrastructure-as-code."},"responsibilities":{"primary":["Deploy and manage Databricks workspaces on AWS","Configure AWS infrastructure including VPCs, IAM roles, S3, and security groups","Enforce cluster policies for cost and resource governance","Monitor and optimize Databricks and AWS costs","Tune Spark workloads for performance"],"secondary":["Set up monitoring and alerting using CloudWatch and Databricks audit logs","Implement Unity Catalog access controls","Automate provisioning using Terraform and Databricks APIs","Document platform standards and optimization playbooks"],"time_allocation":{"platform_ops":40,"optimization":30,"automation":20,"collaboration":10}},"requirements":{"must_have":{"experience_years":3,"technical_skills":["Databricks on AWS","Apache Spark","AWS S3","IAM","VPC","CloudWatch","Delta Lake","Terraform","Python","SQL"],"domain_expertise":["Platform Engineering","Cost Optimization","Cloud Governance"],"soft_skills":["cross-team collaboration","analytical thinking","documentation"]},"nice_to_have":{"technical_skills":["Unity Catalog","Delta Live Tables","MLflow"],"domain_expertise":["FinOps","DataOps","Cloud Center of Excellence"],"certifications":["Databricks Certified Admin","AWS Solutions Architect","AWS DevOps Engineer"]},"deal_breakers":["No AWS Databricks experience","Lack of Spark performance tuning"]},"technical_assessment_focus":{"platform":["Databricks workspace architecture","Cluster policy enforcement","AWS networking and IAM"],"optimization":["Cost governance","Spark tuning","Storage optimization"],"automation":["Terraform","Databricks REST APIs","CLI usage"],"security":["Unity Catalog","IAM roles","Audit logging"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":"Karnataka, India","remote_policy":"onsite","required_onsite_days":5,"relocation_assistance":false,"visa_sponsorship":false},"interview_process":{"stages":["HR screening","Platform technical interview","Managerial round"],"total_duration_weeks":null,"decision_makers":["Platform Lead","Engineering Manager"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Databricks Platform Engineer / Administrator role on AWS focusing on platform operations, cost optimization, Spark performance tuning, security governance, and automation using Terraform.","semantic_tags":["databricks-admin","platform-engineer","aws","spark","finops","cloud-governance","terraform","delta-lake"]}}
{"id":"job_informica_databricks_data_engineer_012","role":{"title":"Databricks Data Engineer","level":"mid","department":"Data Engineering","team":"Cloud Data Platforms","reports_to":"Data Engineering Lead"},"company":{"name":"Informica Solutions","size":null,"industry":"IT Services","culture_keywords":["cloud-native","automation"]},"description":{"summary":"Build scalable Databricks-based data platforms and pipelines end-to-end.","key_value_proposition":"Hands-on ownership of ETL pipelines, Lakehouse architecture, and ML enablement.","full_text":"Developer role focused on Databricks, Delta Lake, ETL/ELT pipelines, and cloud-native data platforms."},"responsibilities":{"primary":["Build ETL/ELT pipelines using Databricks and Delta Lake","Develop data lakes and warehouse models","Manage Databricks jobs and clusters","Enable data quality and metadata"],"secondary":["Support ML workflows","Maintain documentation","Automate deployments"],"time_allocation":{"coding":65,"ops":20,"meetings":15}},"requirements":{"must_have":{"experience_years":2,"technical_skills":["Databricks","PySpark","Python","SQL","Delta Lake","Talend","DataStage"],"domain_expertise":["Data Engineering","Distributed Systems"],"soft_skills":["collaboration"]},"nice_to_have":{"technical_skills":["Redshift","ML pipelines"],"certifications":["Databricks","Azure"]},"deal_breakers":["No Databricks hands-on experience"]},"logistics":{"location":"Bengaluru","remote_policy":"hybrid"},"metadata":{"job_type":"full-time"},"embedding_optimization":{"primary_text":"Databricks Data Engineer building cloud-native ETL pipelines.","semantic_tags":["databricks","data-engineer","delta-lake","etl"]}}
{"id":"job_jpmc_databricks_engineer_013","role":{"title":"Software Engineer II - Databricks Engineer","level":"mid","department":"CDAO","team":"Commercial & Investment Bank","reports_to":"Engineering Manager"},"company":{"name":"JPMorgan Chase","size":"enterprise","industry":"Financial Services","culture_keywords":["agile","secure-by-design"]},"description":{"summary":"Deliver enterprise-grade Databricks data products and integrations across multi-cloud platforms.","key_value_proposition":"Work on large-scale financial data products with modern data mesh principles.","full_text":"Agile role delivering Databricks-based data products, governance frameworks, and cloud integrations."},"responsibilities":{"primary":["Develop Databricks data products","Integrate Databricks with AWS, Azure, GCP","Optimize performance and scalability","Support data mesh enablement"],"secondary":["Train domain teams","Maintain governance frameworks"],"time_allocation":{"coding":60,"design":20,"meetings":20}},"requirements":{"must_have":{"experience_years":2,"technical_skills":["Databricks","Python","Spark","SQL","Delta Lake"],"domain_expertise":["Data Engineering","Cloud Platforms"],"soft_skills":["stakeholder collaboration"]}},"logistics":{"location":"Karnataka","remote_policy":"hybrid"},"metadata":{"job_type":"full-time"},"embedding_optimization":{"primary_text":"Software Engineer II specializing in Databricks and Python at JPMorgan Chase.","semantic_tags":["jpmorgan","databricks","python","data-mesh"]}}
{"id":"job_tcs_databricks_developer_014","role":{"title":"Databricks Developer","level":"mid","department":"Technology","team":"Data Platforms","reports_to":"Project Manager"},"company":{"name":"Tata Consultancy Services","size":"enterprise","industry":"IT Services","culture_keywords":["enterprise-scale","process-driven"]},"description":{"summary":"Develop and migrate Azure Databricks-based data warehouse solutions.","key_value_proposition":"Enterprise-scale Azure Databricks projects with structured delivery.","full_text":"Hands-on role working on Azure Databricks, ADF, and cloud data warehouse migrations."},"responsibilities":{"primary":["Develop Azure Databricks pipelines","Implement data warehouse solutions","Work with Azure Data Factory","Support cloud migrations"],"secondary":["Documentation","Code reviews"],"time_allocation":{"coding":65,"meetings":35}},"requirements":{"must_have":{"experience_years":3,"technical_skills":["Azure Databricks","ADF","SQL","Git"],"domain_expertise":["Data Warehousing","ETL"],"soft_skills":["documentation"]}},"logistics":{"location":"Pune","remote_policy":"onsite"},"metadata":{"job_type":"full-time"},"embedding_optimization":{"primary_text":"Databricks Developer role at TCS using Azure Databricks and ADF.","semantic_tags":["tcs","azure-databricks","etl"]}}
{"id":"job_tenjumps_databricks_data_engineer_015","role":{"title":"Data Engineer - Databricks","level":"mid-senior","department":"Data Engineering","team":"Lakehouse Platform","reports_to":"Data Engineering Manager"},"company":{"name":"Tenjumps","size":null,"industry":"Technology / Data Platforms","culture_keywords":["fast-paced","ownership","high-performance","proactive-leadership"]},"description":{"summary":"Design, build, and optimize scalable data pipelines and analytics solutions on the Databricks Lakehouse Platform.","key_value_proposition":"End-to-end ownership of Lakehouse-based data pipelines powering analytics and AI/ML use cases.","full_text":"Responsible for transforming raw data into high-value datasets using Databricks, PySpark, Delta Lake, and modern governance practices while ensuring scalability, reliability, and performance."},"responsibilities":{"primary":["Develop scalable ETL/ELT pipelines using PySpark, SQL, and Delta Lake","Build and maintain batch and streaming pipelines using Databricks Workflows","Design and optimize Lakehouse architecture and Delta Lake tables","Integrate data from APIs, databases, cloud storage, and third-party platforms","Ensure data quality using Delta Live Tables, expectations, and monitoring","Implement governance and security using Unity Catalog, RBAC, and lineage"],"secondary":["Work with CI/CD pipelines using Git, Databricks Repos, and DevOps tools","Collaborate with analytics, ML, and business teams","Maintain documentation and coding standards"],"time_allocation":{"coding":65,"design":20,"meetings":15}},"requirements":{"must_have":{"experience_years":3,"technical_skills":["Databricks","Apache Spark","PySpark","Python","SQL","Delta Lake","Databricks Workflows","Unity Catalog"],"domain_expertise":["Lakehouse Architecture","Data Modeling","Distributed Systems","ETL/ELT"],"soft_skills":["problem-solving","leadership","proactive ownership"]},"nice_to_have":{"technical_skills":["Kafka","Structured Streaming","Auto Loader","Photon","Serverless SQL","Power BI","Tableau","Looker"],"certifications":["Databricks Data Engineer Associate","Databricks Data Engineer Professional"],"domain_expertise":["ML workflows","Feature Engineering"]},"deal_breakers":["No hands-on Databricks experience"]},"technical_assessment_focus":{"coding":["PySpark pipelines","Delta Lake optimization"],"system_design":["Lakehouse architecture","Streaming vs Batch design"],"domain_knowledge":["Unity Catalog","Data governance"],"problem_solving":["Performance tuning","Data quality failures"]},"logistics":{"location":"Bengaluru, Karnataka","remote_policy":"onsite"},"metadata":{"job_type":"full-time","hiring_urgency":"medium"},"embedding_optimization":{"primary_text":"Databricks Data Engineer building scalable Lakehouse pipelines using PySpark, Delta Lake, and Unity Catalog.","semantic_tags":["databricks","data-engineer","pyspark","delta-lake","lakehouse","unity-catalog"]}}
{"id":"job_vysystems_databricks_lead_engineer_016","role":{"title":"Databricks Lead Engineer","level":"lead","department":"Data Engineering","team":"Databricks Platform","reports_to":"Data Engineering Head"},"company":{"name":"Vy Systems Private Limited","size":null,"industry":"IT Services & Consulting","culture_keywords":["ownership","scalable-solutions","collaborative","engineering-driven"]},"description":{"summary":"Lead design and implementation of scalable data pipelines using Databricks, Spark, and cloud platforms.","key_value_proposition":"Leadership role owning end-to-end Databricks-based data engineering solutions.","full_text":"Responsible for building, optimizing, and governing large-scale data pipelines on Azure Databricks with strong focus on performance, cost efficiency, and enterprise-grade data governance."},"responsibilities":{"primary":["Design, develop, and maintain ETL pipelines using Databricks (PySpark, SQL, Scala)","Build end-to-end data solutions using Azure Data Lake, Delta Lake, and ADF","Optimize Spark jobs for performance and cost efficiency","Implement data governance, quality, and security best practices","Monitor, debug, and troubleshoot Databricks jobs and clusters"],"secondary":["Collaborate with architects, analysts, and stakeholders","Contribute to CI/CD pipelines for data workflows","Document workflows and technical designs"],"time_allocation":{"coding":50,"design":30,"meetings":20}},"requirements":{"must_have":{"experience_years":8,"technical_skills":["Databricks","Apache Spark","PySpark","Python","SQL","Scala","Azure Data Factory","Delta Lake","Azure Data Lake"],"domain_expertise":["Data Engineering","Distributed Systems","ETL/ELT","Cloud Architecture"],"soft_skills":["leadership","stakeholder-communication","problem-solving"]},"nice_to_have":{"technical_skills":["AWS","CI/CD","DevOps","BI platforms"],"certifications":["Databricks Certification","Azure Data Engineer"],"domain_expertise":["Analytics","Business Intelligence"]},"deal_breakers":["No senior-level Databricks experience"]},"technical_assessment_focus":{"coding":["Spark optimization","PySpark ETL"],"system_design":["Cloud data architecture","Lakehouse design"],"domain_knowledge":["Data governance","Security"],"problem_solving":["Cost optimization","Job failures"]},"logistics":{"location":"Remote","remote_policy":"remote"},"metadata":{"job_type":"full-time","hiring_urgency":"high"},"embedding_optimization":{"primary_text":"Lead Databricks Engineer with deep expertise in Spark, Azure Databricks, Delta Lake, and ETL pipelines.","semantic_tags":["databricks","lead-data-engineer","spark","pyspark","azure","delta-lake"]}}
{"id":"job_astellas_databricks_data_engineer_017","role":{"title":"Databricks Data Engineer","level":"mid","department":"InformationX / DigitalX","team":"Data Engineering","reports_to":"Technical Lead"},"company":{"name":"Astellas","size":null,"industry":"Pharmaceuticals & Life Sciences","culture_keywords":["digital-transformation","agile","innovation","global-collaboration"]},"description":{"summary":"Build and maintain Databricks-based data pipelines supporting analytics, ML, and business intelligence.","key_value_proposition":"Opportunity to contribute to large-scale digital transformation within a global pharma organization.","full_text":"Responsible for developing scalable data pipelines, supporting ML workflows, and delivering analytics insights using Databricks, cloud platforms, and modern data engineering practices."},"responsibilities":{"primary":["Develop ETL/ELT pipelines using Databricks notebooks and Delta Lake","Collaborate with data scientists to deploy ML models","Manage Databricks clusters, jobs, and performance optimization","Build data warehouses and data lakes","Integrate data from SAP, Salesforce, APIs, and relational databases"],"secondary":["Perform code reviews and provide technical guidance","Support CI/CD pipelines using Azure DevOps","Maintain documentation and governance standards"],"time_allocation":{"coding":60,"design":25,"meetings":15}},"requirements":{"must_have":{"experience_years":3,"technical_skills":["Databricks","Apache Spark","PySpark","Python","SQL","Delta Lake","Azure DevOps","ETL Tools"],"domain_expertise":["Data Engineering","Distributed Computing","Data Modeling"],"soft_skills":["communication","adaptability","analytical-thinking"]},"nice_to_have":{"technical_skills":["Talend","DataStage","AWS S3","Redshift","SAP","Salesforce"],"certifications":["Databricks Data Engineer","Azure Data Engineer","AWS Data Analytics"],"domain_expertise":["Pharma/Life Sciences","Machine Learning"]},"deal_breakers":["No production Databricks experience"]},"technical_assessment_focus":{"coding":["PySpark transformations","SQL optimization"],"system_design":["End-to-end data pipelines","ML integration"],"domain_knowledge":["Data governance","Cloud services"],"problem_solving":["Pipeline failures","Performance bottlenecks"]},"logistics":{"location":"Karnataka, India","remote_policy":"hybrid"},"metadata":{"job_type":"full-time","hiring_urgency":"medium"},"embedding_optimization":{"primary_text":"Databricks Data Engineer supporting analytics and ML pipelines in a global life sciences organization.","semantic_tags":["databricks","data-engineer","pyspark","delta-lake","azure","life-sciences"]}}
{"id":"job_mindcurv_databricks_data_engineer_016","role":{"title":"Senior Data Engineer - Databricks Specialist","level":"senior","department":"Data & Analytics","team":"Cloud Data Engineering","reports_to":"Data Engineering Lead"},"company":{"name":"Mindcurv (Accenture Song)","size":null,"industry":"Digital Transformation / Cloud & Analytics","culture_keywords":["agile","entrepreneurial","innovation","work-life-balance","client-focused"]},"description":{"summary":"Lead the design and implementation of scalable Lakehouse-based data solutions using Databricks and cloud platforms.","key_value_proposition":"Drive enterprise-scale analytics and ML use cases using modern Databricks Lakehouse architecture.","full_text":"Responsible for building cloud-native data pipelines, optimizing data lake and warehouse architectures, and enforcing governance and security best practices while mentoring junior engineers."},"responsibilities":{"primary":["Design, build, and maintain scalable ETL pipelines using Databricks","Implement Lakehouse architecture with Delta Lake and Unity Catalog","Develop optimized data lake and cloud data warehouse solutions","Optimize pipelines for performance and cost efficiency","Implement data governance, access control, and compliance standards","Monitor and troubleshoot data pipelines"],"secondary":["Collaborate with business and analytics stakeholders","Mentor junior data engineers","Work with clients based in Western Europe"],"time_allocation":{"coding":55,"design":25,"mentoring":10,"meetings":10}},"requirements":{"must_have":{"experience_years":5,"technical_skills":["Databricks","Apache Spark","Delta Lake","Unity Catalog","Python","SQL","PySpark","AWS"],"domain_expertise":["Lakehouse Architecture","Cloud Data Warehousing","Data Modeling","ETL/ELT"],"soft_skills":["communication","leadership","independent-working"]},"nice_to_have":{"technical_skills":["Airflow","MLOps","Power BI"],"certifications":["Databricks Certification","AWS Certification"],"domain_expertise":["Machine Learning workflows"]},"deal_breakers":["No Databricks or cloud experience"]},"technical_assessment_focus":{"coding":["PySpark ETL","SQL optimization"],"system_design":["Lakehouse architecture","Cloud data warehousing"],"domain_knowledge":["Data governance","GDPR/CCPA compliance"],"problem_solving":["Pipeline reliability","Cost optimization"]},"logistics":{"location":"Coimbatore, Tamil Nadu","remote_policy":"hybrid"},"metadata":{"job_type":"full-time","hiring_urgency":"medium"},"embedding_optimization":{"primary_text":"Senior Databricks Data Engineer building scalable Lakehouse solutions using Delta Lake, Unity Catalog, and AWS.","semantic_tags":["databricks","senior-data-engineer","lakehouse","delta-lake","unity-catalog","aws"]}}
{"id":"job_zranga_databricks_data_engineer_017","role":{"title":"Databricks Data Engineer","level":"mid-senior","department":"Data Engineering","team":"Client Delivery","reports_to":"Technical Lead"},"company":{"name":"zRanga","size":null,"industry":"Staffing & Talent Solutions","culture_keywords":["client-centric","flexible","collaborative","fast-delivery"]},"description":{"summary":"Build and optimize scalable Databricks-based data solutions for enterprise clients.","key_value_proposition":"Hands-on role focused on ETL, data modeling, and performance optimization on Databricks.","full_text":"Responsible for designing data pipelines, managing data warehouses, and integrating multiple data sources using Databricks, Spark, and cloud platforms."},"responsibilities":{"primary":["Develop batch and streaming ETL/ELT pipelines","Design data lakes, lakehouses, and warehouses using Delta Lake","Optimize Spark jobs and Databricks clusters","Ensure data quality, governance, and security","Integrate data from APIs, databases, and streaming platforms"],"secondary":["Collaborate with analysts, data scientists, and stakeholders","Support analytics and reporting use cases"],"time_allocation":{"coding":65,"design":20,"meetings":15}},"requirements":{"must_have":{"experience_years":3,"technical_skills":["Databricks","Apache Spark","Python","SQL","Delta Lake","ETL/ELT"],"domain_expertise":["Data Engineering","Data Modeling","Data Warehousing"],"soft_skills":["problem-solving","communication"]},"nice_to_have":{"technical_skills":["Scala","Azure","Streaming platforms"],"domain_expertise":["Advanced analytics"]},"deal_breakers":["Lack of Databricks experience"]},"technical_assessment_focus":{"coding":["PySpark","SQL transformations"],"system_design":["Data lake vs warehouse"],"domain_knowledge":["Data governance"],"problem_solving":["Performance tuning"]},"logistics":{"location":"Hyderabad, Telangana","remote_policy":"hybrid"},"metadata":{"job_type":"full-time","employment_modes":["full-time","contract","freelance"],"hiring_urgency":"medium"},"embedding_optimization":{"primary_text":"Databricks Data Engineer specializing in ETL pipelines, Delta Lake, and Spark optimization.","semantic_tags":["databricks","data-engineer","spark","delta-lake","etl","azure"]}}
{"id":"job_bsri_senior_databricks_developer_018","role":{"title":"Senior Databricks Developer","level":"senior","department":"Engineering","team":"Big Data Development","reports_to":"Project Manager"},"company":{"name":"BSRI Solutions","size":null,"industry":"IT Software Services","culture_keywords":["team-oriented","long-term","analytical","learning-focused"]},"description":{"summary":"Develop and maintain enterprise-grade data solutions using Databricks and Big Data technologies.","key_value_proposition":"Senior hands-on role with strong focus on Spark, streaming, and backend data engineering.","full_text":"Responsible for designing, reviewing, and optimizing Databricks-based data solutions while mentoring junior developers."},"responsibilities":{"primary":["Develop data pipelines using Databricks, Spark, and PySpark","Write optimized SQL and backend logic","Implement Spark Streaming and Kafka-based pipelines","Perform code reviews and mentor junior engineers","Collaborate with cross-functional teams"],"secondary":["Support production systems","Learn and adopt new technologies"],"time_allocation":{"coding":60,"reviewing":20,"meetings":20}},"requirements":{"must_have":{"experience_years":5,"technical_skills":["Databricks","Apache Spark","PySpark","Python","SQL","Scala","Kafka","Linux"],"domain_expertise":["Big Data","Streaming Systems","Backend Development"],"soft_skills":["teamwork","communication","analytical-thinking"]},"nice_to_have":{"technical_skills":["Advanced Spark Streaming"],"domain_expertise":["Large-scale data platforms"]},"deal_breakers":["Less than 5 years IT experience"]},"technical_assessment_focus":{"coding":["Spark jobs","PySpark transformations"],"system_design":["Streaming architectures"],"domain_knowledge":["Big Data ecosystems"],"problem_solving":["Production debugging"]},"logistics":{"location":"Chennai, Tamil Nadu","remote_policy":"onsite"},"metadata":{"job_type":"full-time","salary_range":"₹6L–₹20L","hiring_urgency":"medium"},"embedding_optimization":{"primary_text":"Senior Databricks Developer with expertise in Spark, PySpark, Kafka, and big data pipelines.","semantic_tags":["databricks","senior-developer","spark","pyspark","kafka","big-data"]}}
{"id":"job_cgi_lead_azure_databricks_developer_019","role":{"title":"Lead Azure Databricks Developer","level":"lead","department":"Data Engineering","team":"Data Modernization & Migration","reports_to":"Data Architecture Lead"},"company":{"name":"CGI","size":"large-enterprise","industry":"IT & Business Consulting Services","culture_keywords":["ownership","teamwork","innovation","long-term-growth","client-focused"]},"description":{"summary":"Lead Azure Databricks migration initiatives by analyzing legacy ETL systems and designing scalable cloud-native Lakehouse solutions.","key_value_proposition":"Drive enterprise data modernization through SSIS-to-Databricks migration, data modeling, and performance optimization.","full_text":"Responsible for reviewing legacy ETL mappings, reverse-engineering business logic, designing optimized Azure Databricks solutions, and mentoring engineers during large-scale cloud migration programs."},"responsibilities":{"primary":["Analyze legacy ETL workflows and mapping documents (SSIS)","Design migration strategies to Azure Databricks using PySpark and Delta Lake","Recommend scalable data models for target architecture","Develop reusable ingestion, transformation, and validation frameworks","Collaborate with architects, analysts, and migration teams","Document migration approaches, technical designs, and lineage","Mentor junior developers and enforce best practices"],"secondary":["Support performance tuning and cost optimization","Ensure cloud-native design standards"],"time_allocation":{"analysis":30,"design":30,"coding":25,"mentoring":15}},"requirements":{"must_have":{"experience_years":8,"technical_skills":["Azure Databricks","PySpark","Delta Lake","Azure Data Lake","Azure Data Factory","Synapse Analytics","SQL"],"domain_expertise":["ETL migration","Legacy system analysis","Data modeling","Performance tuning"],"soft_skills":["communication","documentation","technical-leadership"]},"nice_to_have":{"technical_skills":["SSIS","CI/CD","DevOps"],"certifications":["Azure Data Engineer Associate"],"domain_expertise":["Agile","Cloud migration programs"]},"deal_breakers":["No Azure Databricks migration experience"]},"technical_assessment_focus":{"coding":["PySpark transformations","SQL optimization"],"system_design":["SSIS to Databricks migration","Lakehouse architecture"],"domain_knowledge":["Dimensional modeling","Data lineage"],"problem_solving":["Complex ETL reverse-engineering"]},"logistics":{"location":"Karnataka","remote_policy":"hybrid"},"metadata":{"job_type":"full-time","hiring_urgency":"high"},"embedding_optimization":{"primary_text":"Lead Azure Databricks Developer specializing in legacy ETL migration, data modeling, and Lakehouse performance optimization.","semantic_tags":["azure-databricks","lead-data-engineer","etl-migration","delta-lake","pyspark","ssis"]}}
{"id":"job_genpact_principal_consultant_databricks_lead_020","role":{"title":"Principal Consultant - Databricks Lead Developer","level":"principal","department":"Data Engineering & Analytics","team":"Enterprise Data Platforms","reports_to":"Solution Architect"},"company":{"name":"Genpact","size":"enterprise","industry":"Advanced Technology & Digital Transformation","culture_keywords":["innovation","ai-driven","continuous-learning","enterprise-scale","values-driven"]},"description":{"summary":"Lead enterprise Databricks implementations, solving complex data engineering problems across batch and streaming workloads.","key_value_proposition":"Own end-to-end Databricks Lakehouse solutions with strong focus on performance, scalability, and cost efficiency.","full_text":"Responsible for designing and implementing complex Databricks data pipelines, guiding architecture decisions, optimizing Spark workloads, and collaborating with architects and cross-functional teams."},"responsibilities":{"primary":["Design and implement enterprise Databricks Lakehouse solutions","Develop complex batch and streaming pipelines using Spark and Delta Lake","Optimize Spark jobs for performance and cost efficiency","Collaborate with architects and lead engineers","Implement Databricks workflows and orchestration","Write unit and integration tests","Guide teams and promote best practices"],"secondary":["Track emerging technologies","Support migration to unified data platforms"],"time_allocation":{"coding":50,"architecture":25,"optimization":15,"mentoring":10}},"requirements":{"must_have":{"experience_years":10,"technical_skills":["Databricks","Apache Spark","Delta Lake","Python","Scala","SQL","Spark SQL","Hive","Databricks Workflows"],"domain_expertise":["Data Engineering","Lakehouse Architecture","Batch & Streaming pipelines","Performance tuning"],"soft_skills":["analytical-thinking","communication","technical-leadership"]},"nice_to_have":{"technical_skills":["Unity Catalog","Databricks SQL Endpoints","CI/CD","dbt","Docker","Kubernetes"],"domain_expertise":["Migration projects","Unified data platforms"],"certifications":["Databricks","Cloud certifications"]},"deal_breakers":["No end-to-end Databricks project experience"]},"technical_assessment_focus":{"coding":["Spark transformations","SQL & Spark SQL"],"system_design":["Enterprise Lakehouse","Streaming architectures"],"domain_knowledge":["Cloud data services","Distributed systems"],"problem_solving":["Cost and performance optimization"]},"logistics":{"location":"Chennai, Tamil Nadu","remote_policy":"hybrid"},"metadata":{"job_type":"full-time","hiring_urgency":"high"},"embedding_optimization":{"primary_text":"Principal Databricks Lead Developer building enterprise Lakehouse platforms with Spark, Delta Lake, and cloud services.","semantic_tags":["databricks","principal-consultant","spark","delta-lake","lakehouse","streaming"]}}
{"id":"job_tezo_databricks_developer_001","role":{"title":"Databricks Developer","level":"senior","department":"Engineering","team":"Data Engineering","reports_to":"Data Engineering Manager"},"company":{"name":"Tezo","size":null,"industry":"Technology / Data Engineering","culture_keywords":["scalability","cloud-native","collaboration","performance","governance"]},"description":{"summary":"Build and optimize scalable data pipelines using Databricks, Apache Spark, and Delta Lake across cloud platforms.","key_value_proposition":"Work on enterprise-grade Databricks implementations across AWS, Azure, and GCP with strong focus on Lakehouse architecture.","full_text":"The Databricks Developer will design, develop, and optimize batch and streaming ETL/ELT pipelines using Spark, Delta Lake, and cloud-native services. The role involves end-to-end data delivery, performance tuning, governance, and collaboration with cross-functional teams."},"responsibilities":{"primary":["Develop scalable data pipelines using Databricks (PySpark/Scala)","Implement Delta Lake and Lakehouse architecture","Build batch and streaming ETL/ELT pipelines","Create and maintain Databricks notebooks, clusters, workflows","Integrate Databricks with cloud services (AWS, Azure, GCP)","Ensure data quality, performance, security, and governance"],"secondary":["Perform data modeling and schema design","Integrate with BI and reporting systems","Collaborate with architects and business teams","Apply cost optimization and best practices"],"time_allocation":{"coding":65,"mentoring":10,"meetings":25}},"requirements":{"must_have":{"experience_years":5,"technical_skills":["Databricks","Apache Spark","PySpark","Scala","Delta Lake","SQL","ETL/ELT","AWS","Azure","GCP"],"domain_expertise":["Data Engineering","Lakehouse Architecture","Cloud Data Platforms"],"soft_skills":["problem-solving","collaboration","ownership","communication"]},"nice_to_have":{"education":[],"technical_skills":["Glue","ADF","Synapse","BigQuery","EMR","Lambda","CI/CD"],"domain_expertise":["BI Integration","Data Modeling"]},"deal_breakers":["No hands-on Databricks experience","Lack of Spark or Delta Lake expertise"]},"technical_assessment_focus":{"coding":["PySpark/Scala development","ETL pipeline implementation"],"system_design":["Lakehouse architecture","Batch and streaming systems"],"domain_knowledge":["Delta Lake optimization","Cloud data services"],"problem_solving":["Performance tuning","Pipeline reliability"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":"Hyderabad, India","remote_policy":"onsite","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Managerial discussion"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Technical Panel"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"high","last_updated":null},"embedding_optimization":{"primary_text":"Senior Databricks Developer role at Tezo in Hyderabad. 5–11 years experience. Databricks, Spark, PySpark, Delta Lake, AWS, Azure, GCP.","semantic_tags":["databricks","spark","delta-lake","cloud-data","etl","senior-data-engineer"]}}
{"id":"job_litmus7_databricks_platform_support_001","role":{"title":"Databricks Platform Engineer","level":"mid-senior","department":"Platform Engineering","team":"Data Platform Operations","reports_to":"Platform Manager"},"company":{"name":"Litmus7","size":null,"industry":"Retail Technology / Data Platforms","culture_keywords":["agile","automation","reliability","security","operations"]},"description":{"summary":"Manage and support Databricks platforms across AWS, Azure, and GCP ensuring reliability, security, and performance.","key_value_proposition":"Own enterprise Databricks platform operations with focus on automation, governance, and cost optimization.","full_text":"The Databricks Platform Engineer will administer and support Databricks environments, handle production issues, manage clusters, implement security and compliance, and automate platform operations using Terraform and APIs."},"responsibilities":{"primary":["Administer Databricks workspaces and access control","Provide production support and incident resolution","Manage and optimize Databricks clusters","Implement security, compliance, and governance","Integrate Databricks with external data platforms","Automate platform provisioning using Terraform"],"secondary":["Support data engineering and data science teams","Monitor platform performance and costs","Maintain CI/CD integrations","Document operational procedures"],"time_allocation":{"coding":40,"mentoring":10,"meetings":50}},"requirements":{"must_have":{"experience_years":3,"technical_skills":["Databricks Administration","Terraform","AWS","Azure","GCP","SQL","PySpark","Databricks REST API"],"domain_expertise":["Platform Operations","Cloud Infrastructure","Data Governance"],"soft_skills":["troubleshooting","communication","ownership"]},"nice_to_have":{"education":[],"technical_skills":["MLflow","Workflows","DLT","dbx CLI"],"domain_expertise":["Streaming Pipelines"]},"deal_breakers":["No production Databricks support experience"]},"technical_assessment_focus":{"coding":["Automation scripting","Debugging SQL/PySpark"],"system_design":["Platform architecture","Security and access models"],"domain_knowledge":["Databricks internals","Cloud networking"],"problem_solving":["Incident resolution","Performance and cost issues"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":"Gandhinagar, India","remote_policy":"onsite","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Operations round"],"total_duration_weeks":null,"decision_makers":["Platform Manager"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Databricks Platform Engineer role at Litmus7 focusing on administration, Terraform, AWS/Azure/GCP, security and production support.","semantic_tags":["databricks-admin","platform-engineering","terraform","cloud","production-support"]}}
{"id":"job_nttdata_databricks_engineer_001","role":{"title":"Databricks Engineer","level":"senior","department":"Data & Analytics","team":"Data Engineering","reports_to":"Project Manager"},"company":{"name":"NTT DATA","size":"10000+","industry":"IT Services & Consulting","culture_keywords":["enterprise-scale","innovation","inclusion","delivery-focused"]},"description":{"summary":"Build and optimize Azure-based Databricks pipelines with strong data quality and governance.","key_value_proposition":"Work on enterprise Azure data platforms serving global Fortune 100 clients.","full_text":"The Databricks Engineer will develop end-to-end data pipelines using Azure Databricks, ADF, and Synapse. Responsibilities include optimization, data quality enforcement, documentation, and SLA-driven delivery."},"responsibilities":{"primary":["Develop Azure Databricks pipelines","Build notebooks and optimize workloads","Implement data quality and monitoring","Develop SQL and Python-based transformations","Integrate with Azure Data Factory and Synapse"],"secondary":["CI/CD using Azure DevOps","Documentation and knowledge transfer","Operational support"],"time_allocation":{"coding":60,"mentoring":10,"meetings":30}},"requirements":{"must_have":{"experience_years":5,"technical_skills":["Azure Databricks","ADF","Synapse","Python","SQL","Apache Spark"],"domain_expertise":["Data Engineering","Azure Data Platform"],"soft_skills":["problem-solving","communication"]},"nice_to_have":{"education":["Bachelor's degree"],"technical_skills":["SSIS","NoSQL"],"domain_expertise":["Enterprise Data Warehousing"]},"deal_breakers":["No Azure Databricks experience"]},"technical_assessment_focus":{"coding":["Python","SQL"],"system_design":["Azure data pipelines"],"domain_knowledge":["Data quality","Performance tuning"],"problem_solving":["Production issues"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":true,"target_percent":null}},"logistics":{"location":"Delhi, India","remote_policy":"hybrid","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR","Technical","Managerial"],"total_duration_weeks":null,"decision_makers":["Hiring Manager"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":"350367","hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Senior Databricks Engineer at NTT DATA in Delhi using Azure Databricks, ADF, Synapse, Python, SQL.","semantic_tags":["azure-databricks","enterprise-data","senior-engineer","cloud"]}}
{"id":"job_sid_senior_databricks_engineer_001","role":{"title":"Senior Databricks Engineer","level":"senior","department":"Engineering","team":"Data Engineering","reports_to":"Technical Lead"},"company":{"name":"SID Information Technologies","size":null,"industry":"Healthcare Analytics / IT Services","culture_keywords":["innovation","healthcare","scalability","engineering-excellence"]},"description":{"summary":"Design and optimize AWS-based Databricks data pipelines for healthcare analytics.","key_value_proposition":"Work on cutting-edge healthcare analytics platforms using Databricks and AWS.","full_text":"The Senior Databricks Engineer will build scalable pipelines using Python, Databricks, Spark, and AWS services while ensuring automation, performance, and reliability."},"responsibilities":{"primary":["Build scalable AWS data pipelines","Implement Databricks and Spark processing","Manage CI/CD pipelines","Optimize cloud storage and compute","Collaborate with cross-functional teams"],"secondary":["Infrastructure automation","Performance tuning"],"time_allocation":{"coding":65,"mentoring":10,"meetings":25}},"requirements":{"must_have":{"experience_years":6,"technical_skills":["Python","Databricks","Apache Spark","AWS","SQL","Jenkins"],"domain_expertise":["Data Engineering","Cloud Analytics"],"soft_skills":["collaboration","ownership"]},"nice_to_have":{"education":[],"technical_skills":[],"domain_expertise":["Healthcare Analytics"]},"deal_breakers":["No AWS Databricks experience"]},"technical_assessment_focus":{"coding":["Python","Spark"],"system_design":["AWS pipelines"],"domain_knowledge":["Healthcare data"],"problem_solving":["Performance tuning"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":"Hyderabad, India","remote_policy":"hybrid","required_onsite_days":3,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["Technical","Managerial"],"total_duration_weeks":null,"decision_makers":["Tech Lead"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"high","last_updated":null},"embedding_optimization":{"primary_text":"Senior Databricks Engineer at SID Information Technologies working on AWS healthcare analytics.","semantic_tags":["senior-databricks","aws","healthcare-data","spark"]}}
{"id":"job_asyva_azure_databricks_engineer_001","role":{"title":"Azure Data Engineer","level":"mid-senior","department":"Engineering","team":"Data Engineering","reports_to":"Delivery Manager"},"company":{"name":"ASYVA INFOTECH","size":null,"industry":"IT Services / Data Engineering","culture_keywords":["global-clients","contract","cloud-native","collaboration"]},"description":{"summary":"Build cloud-native Azure Databricks data pipelines for global clients.","key_value_proposition":"Long-term contract role working on modern Azure & Databricks architectures.","full_text":"The Azure Databricks Engineer will design, develop, and optimize ETL/ELT pipelines using Azure Databricks, ADF, Synapse, and related services while ensuring data quality, governance, and CI/CD automation."},"responsibilities":{"primary":["Develop ETL/ELT pipelines using Databricks","Work with Azure Data Factory, ADLS, Synapse","Optimize Spark jobs and Bronze–Silver–Gold models","Implement CI/CD pipelines","Ensure data quality and governance"],"secondary":["Collaborate with Power BI teams","Documentation and code reviews"],"time_allocation":{"coding":65,"mentoring":10,"meetings":25}},"requirements":{"must_have":{"experience_years":3,"technical_skills":["Azure Databricks","PySpark","Python","SQL","ADF","ADLS","Synapse"],"domain_expertise":["Azure Data Engineering","ETL/ELT"],"soft_skills":["analytical-thinking","communication"]},"nice_to_have":{"education":[],"technical_skills":["dbt","Snowflake","Airflow","Docker"],"domain_expertise":["Event-driven architectures"]},"deal_breakers":["No Azure Databricks experience"]},"technical_assessment_focus":{"coding":["PySpark","SQL"],"system_design":["Azure pipelines"],"domain_knowledge":["Delta Lake","Spark optimization"],"problem_solving":["Debugging pipelines"]},"compensation":{"salary_range":{"min":null,"max":106516.53,"currency":"INR","target":"monthly"},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":null,"target_percent":null}},"logistics":{"location":"India","remote_policy":"remote","required_onsite_days":0,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR","Technical"],"total_duration_weeks":null,"decision_makers":["Hiring Manager"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"contract","requisition_id":null,"hiring_urgency":"high","last_updated":null},"embedding_optimization":{"primary_text":"Azure Databricks Engineer contract role at ASYVA INFOTECH. Azure, PySpark, Databricks, ADF, Synapse.","semantic_tags":["azure-databricks","contract-role","etl","cloud-data","remote"]}}
{"id":"job_capgemini_azure_databricks_engineer_001","role":{"title":"Azure Databricks Engineer","level":"mid-senior","department":"Cloud & Data","team":"Data Engineering","reports_to":"Project / Delivery Manager"},"company":{"name":"Capgemini","size":"10000+","industry":"IT Services & Digital Transformation","culture_keywords":["collaboration","sustainability","innovation","inclusion","learning"]},"description":{"summary":"Design, develop, and optimize Azure-based data pipelines using Databricks, ADF, PySpark, and Delta Lake.","key_value_proposition":"Work on large-scale enterprise Azure data platforms for global clients in a collaborative and inclusive environment.","full_text":"The Azure Databricks Engineer will design end-to-end ETL pipelines using Azure Data Factory and Databricks, perform complex transformations using PySpark and SQL, implement data quality frameworks, manage secure access, and deliver solutions in an Agile environment."},"responsibilities":{"primary":["Design and develop end-to-end data pipelines using Azure Databricks and ADF","Build and optimize ETL workflows for structured and semi-structured data","Write efficient PySpark and SQL for transformations and aggregations","Implement data validation and quality frameworks","Manage secure access using Azure Key Vault and RBAC"],"secondary":["Work in Agile/Scrum teams","Collaborate with global cross-functional teams","Participate in CI/CD pipeline development"],"time_allocation":{"coding":60,"mentoring":10,"meetings":30}},"requirements":{"must_have":{"experience_years":4,"technical_skills":["Azure Databricks","Azure Data Factory","PySpark","SQL","Delta Lake","Azure Data Lake","Azure Synapse","Python","Scala","Hive"],"domain_expertise":["Azure Data Engineering","ETL/ELT","Cloud Data Platforms"],"soft_skills":["collaboration","judgement","communication","solution-design"]},"nice_to_have":{"education":[],"technical_skills":["CI/CD","Azure Key Vault"],"domain_expertise":["Enterprise Analytics"]},"deal_breakers":["Less than 4 years Azure Databricks experience"]},"technical_assessment_focus":{"coding":["PySpark","SQL"],"system_design":["Azure data pipelines","Synapse & ADLS architecture"],"domain_knowledge":["Data validation","Performance tuning"],"problem_solving":["Optimization and reliability issues"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":true,"target_percent":null}},"logistics":{"location":"Mumbai, Bangalore, Chennai, Hyderabad, India","remote_policy":"hybrid","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Managerial round"],"total_duration_weeks":null,"decision_makers":["Hiring Manager","Technical Panel"]},"metadata":{"posted_date":"2025-11-26","application_deadline":null,"job_type":"full-time","requisition_id":"342347-en_GB","hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Azure Databricks Engineer at Capgemini with 4+ years experience using PySpark, ADF, Delta Lake, Synapse, and Azure Data Lake.","semantic_tags":["azure-databricks","data-engineering","etl","pyspark","cloud-data","enterprise"]}}
{"id":"job_chubb_senior_data_engineer_databricks_001","role":{"title":"Senior Data Engineer","level":"senior","department":"Engineering & Analytics","team":"Data Engineering","reports_to":"Engineering Manager"},"company":{"name":"Chubb","size":"10000+","industry":"Insurance & Financial Services","culture_keywords":["engineering-excellence","analytics-driven","collaboration","continuous-learning"]},"description":{"summary":"Build and optimize large-scale cloud-based data solutions using Azure Databricks, PySpark, and SQL.","key_value_proposition":"Work on high-impact data engineering solutions driving digital transformation in the global insurance domain.","full_text":"The Senior Data Engineer will design and implement complex data transformations using Azure Databricks, PySpark, and SQL. The role involves performance tuning, Spark optimization, CI/CD automation, and close collaboration with business stakeholders in the insurance domain."},"responsibilities":{"primary":["Develop cloud-based data solutions using Azure Databricks","Build complex transformations for structured and semi-structured data","Optimize Spark workloads and tune performance","Write advanced SQL stored procedures and functions","Implement CI/CD pipelines and DevOps practices"],"secondary":["Collaborate with business stakeholders","Analyze insurance domain data requirements","Participate in Agile delivery processes"],"time_allocation":{"coding":65,"mentoring":15,"meetings":20}},"requirements":{"must_have":{"experience_years":8,"technical_skills":["Azure","Databricks","PySpark","Python","SQL","T-SQL","Apache Spark","Git","CI/CD"],"domain_expertise":["Data Engineering","Big Data Processing","Insurance Analytics"],"soft_skills":["communication","analytical-thinking","problem-solving"]},"nice_to_have":{"education":[],"technical_skills":["Azure DevOps","Jenkins"],"domain_expertise":["Insurance Domain"]},"deal_breakers":["No Spark performance tuning experience","No Azure Databricks exposure"]},"technical_assessment_focus":{"coding":["PySpark","SQL","Python"],"system_design":["Cloud-based data platforms"],"domain_knowledge":["Spark architecture","Insurance data modeling"],"problem_solving":["Performance bottlenecks","Complex transformation logic"]},"compensation":{"salary_range":{"min":null,"max":null,"currency":"INR","target":null},"equity":{"offered":false,"range_percent":""},"bonus":{"offered":true,"target_percent":null}},"logistics":{"location":"Bangalore, Hyderabad, India","remote_policy":"hybrid","required_onsite_days":null,"relocation_assistance":null,"visa_sponsorship":null},"interview_process":{"stages":["HR screening","Technical interview","Managerial interview"],"total_duration_weeks":null,"decision_makers":["Engineering Manager","Technical Panel"]},"metadata":{"posted_date":null,"application_deadline":null,"job_type":"full-time","requisition_id":null,"hiring_urgency":"medium","last_updated":null},"embedding_optimization":{"primary_text":"Senior Data Engineer at Chubb working on Azure Databricks, PySpark, SQL, and insurance analytics.","semantic_tags":["senior-data-engineer","databricks","azure","spark","insurance-data","cloud-engineering"]}}